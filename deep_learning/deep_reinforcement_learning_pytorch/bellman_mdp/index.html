



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. Used by thousands of students and professionals from top tech companies, research institutions.">
      
      
        <link rel="canonical" href="https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/">
      
      
        <meta name="author" content="Ritchie Ng">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Markov Decision Processes and Bellman Equations - Deep Learning Wizard</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#546e7a">
      
    
    
      <script src="../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="blue-grey" data-md-color-accent="blue">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../../#markov-decision-processes-mdps" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-header-nav__button md-logo">
          
            <i class="md-icon">whatshot</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Deep Learning Wizard
              </span>
              <span class="md-header-nav__topic">
                Markov Decision Processes and Bellman Equations
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href="../../.." title="Home" class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
      
        
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../intro/" title="Deep Learning Tutorials" class="md-tabs__link md-tabs__link--active">
          Deep Learning Tutorials
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../database/intro/" title="Scalable Database Tutorials" class="md-tabs__link">
          Scalable Database Tutorials
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../news/news/" title="News" class="md-tabs__link">
          News
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-nav__button md-logo">
      
        <i class="md-icon">whatshot</i>
      
    </a>
    Deep Learning Wizard
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../supporters/" title="Supporters" class="md-nav__link">
      Supporters
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../review/" title="Reviews" class="md-nav__link">
      Reviews
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Deep Learning Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Deep Learning Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../intro/" title="Introduction" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../course_progression/" title="Course Progression" class="md-nav__link">
      Course Progression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_matrices/" title="Matrices" class="md-nav__link">
      Matrices
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_gradients/" title="Gradients" class="md-nav__link">
      Gradients
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_linear_regression/" title="Linear Regression" class="md-nav__link">
      Linear Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_logistic_regression/" title="Logistic Regression" class="md-nav__link">
      Logistic Regression
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_feedforward_neuralnetwork/" title="Feedforward Neural Networks" class="md-nav__link">
      Feedforward Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_convolutional_neuralnetwork/" title="Convolutional Neural Networks" class="md-nav__link">
      Convolutional Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_recurrent_neuralnetwork/" title="Recurrent Neural Networks" class="md-nav__link">
      Recurrent Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_lstm_neuralnetwork/" title="Long Short Term Memory Neural Networks" class="md-nav__link">
      Long Short Term Memory Neural Networks
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/derivative_gradient_jacobian/" title="Derivative, Gradient and Jacobian" class="md-nav__link">
      Derivative, Gradient and Jacobian
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/" title="Forward- and Backward-propagation and Gradient Descent" class="md-nav__link">
      Forward- and Backward-propagation and Gradient Descent
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/lr_scheduling/" title="Learning Rate Scheduling" class="md-nav__link">
      Learning Rate Scheduling
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/optimizers/" title="Optimization Algorithms" class="md-nav__link">
      Optimization Algorithms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/weight_initialization_activation_functions/" title="Weight Initialization and Activation Functions" class="md-nav__link">
      Weight Initialization and Activation Functions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../supervised_to_rl/" title="Supervised Learning to Reinforcement Learning" class="md-nav__link">
      Supervised Learning to Reinforcement Learning
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Markov Decision Processes and Bellman Equations
      </label>
    
    <a href="./" title="Markov Decision Processes and Bellman Equations" class="md-nav__link md-nav__link--active">
      Markov Decision Processes and Bellman Equations
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdps" title="Markov Decision Processes (MDPs)" class="md-nav__link">
    Markov Decision Processes (MDPs)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-main-characteristics-for-mdps" title="Two main characteristics for MDPs" class="md-nav__link">
    Two main characteristics for MDPs
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-markov-models" title="Types of Markov Models" class="md-nav__link">
    Types of Markov Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-components-of-mdps" title="5 Components of MDPs" class="md-nav__link">
    5 Components of MDPs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moving-from-mdps-to-optimal-policy" title="Moving From MDPs to Optimal Policy" class="md-nav__link">
    Moving From MDPs to Optimal Policy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policy-pi_pi_" title="Optimal Policy \pi_*\pi_*" class="md-nav__link">
    Optimal Policy \pi_*\pi_*
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="Comments" class="md-nav__link md-nav__link--active">
            Comments
          </a>
        </li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../readings/" title="Additional Readings" class="md-nav__link">
      Additional Readings
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Scalable Database Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Scalable Database Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../database/intro/" title="Welcome" class="md-nav__link">
      Welcome
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../database/setting_up_cluster/" title="Cassandra Cluster Setup" class="md-nav__link">
      Cassandra Cluster Setup
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      News
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        News
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/news/" title="Welcome" class="md-nav__link">
      Welcome
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/ammi_facebook_google_recap_2018_11_21/" title="AMMI (AIMS) supported by Facebook and Google, November 2018" class="md-nav__link">
      AMMI (AIMS) supported by Facebook and Google, November 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/" title="NExT++ AI in Healthcare and Finance, Nanjing, November 2018" class="md-nav__link">
      NExT++ AI in Healthcare and Finance, Nanjing, November 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_devcon_recap_2018_10_02/" title="Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018" class="md-nav__link">
      Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_developer_conference_2018_09_05/" title="Facebook PyTorch Developer Conference, San Francisco, September 2018" class="md-nav__link">
      Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nvidia_nus_mit_datathon_2018_07_05/" title="NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018" class="md-nav__link">
      NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_1y_2018_06_01/" title="Featured on PyTorch Website 2018" class="md-nav__link">
      Featured on PyTorch Website 2018
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/nvidia_self_driving_cars_talk_2017_06_21/" title="NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017" class="md-nav__link">
      NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_nvidia_inception_2018_05_01/" title="NVIDIA Inception Partner Status, Singapore, May 2017" class="md-nav__link">
      NVIDIA Inception Partner Status, Singapore, May 2017
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdps" title="Markov Decision Processes (MDPs)" class="md-nav__link">
    Markov Decision Processes (MDPs)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-main-characteristics-for-mdps" title="Two main characteristics for MDPs" class="md-nav__link">
    Two main characteristics for MDPs
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-markov-models" title="Types of Markov Models" class="md-nav__link">
    Types of Markov Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-components-of-mdps" title="5 Components of MDPs" class="md-nav__link">
    5 Components of MDPs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moving-from-mdps-to-optimal-policy" title="Moving From MDPs to Optimal Policy" class="md-nav__link">
    Moving From MDPs to Optimal Policy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policy-pi_pi_" title="Optimal Policy \pi_*\pi_*" class="md-nav__link">
    Optimal Policy \pi_*\pi_*
  </a>
  
</li>
      
      
      
      
      
        <li class="md-nav__item">
          <a href="#__comments" title="Comments" class="md-nav__link md-nav__link--active">
            Comments
          </a>
        </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Markov Decision Processes and Bellman Equations</h1>
                
                <h2 id="markov-decision-processes-mdps">Markov Decision Processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Permanent link">&para;</a></h2>
<ul>
<li>Typically we can <strong>frame all RL tasks as MDPs</strong><ul>
<li>Intuitively, it's sort of a way to frame RL tasks such that we can solve them in a "principled" manner. We will go into the specifics throughout this tutorial</li>
</ul>
</li>
<li>The key in MDPs is the <strong>Markov Property</strong><ul>
<li>Essentially the future depends on the present and not the past<ul>
<li>More specifically, the future is independent of the past given the present</li>
<li>There's an assumption the present state encapsulates past information. This is not always true, see the note below.</li>
</ul>
</li>
</ul>
</li>
<li>Putting into the context of what we have covered so far: our agent can (1) <strong>control its action</strong> based on its current (2) <strong>completely known state</strong><ul>
<li>Back to the "driving to avoid puppy" example: given we know there is a dog in front of the car as the current state, the agent can decide to take a left/right turn to avoid colliding with the puppy</li>
</ul>
</li>
</ul>
<h2 id="two-main-characteristics-for-mdps">Two main characteristics for MDPs<a class="headerlink" href="#two-main-characteristics-for-mdps" title="Permanent link">&para;</a></h2>
<ol>
<li>Control over state transitions</li>
<li>States completely observable</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Other Markov Models</p>
<p>Permutations of whether there is presence of the two main characteristics would lead to different Markov models. These are not important now, but it gives you an idea of what other frameworks we can use besides MDPs.</p>
<h6 id="types-of-markov-models">Types of Markov Models<a class="headerlink" href="#types-of-markov-models" title="Permanent link">&para;</a></h6>
<ol>
<li><ins class="critic">Control</ins> over state transitions and <ins class="critic">completely observable</ins> states: <strong>MDPs</strong></li>
<li><ins class="critic">Control</ins> over state transitions and <mark>partially observable states</mark>: <strong>Partially Observable MDPs (POMDPs)</strong></li>
<li><mark>No control</mark> over state transitions and <ins class="critic">completely observable</ins> states: <strong>Markov Chain</strong></li>
<li><mark>No control</mark> over state transitions and <mark>partially observable</mark> states: <strong>Hidden Markov Model</strong></li>
</ol>
</div>
<h2 id="5-components-of-mdps">5 Components of MDPs<a class="headerlink" href="#5-components-of-mdps" title="Permanent link">&para;</a></h2>
<ol>
<li><span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span>: set of states</li>
<li><span><span class="MathJax_Preview">\mathcal{A}</span><script type="math/tex">\mathcal{A}</script></span>: set of actions</li>
<li><span><span class="MathJax_Preview">\mathcal{R}</span><script type="math/tex">\mathcal{R}</script></span>: reward function</li>
<li><span><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span>: transition probability function</li>
<li><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span>: discount for future rewards</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Remembering 5 components with a mnemonic</p>
<p>A mnemonic I use to remember the 5 components is the acronym "SARPY" (sar-py). I know <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> is not <span><span class="MathJax_Preview">\mathcal{Y}</span><script type="math/tex">\mathcal{Y}</script></span> but it looks like a <code class="codehilite">y</code> so there's that.</p>
</div>
<h2 id="moving-from-mdps-to-optimal-policy">Moving From MDPs to Optimal Policy<a class="headerlink" href="#moving-from-mdps-to-optimal-policy" title="Permanent link">&para;</a></h2>
<ul>
<li>We have an <strong>agent</strong> acting in an <strong>environment</strong></li>
<li>The way the <strong>environment</strong> reacts to the agent's <strong>actions (<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>)</strong> is dictated by a <strong>model</strong></li>
<li>The agent can take <strong>actions (<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>)</strong> to move from one <strong>state (<span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>)</strong> to another <strong>new state (<span><span class="MathJax_Preview">s')</span><script type="math/tex">s')</script></span></strong></li>
<li>When the agent has transited to a <strong>new state (<span><span class="MathJax_Preview">s')</span><script type="math/tex">s')</script></span></strong>, there will a <strong>reward (<span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>)</strong></li>
<li>We may or may not know our <strong>model</strong><ol>
<li><strong>Model-based RL</strong>: this is where we can <ins class="critic">clearly define</ins> our (1) transition probabilities and/or (2) reward function<ul>
<li>A global minima can somehow be attained via Dynamic Programming(DP)</li>
</ul>
</li>
<li><strong>Model-free RL</strong>: this is where we <mark>cannot clearly define</mark> our (1) transition probabilities and/or (2) reward function</li>
</ol>
</li>
<li>How the agent <strong>acts (<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>)</strong> in its current <strong>state (<span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>)</strong> is specified by its <strong>policy (<span><span class="MathJax_Preview">\pi(s)</span><script type="math/tex">\pi(s)</script></span>)</strong><ul>
<li>It can either be deterministic or stochastic<ol>
<li><strong>Deterministic policy</strong>: <span><span class="MathJax_Preview">a = \pi(s)</span><script type="math/tex">a = \pi(s)</script></span></li>
<li><strong>Stochastic policy</strong>: <span><span class="MathJax_Preview">\mathbb{P}_\pi [A=a \vert S=s] = \pi(a | s)</span><script type="math/tex">\mathbb{P}_\pi [A=a \vert S=s] = \pi(a | s)</script></span><ul>
<li>This is the proability of taking an action given the current state under the policy</li>
<li><span><span class="MathJax_Preview">\mathcal{A}</span><script type="math/tex">\mathcal{A}</script></span>: set of all actions</li>
<li><span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span>: set of all states</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>When the agent acts given its state under the <strong>policy (<span><span class="MathJax_Preview">\pi(a | s)</span><script type="math/tex">\pi(a | s)</script></span>)</strong>, the <strong>transition probability function <span><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span></strong> determines the subsequent <strong>state (<span><span class="MathJax_Preview">s'</span><script type="math/tex">s'</script></span>)</strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{P}_{ss'}^a = \mathcal{P}(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a]</span><script type="math/tex">\mathcal{P}_{ss'}^a = \mathcal{P}(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a]</script></span></li>
</ul>
</li>
<li>When the agent act based on its <strong>policy (<span><span class="MathJax_Preview">\pi(a | s)</span><script type="math/tex">\pi(a | s)</script></span>)</strong> and transited to the new state determined by the transition probability function <span><span class="MathJax_Preview">\mathcal{P}_{ss'}^a</span><script type="math/tex">\mathcal{P}_{ss'}^a</script></span> it gets a reward based on the <strong>reward function</strong> as a feedback<ul>
<li><span><span class="MathJax_Preview">\mathcal{R}_s^a = \mathbb{E} [\mathcal{R}_{t+1} \vert S_t = s, A_t = a]</span><script type="math/tex">\mathcal{R}_s^a = \mathbb{E} [\mathcal{R}_{t+1} \vert S_t = s, A_t = a]</script></span></li>
</ul>
</li>
<li>Rewards are short-term, given as feedback after the agent takes an action and transits to a new state. Summing all future rewards and discounting them would lead to our <strong>return <span><span class="MathJax_Preview">\mathcal{G}</span><script type="math/tex">\mathcal{G}</script></span></strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{G}_t = \sum_{i=0}^{N} \gamma^k \mathcal{R}_{t+1+i}</span><script type="math/tex">\mathcal{G}_t = \sum_{i=0}^{N} \gamma^k \mathcal{R}_{t+1+i}</script></span><ul>
<li><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span>, our discount factor which ranges from 0 to 1 (inclusive) reduces the weightage of future rewards allowing us to balance between short-term and long-term goals</li>
</ul>
</li>
</ul>
</li>
<li>With our <strong>return <span><span class="MathJax_Preview">\mathcal{G}</span><script type="math/tex">\mathcal{G}</script></span></strong>, we then have our <strong>state-value function <span><span class="MathJax_Preview">\mathcal{V}_{\pi}</span><script type="math/tex">\mathcal{V}_{\pi}</script></span></strong> (how good to stay in that state) and our <strong>action-value or q-value function <span><span class="MathJax_Preview">\mathcal{Q}_{\pi}</span><script type="math/tex">\mathcal{Q}_{\pi}</script></span></strong> (how good to take the action)<ul>
<li><span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s]</span><script type="math/tex">\mathcal{V}_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s]</script></span></li>
<li><span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s, \mathcal{A}_t = a]</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s, \mathcal{A}_t = a]</script></span></li>
<li>The advantage function is simply the difference between the two functions <span><span class="MathJax_Preview">\mathcal{A}_{\pi}(s, a) = \mathcal{Q}_{\pi}(s, a) - \mathcal{V}_{\pi}(s)</span><script type="math/tex">\mathcal{A}_{\pi}(s, a) = \mathcal{Q}_{\pi}(s, a) - \mathcal{V}_{\pi}(s)</script></span><ul>
<li>Seems useless at this stage, but this advantage function will be used in some key algorithms we are covering</li>
</ul>
</li>
</ul>
</li>
<li>Since our policy determines how our agent acts given its state, achieving an <strong>optimal policy <span><span class="MathJax_Preview">\pi_*</span><script type="math/tex">\pi_*</script></span></strong> would mean achieving optimal actions that is exactly what we want!</li>
</ul>
<h2 id="optimal-policy-pi_pi_">Optimal Policy <span><span class="MathJax_Preview">\pi_*</span><script type="math/tex">\pi_*</script></span><a class="headerlink" href="#optimal-policy-pi_pi_" title="Permanent link">&para;</a></h2>
<ul>
<li>Optimal policy &rarr; optimal state-value and action-value functions &rarr; max return &rarr; argmax of those functions<ul>
<li><span><span class="MathJax_Preview">\pi_{*} = \arg\max_{\pi} \mathcal{V}_{\pi}(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s, a)</span><script type="math/tex">\pi_{*} = \arg\max_{\pi} \mathcal{V}_{\pi}(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s, a)</script></span></li>
</ul>
</li>
</ul>
<p>To be continued...</p>
                
                  
                
              
              
                


  <h2 id="__comments">Comments</h2>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = "https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/";
      this.page.identifier =
        "deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//deep-learning-wizard.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="deep_learning/deep_reinforcement_learning_pytorch/supervised_to_rl/" title="Supervised Learning to Reinforcement Learning" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Supervised Learning to Reinforcement Learning
              </span>
            </div>
          </a>
        
        
          <a href="deep_learning/readings/" title="Additional Readings" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Additional Readings
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2018 Deep Learning Wizard
          </div>
        
        proudly an
        <a href="http://www.nvidia.com/page/home.html">NVIDIA Inception Partner</a>
        based in Singapore by
        <a href="https://www.ritchieng.com/">
          Ritchie Ng (NVIDIA Deep Learning Institute Instructor)</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/ritchieng" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://www.facebook.com/DeepLearningWizard/" class="md-footer-social__link fa fa-facebook"></a>
    
      <a href="https://www.linkedin.com/company/deeplearningwizard/" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.583bbe55.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
        <script>!function(e,a,t,n,o,c,i){e.GoogleAnalyticsObject=o,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),i=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",i.parentNode.insertBefore(c,i)}(window,document,"script",0,"ga"),ga("create","UA-122083328-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview");var links=document.getElementsByTagName("a");if(Array.prototype.map.call(links,function(e){e.host!=document.location.host&&e.addEventListener("click",function(){var a=e.getAttribute("data-md-action")||"follow";ga("send","event","outbound",a,e.href)})}),document.forms.search){var query=document.forms.search.query;query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}</script>
      
    
  </body>
</html>