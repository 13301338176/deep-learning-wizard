{
    "docs": [
        {
            "location": "/",
            "text": "About Us\n\u00b6\n\n\nWe deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our \nvideo course\n and join over 2300 deep learning wizards. \n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nPyTorch as our Preferred Deep Learning Library\n\u00b6\n\n\nWe chose \nPyTorch\n because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# It is this easy! \n\n\nimport\n \ntorch\n\n\n\n# Create a variable of value 1 each.\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\nb\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\n\n# Add the 2 variables to give you 2, it's that simple!\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\n\n\n\n\n\nMade for Visual and Book Lovers\n\u00b6\n\n\nWe are visual creatures, that is why we offer detailed \nvideo courses\n on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. \n\n\nFor book lovers, you will be happy to know \nDeep Learning Wizard's wikipedia\n will always be updated first prior to our release of video courses.\n\n\nExperienced Research and Applied Team\n\u00b6\n\n\n\n\nRitchie Ng\n\n\nCurrently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.\n\n\nMy passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.\n\n\nI was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.\n\n\nCheck out my profile link at \nritchieng.com\n\n\n\n\n\n\nJie Fu\n\n\nI am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.\n\n\nI earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.\n\n\nI am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.\n\n\nCheck out my profile link at \nbigaidream.github.io",
            "title": "Home"
        },
        {
            "location": "/#about-us",
            "text": "We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our  video course  and join over 2300 deep learning wizards.   To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.",
            "title": "About Us"
        },
        {
            "location": "/#pytorch-as-our-preferred-deep-learning-library",
            "text": "We chose  PyTorch  because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.  1\n2\n3\n4\n5\n6\n7\n8\n9 # It is this easy!   import   torch  # Create a variable of value 1 each.  a   =   torch . Tensor ([ 1 ])  b   =   torch . Tensor ([ 1 ])  # Add the 2 variables to give you 2, it's that simple!  c   =   a   +   b",
            "title": "PyTorch as our Preferred Deep Learning Library"
        },
        {
            "location": "/#made-for-visual-and-book-lovers",
            "text": "We are visual creatures, that is why we offer detailed  video courses  on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch.   For book lovers, you will be happy to know  Deep Learning Wizard's wikipedia  will always be updated first prior to our release of video courses.",
            "title": "Made for Visual and Book Lovers"
        },
        {
            "location": "/#experienced-research-and-applied-team",
            "text": "Ritchie Ng  Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.  My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.  I was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.  Check out my profile link at  ritchieng.com    Jie Fu  I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.  I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.  I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.  Check out my profile link at  bigaidream.github.io",
            "title": "Experienced Research and Applied Team"
        },
        {
            "location": "/supporters/",
            "text": "Supporters\n\u00b6\n\n\nMore than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.\n\n\nCorporations\n\u00b6\n\n\n\n\n NVIDIA (NVIDIA Inception Partner)\n\n\n Facebook\n\n\n Amazon\n\n\n\n\nResearch Institutions\n\u00b6\n\n\n\n\n Montreal Institute of Learning Algorithms (MILA), Montreal, Canada\n\n\n Imperial College London, UK\n\n\n Massachusetts Institute of Technology (MIT), USA  \n\n\n National University of Singapore (NUS), Singapore\n\n\n Nanyang Technological University (NTU), Singapore\n\n\n\n\nNVIDIA Inception Partner\n\u00b6\n\n\n\n\n\"NVIDIA Inception Partner\n\n\n\n\nDeep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#supporters",
            "text": "More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#corporations",
            "text": "NVIDIA (NVIDIA Inception Partner)   Facebook   Amazon",
            "title": "Corporations"
        },
        {
            "location": "/supporters/#research-institutions",
            "text": "Montreal Institute of Learning Algorithms (MILA), Montreal, Canada   Imperial College London, UK   Massachusetts Institute of Technology (MIT), USA     National University of Singapore (NUS), Singapore   Nanyang Technological University (NTU), Singapore",
            "title": "Research Institutions"
        },
        {
            "location": "/supporters/#nvidia-inception-partner",
            "text": "\"NVIDIA Inception Partner   Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "NVIDIA Inception Partner"
        },
        {
            "location": "/review/",
            "text": "Reviews\n\u00b6\n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nThese are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.\n\n\n\n\nRoberto Trevi\u00f1o Cervantes\n\n\nCongratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.\n\n\n\n\n\n\nMuktabh Mayank\n\n\nThis course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.\n\n\n\n\n\n\nCharles Neiswender\n\n\nI really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.\n\n\n\n\n\n\nIan Lipton\n\n\nThis was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.\n\n\n\n\nAnd check out hundreds of more reviews for our \nvideo course\n!",
            "title": "Reviews"
        },
        {
            "location": "/review/#reviews",
            "text": "To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.  These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.   Roberto Trevi\u00f1o Cervantes  Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.    Muktabh Mayank  This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.    Charles Neiswender  I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.    Ian Lipton  This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.   And check out hundreds of more reviews for our  video course !",
            "title": "Reviews"
        },
        {
            "location": "/deep_learning/intro/",
            "text": "Deep Learning Theory and Programming Tutorials\n\u00b6\n\n\nOur main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our \nvideo course\n.\n\n\n\n\nWork-in-progress\n\n\nThis open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact \nRitchie Ng\n if you would like to contribute via our \nFacebook\n page.\n\n\nAlso take note that these notes are best used as a referral if you are new to deep learning and programming. Go head and take our \nvideo course\n that provides a much easier experience.",
            "title": "Introduction"
        },
        {
            "location": "/deep_learning/intro/#deep-learning-theory-and-programming-tutorials",
            "text": "Our main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our  video course .   Work-in-progress  This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact  Ritchie Ng  if you would like to contribute via our  Facebook  page.  Also take note that these notes are best used as a referral if you are new to deep learning and programming. Go head and take our  video course  that provides a much easier experience.",
            "title": "Deep Learning Theory and Programming Tutorials"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/",
            "text": "PyTorch Fundamentals - Matrices\n\u00b6\n\n\nMatrices\n\u00b6\n\n\nMatrices Brief Introduction\n\u00b6\n\n\n\n\n Basic definition: rectangular array of numbers.\n\n\n Tensors (PyTorch)\n\n\n Ndarrays (NumPy)\n\n\n\n\n2 x 2 Matrix (R x C)\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n2 x 3 Matrix\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nCreating Matrices\n\u00b6\n\n\n\n\nCreate list\n\n\n1\n2\n3\n# Creating a 2x2 array\n\n\narr\n \n=\n \n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\nprint\n(\narr\n)\n\n\n\n\n\n\n\n\n\n1\n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\n\n\n\n\n\n\n\nCreate numpy array via list\n\n\n1\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n\n\n\n1\n2\n# Convert to NumPy\n\n\nnp\n.\narray\n(\narr\n)\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n1\n,\n \n2\n],\n\n       \n[\n3\n,\n \n4\n]])\n\n\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\n1\nimport\n \ntorch\n\n\n\n\n\n\n\n1\n2\n# Convert to PyTorch Tensor\n\n\ntorch\n.\nTensor\n(\narr\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n2\n\n\n3\n  \n4\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate Matrices with Default Values\n\u00b6\n\n\n\n\nCreate 2x2 numpy array of 1's\n\n\n1\nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n \n1.\n,\n  \n1.\n],\n\n       \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\n\n\nCreate 2x2 torch tensor of 1's\n\n\n1\ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\n\n\n\n\n\n\n\nCreate 2x2 numpy array of random numbers\n\n\n1\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n \n0.68270631\n,\n  \n0.87721678\n],\n\n       \n[\n \n0.07420986\n,\n  \n0.79669375\n]])\n\n\n\n\n\n\n\n\n\nCreate 2x2 PyTorch tensor of random numbers\n\n\n1\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0.3900\n  \n0.8268\n\n\n0.3888\n  \n0.5914\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nSeeds for Reproducibility\n\u00b6\n\n\n\n\nWhy do we need seeds?\n\n\nWe need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.\n\n\n\n\n\n\nCreate seed to enable fixed numbers for random number generation \n\n\n1\n2\n3\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\n\n\nRepeat random array generation to check\n\n\nIf you do not set the seed, you would not get the same set of numbers like here.\n\n1\n2\n3\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\n\n\nCreate a numpy array without seed\n\n\nNotice how you get different numbers compared to the first 2 tries?\n\n1\n2\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n \n0.56804456\n,\n  \n0.92559664\n],\n\n       \n[\n \n0.07103606\n,\n  \n0.0871293\n \n]])\n\n\n\n\n\n\n\n\n\nRepeat numpy array generation without seed\n\n\nYou get the point now, you get a totally different set of numbers.\n\n1\n2\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n1\n2\narray\n([[\n \n0.0202184\n \n,\n  \n0.83261985\n],\n\n       \n[\n \n0.77815675\n,\n  \n0.87001215\n]])\n\n\n\n\n\n\n\n\n\nCreate a PyTorch tensor with a fixed seed\n\n\n1\n2\n3\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\n\n\nRepeat creating a PyTorch fixed seed tensor\n\n\n1\n2\n3\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0.5488\n  \n0.5928\n\n\n0.7152\n  \n0.8443\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nCreating a PyTorch tensor without seed\n\n\nLike with a numpy array of random numbers without seed, you will not get the same results as above.\n\n1\n2\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n0.6028\n  \n0.8579\n\n\n0.5449\n  \n0.8473\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nRepeat creating a PyTorch tensor without seed\n\n\nNotice how these are different numbers again?\n\n1\n2\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n0.4237\n  \n0.6236\n\n\n0.6459\n  \n0.3844\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nSeed for GPU is different for now...\n\n\n\n\nFix a seed for GPU tensors\n\n\nWhen you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. \n\n1\n2\nif\n \ntorch\n.\ncuda\n.\nis_available\n():\n\n    \ntorch\n.\ncuda\n.\nmanual_seed_all\n(\n0\n)\n\n\n\n\n\n\n\n\nNumPy and Torch Bridge\n\u00b6\n\n\nNumPy to Torch\n\u00b6\n\n\n\n\nCreate a numpy array of 1's\n\n\n1\n2\n# Numpy array\n\n\nnp_array\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\n1\nprint\n(\nnp_array\n)\n\n\n\n\n\n\n\n\n\n1\n2\n[[\n \n1.\n  \n1.\n]\n\n\n[\n \n1.\n  \n1.\n]]\n\n\n\n\n\n\n\n\n\nGet the type of class for the numpy array\n\n\n1\nprint\n(\ntype\n(\nnp_array\n))\n\n\n\n\n\n\n\n\n\n1\n<\nclass\n \n'\nnumpy\n.\nndarray\n'>\n\n\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\n1\n2\n# Convert to Torch Tensor\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nfrom_numpy\n(\nnp_array\n)\n\n\n\n\n\n\n\n1\nprint\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nGet type of class for PyTorch tensor\n\n\nNotice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type.\n\n1\nprint\n(\ntype\n(\ntorch_tensor\n))\n\n\n\n\n\n\n\n\n1\n<\nclass\n \n'\ntorch\n.\nDoubleTensor\n'>\n\n\n\n\n\n\n\n\n\nCreate PyTorch tensor from a different numpy datatype\n\n\nYou will get an error running this code because PyTorch tensor don't support all datatype. \n\n1\n2\n3\n# Data types matter: intentional error\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n---------------------------------------------------------------------------\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n57\n-\nb8b085f9b39d\n>\n \nin\n \n<\nmodule\n>\n()\n\n      \n1\n \n# Data types matter\n\n      \n2\n \nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\n---->\n \n3\n \ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\nRuntimeError\n:\n \ncan\n't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\n\n\n\n\n\n\n\n\n\nWhat conversion support does Numpy to PyTorch tensor bridge gives?\n\n\n\n\ndouble\n\n\nfloat\n \n\n\nint64\n, \nint32\n, \nuint8\n \n\n\n\n\n\n\n\n\nCreate PyTorch long tensor\n\n\nSee how a int64 numpy array gives you a PyTorch long tensor?\n\n1\n2\n3\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n1  1\n1  1\n[torch.LongTensor of size 2x2]\n\n\n\n\n\n\n\n\nCreate PyTorch int tensor\n\n\n1\n2\n3\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nIntTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nCreate PyTorch byte tensor\n\n\n1\n2\n3\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nuint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nByteTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nCreate PyTorch Double Tensor\n\n\n1\n2\n3\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\nAlternatively you can do this too via \nnp.double\n\n\n1\n2\n3\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\ndouble\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nCreate PyTorch Float Tensor\n\n\n1\n2\n3\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nTensor Type Bug Guide\n\n\nThese things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!\n\n\n\n\n\n\n\n\n\n\nNumPy Array Type\n\n\nTorch Tensor Type\n\n\n\n\n\n\n\n\n\n\nint64\n\n\nLongTensor\n\n\n\n\n\n\nint32\n\n\nIntegerTensor\n\n\n\n\n\n\nuint8\n\n\nByteTensor\n\n\n\n\n\n\nfloat64\n\n\nDoubleTensor\n\n\n\n\n\n\nfloat32\n\n\nFloatTensor\n\n\n\n\n\n\ndouble\n\n\nDoubleTensor\n\n\n\n\n\n\n\n\nTorch to NumPy\n\u00b6\n\n\n\n\nCreate PyTorch tensor of 1's\n\n\nYou would realize this defaults to a float tensor by default if you do this.\n\n\n1\ntorch_tensor\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n1\ntype\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\n\n\n1\ntorch\n.\nFloatTensor\n\n\n\n\n\n\n\n\n\nConvert tensor to numpy\n\n\nIt's as simple as this.\n\n\n1\ntorch_to_numpy\n \n=\n \ntorch_tensor\n.\nnumpy\n()\n\n\n\n\n\n\n\n1\ntype\n(\ntorch_to_numpy\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Wowza, we did it.\n\n\nnumpy\n.\nndarray\n\n\n\n\n\n\n\nTensors on CPU vs GPU\n\u00b6\n\n\n\n\nMove tensor to CPU and back\n\n\nThis by default creates a tensor on CPU. You do not need to do anything.\n\n1\n2\n# CPU\n\n\ntensor_cpu\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\nIf you would like to send a tensor to your GPU, you just need to do a simple \n.cuda\n()\n\n\n1\n2\n3\n# CPU to GPU\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\ntensor_cpu\n.\nto\n(\ndevice\n)\n\n\n\n\n\n\n\nAnd if you want to move that tensor on the GPU back to the CPU, just do the following.\n\n\n1\n2\n# GPU to CPU\n\n\ntensor_cpu\n.\ncpu\n()\n\n\n\n\n\n\n\n\n\nTensor Operations\n\u00b6\n\n\nResizing Tensor\n\u00b6\n\n\n\n\nCreating a 2x2 tensor\n\n\n1\n2\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nGetting size of tensor\n\n\n1\nprint\n(\na\n.\nsize\n())\n\n\n\n\n\n\n\n\n\n1\ntorch\n.\nSize\n([\n2\n,\n \n2\n])\n\n\n\n\n\n\n\n\n\nResize tensor to 4x1\n\n\n1\na\n.\nview\n(\n4\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n4\n]\n\n\n\n\n\n\n\n\n\nGet size of resized tensor\n\n\n1\na\n.\nview\n(\n4\n)\n.\nsize\n()\n\n\n\n\n\n\n\n\n\n1\ntorch\n.\nSize\n([\n4\n])\n\n\n\n\n\n\n\nElement-wise Addition\n\u00b6\n\n\n\n\nCreating first 2x2 tensor\n\n\n1\n2\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nCreating second 2x2 tensor\n\n\n1\n2\nb\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise addition of 2 tensors\n\n\n1\n2\n3\n# Element-wise addition\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nAlternative element-wise addition of 2 tensors\n\n\n1\n2\n3\n# Element-wise addition\n\n\nc\n \n=\n \ntorch\n.\nadd\n(\na\n,\n \nb\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nIn-place element-wise addition\n\n\nThis would replace the c tensor values with the new addition. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# In-place addition\n\n\nprint\n(\n'Old c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\nc\n.\nadd_\n(\na\n)\n\n\n\nprint\n(\n'-'\n*\n60\n)\n\n\nprint\n(\n'New c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nOld\n \nc\n \ntensor\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n------------------------------------------------------------\n\n\nNew\n \nc\n \ntensor\n\n\n \n3\n  \n3\n\n \n3\n  \n3\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise Subtraction\n\u00b6\n\n\n\n\nCheck values of tensor a and b'\n\n\nTake note that you've created tensor a and b of sizes 2x2 filled with 1's each above. \n\n1\n2\nprint\n(\na\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise subtraction: method 1\n\n\n1\na\n \n-\n \nb\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise subtraction: method 2\n\n\n1\n2\n3\n# Not in-place\n\n\nprint\n(\na\n.\nsub\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise subtraction: method 3\n\n\nThis will replace a with the final result filled with 2's\n\n1\n2\n3\n# Inplace\n\n\nprint\n(\na\n.\nsub_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-Wise Multiplication\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\n1\n2\n3\n4\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise multiplication: method 1\n\n\n1\na\n \n*\n \nb\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise multiplication: method 2\n\n\n1\n2\n3\n# Not in-place\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise multiplication: method 3\n\n\n1\n2\n3\n# In-place\n\n\nprint\n(\na\n.\nmul_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-Wise Division\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\n1\n2\n3\n4\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise division: method 1\n\n\n1\nb\n \n/\n \na\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise division: method 2\n\n\n1\ntorch\n.\ndiv\n(\nb\n,\n \na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\n\n\nElement-wise division: method 3\n\n\n1\n2\n# Inplace\n\n\nb\n.\ndiv_\n(\na\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nTensor Mean\n\u00b6\n\n\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n\n\n\n\n mean = 55 /10 = 5.5 \n\n\n mean = 55 /10 = 5.5 \n\n\n\n\n\n\nCreate tensor of size 10 filled from 1 to 10\n\n\n1\n2\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nsize\n()\n\n\n\n\n\n\n\n\n\n1\ntorch\n.\nSize\n([\n10\n])\n\n\n\n\n\n\n\n\n\nGet tensor mean\n\n\nHere we get 5.5 as we've calculated manually above.\n\n\n1\na\n.\nmean\n(\ndim\n=\n0\n)\n\n\n\n\n\n\n\n\n\n1\n2\n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.\n\n\n1\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n7\n-\n81\naec0cf1c00\n>\n \nin\n \n<\nmodule\n>\n()\n\n\n---->\n \n1\n \na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\nRuntimeError\n:\n \ndimension\n \nout\n \nof\n \nrange\n \n(\nexpected\n \nto\n \nbe\n \nin\n \nrange\n \nof\n \n[\n-\n1\n,\n \n0\n],\n \nbut\n \ngot\n \n1\n)\n\n\n\n\n\n\n\n\n\nCreate a 2x10 Tensor, of 1-10 digits each\n\n\n1\na\n \n=\n \ntorch\n.\nTensor\n([[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n],\n \n[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n]])\n\n\n\n\n\n\n1\na\n.\nsize\n()\n\n\n\n\n\n\n\n\n1\ntorch\n.\nSize\n([\n2\n,\n \n10\n])\n\n\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we won't get an error like previously because we've a tensor of size 2x10\n\n\n1\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n \n5.5000\n\n \n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx1\n]\n\n\n\n\n\n\n\nTensor Standard Deviation\n\u00b6\n\n\n\n\nGet standard deviation of tensor\n\n\n\n\n1\n2\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nstd\n(\ndim\n=\n0\n)\n\n\n\n\n\n\n\n1\n2\n \n3.0277\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Create Matrices\n\n\n Create Matrices with Default Initialization Values\n\n\n Zeros \n\n\n Ones\n\n\n\n\n\n\n Initialize Seeds for Reproducibility on GPU and CPU\n\n\n Convert Matrices: NumPy \n Torch and Torch \n NumPy\n\n\n Move Tensors: CPU \n GPU and GPU \n CPU\n\n\n Run Important Tensor Operations\n\n\n Element-wise addition, subtraction, multiplication and division\n\n\n Resize\n\n\n Calculate mean \n\n\n Calculate standard deviation",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#pytorch-fundamentals-matrices",
            "text": "",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices",
            "text": "",
            "title": "Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction",
            "text": "Basic definition: rectangular array of numbers.   Tensors (PyTorch)   Ndarrays (NumPy)   2 x 2 Matrix (R x C)     1  1      1  1     2 x 3 Matrix     1  1  1      1  1  1",
            "title": "Matrices Brief Introduction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices",
            "text": "Create list  1\n2\n3 # Creating a 2x2 array  arr   =   [[ 1 ,   2 ],   [ 3 ,   4 ]]  print ( arr )     1 [[ 1 ,   2 ],   [ 3 ,   4 ]]     Create numpy array via list  1 import   numpy   as   np    1\n2 # Convert to NumPy  np . array ( arr )     1\n2 array ([[ 1 ,   2 ], \n        [ 3 ,   4 ]])     Convert numpy array to PyTorch tensor  1 import   torch    1\n2 # Convert to PyTorch Tensor  torch . Tensor ( arr )     1\n2\n3 1    2  3    4  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Creating Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values",
            "text": "Create 2x2 numpy array of 1's  1 np . ones (( 2 ,   2 ))     1\n2 array ([[   1. ,    1. ], \n        [   1. ,    1. ]])     Create 2x2 torch tensor of 1's  1 torch . ones (( 2 ,   2 ))     1\n2\n3  1  1\n 1  1\n[torch.FloatTensor of size 2x2]    Create 2x2 numpy array of random numbers  1 np . random . rand ( 2 ,   2 )     1\n2 array ([[   0.68270631 ,    0.87721678 ], \n        [   0.07420986 ,    0.79669375 ]])     Create 2x2 PyTorch tensor of random numbers  1 torch . rand ( 2 ,   2 )     1\n2\n3 0.3900    0.8268  0.3888    0.5914  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Create Matrices with Default Values"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility",
            "text": "Why do we need seeds?  We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.    Create seed to enable fixed numbers for random number generation   1\n2\n3 # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )     1\n2 array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])     Repeat random array generation to check  If you do not set the seed, you would not get the same set of numbers like here. 1\n2\n3 # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )     1\n2 array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])     Create a numpy array without seed  Notice how you get different numbers compared to the first 2 tries? 1\n2 # No seed  np . random . rand ( 2 ,   2 )     1\n2 array ([[   0.56804456 ,    0.92559664 ], \n        [   0.07103606 ,    0.0871293   ]])     Repeat numpy array generation without seed  You get the point now, you get a totally different set of numbers. 1\n2 # No seed  np . random . rand ( 2 ,   2 )     1\n2 array ([[   0.0202184   ,    0.83261985 ], \n        [   0.77815675 ,    0.87001215 ]])     Create a PyTorch tensor with a fixed seed  1\n2\n3 # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )      Repeat creating a PyTorch fixed seed tensor  1\n2\n3 # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )     1\n2\n3 0.5488    0.5928  0.7152    0.8443  [ torch . FloatTensor   of   size   2 x2 ]     Creating a PyTorch tensor without seed  Like with a numpy array of random numbers without seed, you will not get the same results as above. 1\n2 # Torch No Seed  torch . rand ( 2 ,   2 )     1\n2\n3 0.6028    0.8579  0.5449    0.8473  [ torch . FloatTensor   of   size   2 x2 ]     Repeat creating a PyTorch tensor without seed  Notice how these are different numbers again? 1\n2 # Torch No Seed  torch . rand ( 2 ,   2 )     1\n2\n3 0.4237    0.6236  0.6459    0.3844  [ torch . FloatTensor   of   size   2 x2 ]    Seed for GPU is different for now...   Fix a seed for GPU tensors  When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above.  1\n2 if   torch . cuda . is_available (): \n     torch . cuda . manual_seed_all ( 0 )",
            "title": "Seeds for Reproducibility"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge",
            "text": "",
            "title": "NumPy and Torch Bridge"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch",
            "text": "Create a numpy array of 1's  1\n2 # Numpy array  np_array   =   np . ones (( 2 ,   2 ))    1 print ( np_array )     1\n2 [[   1.    1. ]  [   1.    1. ]]     Get the type of class for the numpy array  1 print ( type ( np_array ))     1 < class   ' numpy . ndarray '>     Convert numpy array to PyTorch tensor  1\n2 # Convert to Torch Tensor  torch_tensor   =   torch . from_numpy ( np_array )    1 print ( torch_tensor )     1\n2\n3   1    1 \n  1    1  [ torch . DoubleTensor   of   size   2 x2 ]     Get type of class for PyTorch tensor  Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. 1 print ( type ( torch_tensor ))     1 < class   ' torch . DoubleTensor '>     Create PyTorch tensor from a different numpy datatype  You will get an error running this code because PyTorch tensor don't support all datatype.  1\n2\n3 # Data types matter: intentional error  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  torch . from_numpy ( np_array_new )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ---------------------------------------------------------------------------  RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 57 - b8b085f9b39d >   in   < module > () \n       1   # Data types matter \n       2   np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  ---->   3   torch . from_numpy ( np_array_new )  RuntimeError :   can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.     What conversion support does Numpy to PyTorch tensor bridge gives?   double  float    int64 ,  int32 ,  uint8       Create PyTorch long tensor  See how a int64 numpy array gives you a PyTorch long tensor? 1\n2\n3 # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int64 )  torch . from_numpy ( np_array_new )     1\n2\n3 1  1\n1  1\n[torch.LongTensor of size 2x2]    Create PyTorch int tensor  1\n2\n3 # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int32 )  torch . from_numpy ( np_array_new )     1\n2\n3 1    1  1    1  [ torch . IntTensor   of   size   2 x2 ]     Create PyTorch byte tensor  1\n2\n3 # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . uint8 )  torch . from_numpy ( np_array_new )     1\n2\n3 1    1  1    1  [ torch . ByteTensor   of   size   2 x2 ]     Create PyTorch Double Tensor  1\n2\n3 # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float64 )  torch . from_numpy ( np_array_new )    Alternatively you can do this too via  np.double  1\n2\n3 # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . double )  torch . from_numpy ( np_array_new )     1\n2\n3 1    1  1    1  [ torch . DoubleTensor   of   size   2 x2 ]     Create PyTorch Float Tensor  1\n2\n3 # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float32 )  torch . from_numpy ( np_array_new )     1\n2\n3 1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Tensor Type Bug Guide  These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!      NumPy Array Type  Torch Tensor Type      int64  LongTensor    int32  IntegerTensor    uint8  ByteTensor    float64  DoubleTensor    float32  FloatTensor    double  DoubleTensor",
            "title": "NumPy to Torch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy",
            "text": "Create PyTorch tensor of 1's  You would realize this defaults to a float tensor by default if you do this.  1 torch_tensor   =   torch . ones ( 2 ,   2 )    1 type ( torch_tensor )     1 torch . FloatTensor     Convert tensor to numpy  It's as simple as this.  1 torch_to_numpy   =   torch_tensor . numpy ()    1 type ( torch_to_numpy )     1\n2 # Wowza, we did it.  numpy . ndarray",
            "title": "Torch to NumPy"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu",
            "text": "Move tensor to CPU and back  This by default creates a tensor on CPU. You do not need to do anything. 1\n2 # CPU  tensor_cpu   =   torch . ones ( 2 ,   2 )    If you would like to send a tensor to your GPU, you just need to do a simple  .cuda ()  1\n2\n3 # CPU to GPU  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  tensor_cpu . to ( device )    And if you want to move that tensor on the GPU back to the CPU, just do the following.  1\n2 # GPU to CPU  tensor_cpu . cpu ()",
            "title": "Tensors on CPU vs GPU"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations",
            "text": "",
            "title": "Tensor Operations"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor",
            "text": "Creating a 2x2 tensor  1\n2 a   =   torch . ones ( 2 ,   2 )  print ( a )     1\n2\n3 1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Getting size of tensor  1 print ( a . size ())     1 torch . Size ([ 2 ,   2 ])     Resize tensor to 4x1  1 a . view ( 4 )     1\n2\n3\n4\n5 1  1  1  1  [ torch . FloatTensor   of   size   4 ]     Get size of resized tensor  1 a . view ( 4 ) . size ()     1 torch . Size ([ 4 ])",
            "title": "Resizing Tensor"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition",
            "text": "Creating first 2x2 tensor  1\n2 a   =   torch . ones ( 2 ,   2 )  print ( a )     1\n2\n3 1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Creating second 2x2 tensor  1\n2 b   =   torch . ones ( 2 ,   2 )  print ( b )     1\n2\n3 1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise addition of 2 tensors  1\n2\n3 # Element-wise addition  c   =   a   +   b  print ( c )     1\n2\n3   2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]     Alternative element-wise addition of 2 tensors  1\n2\n3 # Element-wise addition  c   =   torch . add ( a ,   b )  print ( c )     1\n2\n3   2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]     In-place element-wise addition  This would replace the c tensor values with the new addition.   1\n2\n3\n4\n5\n6\n7\n8\n9 # In-place addition  print ( 'Old c tensor' )  print ( c )  c . add_ ( a )  print ( '-' * 60 )  print ( 'New c tensor' )  print ( c )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 Old   c   tensor \n\n  2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]  ------------------------------------------------------------  New   c   tensor \n\n  3    3 \n  3    3  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Addition"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction",
            "text": "Check values of tensor a and b'  Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above.  1\n2 print ( a )  print ( b )     1\n2\n3\n4\n5\n6\n7\n8   1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ] \n\n\n  1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise subtraction: method 1  1 a   -   b     1\n2\n3 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise subtraction: method 2  1\n2\n3 # Not in-place  print ( a . sub ( b ))  print ( a )     1\n2\n3\n4\n5\n6\n7\n8 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise subtraction: method 3  This will replace a with the final result filled with 2's 1\n2\n3 # Inplace  print ( a . sub_ ( b ))  print ( a )     1\n2\n3\n4\n5\n6\n7\n8 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Subtraction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  1\n2\n3\n4 a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )     1\n2\n3\n4\n5\n6\n7\n8 1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise multiplication: method 1  1 a   *   b     1\n2\n3 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise multiplication: method 2  1\n2\n3 # Not in-place  print ( torch . mul ( a ,   b ))  print ( a )     1\n2\n3\n4\n5\n6\n7 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise multiplication: method 3  1\n2\n3 # In-place  print ( a . mul_ ( b ))  print ( a )     1\n2\n3\n4\n5\n6\n7 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Multiplication"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  1\n2\n3\n4 a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )     1\n2\n3\n4\n5\n6\n7\n8 1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise division: method 1  1 b   /   a     1\n2\n3 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise division: method 2  1 torch . div ( b ,   a )     1\n2\n3 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]     Element-wise division: method 3  1\n2 # Inplace  b . div_ ( a )     1\n2\n3 0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Division"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean",
            "text": "1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55  1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55     mean = 55 /10 = 5.5    mean = 55 /10 = 5.5     Create tensor of size 10 filled from 1 to 10  1\n2 a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . size ()     1 torch . Size ([ 10 ])     Get tensor mean  Here we get 5.5 as we've calculated manually above.  1 a . mean ( dim = 0 )     1\n2 5.5000  [ torch . FloatTensor   of   size   1 ]     Get tensor mean on second dimension  Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.  1 a . mean ( dim = 1 )     1\n2\n3\n4\n5\n6\n7 RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 7 - 81 aec0cf1c00 >   in   < module > ()  ---->   1   a . mean ( dim = 1 )  RuntimeError :   dimension   out   of   range   ( expected   to   be   in   range   of   [ - 1 ,   0 ],   but   got   1 )     Create a 2x10 Tensor, of 1-10 digits each  1 a   =   torch . Tensor ([[ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ],   [ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ]])    1 a . size ()     1 torch . Size ([ 2 ,   10 ])     Get tensor mean on second dimension  Here we won't get an error like previously because we've a tensor of size 2x10  1 a . mean ( dim = 1 )     1\n2\n3   5.5000 \n  5.5000  [ torch . FloatTensor   of   size   2 x1 ]",
            "title": "Tensor Mean"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation",
            "text": "Get standard deviation of tensor   1\n2 a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . std ( dim = 0 )    1\n2   3.0277  [ torch . FloatTensor   of   size   1 ]",
            "title": "Tensor Standard Deviation"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#summary",
            "text": "We've learnt to...   Success    Create Matrices   Create Matrices with Default Initialization Values   Zeros    Ones     Initialize Seeds for Reproducibility on GPU and CPU   Convert Matrices: NumPy   Torch and Torch   NumPy   Move Tensors: CPU   GPU and GPU   CPU   Run Important Tensor Operations   Element-wise addition, subtraction, multiplication and division   Resize   Calculate mean    Calculate standard deviation",
            "title": "Summary"
        },
        {
            "location": "/news/news/",
            "text": "Welcome to our Blog\n\u00b6\n\n\nHere, we post news related to Deep Learning Wizard's releases, features and achievements \n\n\nNotable News\n\u00b6\n\n\n\n\n Featured on PyTorch Website in 2018\n\n\n Reached 2200+ students in 2018\n\n\n Reached 1000+ students in 2017\n\n\n And more...",
            "title": "Welcome"
        },
        {
            "location": "/news/news/#welcome-to-our-blog",
            "text": "Here, we post news related to Deep Learning Wizard's releases, features and achievements",
            "title": "Welcome to our Blog"
        },
        {
            "location": "/news/news/#notable-news",
            "text": "Featured on PyTorch Website in 2018   Reached 2200+ students in 2018   Reached 1000+ students in 2017   And more...",
            "title": "Notable News"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/",
            "text": "Featured on PyTorch Website\n\u00b6\n\n\nPyTorch a Year Later\n\u00b6\n\n\nWe are featured on \nPyTorch website's post\n \n\n\nI used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.\n\n\nA year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!\n\n\nA big shoutout for \nAlfredo Canziani\n who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.\n\n\nTo more great years ahead for PyTorch \n\n\nCheers,\n\nRitchie Ng",
            "title": "Featured on PyTorch Website 2018"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website",
            "text": "",
            "title": "Featured on PyTorch Website"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later",
            "text": "We are featured on  PyTorch website's post    I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.  A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!  A big shoutout for  Alfredo Canziani  who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.  To more great years ahead for PyTorch   Cheers, Ritchie Ng",
            "title": "PyTorch a Year Later"
        }
    ]
}