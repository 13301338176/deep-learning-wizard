{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Us \u00b6 We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia . For visual learners, feel free to sign up for our video course and join over 3000 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. PyTorch as our Preferred Deep Learning Library \u00b6 We chose PyTorch because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook. # It is this easy! import torch # Create a variable of value 1 each. a = torch . Tensor ([ 1 ]) b = torch . Tensor ([ 1 ]) # Add the 2 variables to give you 2, it's that simple! c = a + b Made for Visual and Book Lovers \u00b6 We are visual creatures, that is why we offer detailed video courses on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. For book lovers, you will be happy to know Deep Learning Wizard's wikipedia will always be updated first prior to our release of video courses. Experienced Research and Applied Team \u00b6 Ritchie Ng Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai , an AI hedge fund based in Singapore comprising quants and traders from JPMorgan and Nomura. I have built the whole AI tech stack in a production environment with rigorous time-sensitive and fail-safe software testing powering multi-million dollar trades daily. Additionally, I co-run, as portfolio manager, our systematic end-to-end deep learning portfolio with the CIO. I am also an NVIDIA Deep Learning Institute instructor leading all deep learning workshops in NUS, Singapore and conducting workshops across Southeast Asia. In my free time, I'm into deep learning research with MILA and NUS where I am a research associate in NExT (NUS) . My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala , Facebook AI Research, and Alfredo Canziani , Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Video Tutorial. I have taught Deep Learning Foundations with Alfredo Canziani at Rwanda, Africa for the African Masters in Machine Intelligence (AMMI) in 2018 supported by Google and Facebook. I was previously conducting research in meta-learning for hyperparameter optimization for deep learning algorithms in NExT Search Centre that is jointly setup between National University of Singapore (NUS), Tsinghua University and University of Southampton led by co-directors Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). I graduated from NUS where I was an NUS Global Merit Scholar, Chua Thian Poh Community Leadership Programme Fellow, Philip Yeo Innovation Fellow, and NUS Enterprise I&E Praticum Award recipient. Check out my profile link at ritchieng.com Jie Fu I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal. I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low. I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner. Check out my profile link at bigaidream.github.io Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Home"},{"location":"#about-us","text":"We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia . For visual learners, feel free to sign up for our video course and join over 3000 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.","title":"About Us"},{"location":"#pytorch-as-our-preferred-deep-learning-library","text":"We chose PyTorch because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook. # It is this easy! import torch # Create a variable of value 1 each. a = torch . Tensor ([ 1 ]) b = torch . Tensor ([ 1 ]) # Add the 2 variables to give you 2, it's that simple! c = a + b","title":"PyTorch as our Preferred Deep Learning Library"},{"location":"#made-for-visual-and-book-lovers","text":"We are visual creatures, that is why we offer detailed video courses on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. For book lovers, you will be happy to know Deep Learning Wizard's wikipedia will always be updated first prior to our release of video courses.","title":"Made for Visual and Book Lovers"},{"location":"#experienced-research-and-applied-team","text":"Ritchie Ng Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai , an AI hedge fund based in Singapore comprising quants and traders from JPMorgan and Nomura. I have built the whole AI tech stack in a production environment with rigorous time-sensitive and fail-safe software testing powering multi-million dollar trades daily. Additionally, I co-run, as portfolio manager, our systematic end-to-end deep learning portfolio with the CIO. I am also an NVIDIA Deep Learning Institute instructor leading all deep learning workshops in NUS, Singapore and conducting workshops across Southeast Asia. In my free time, I'm into deep learning research with MILA and NUS where I am a research associate in NExT (NUS) . My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala , Facebook AI Research, and Alfredo Canziani , Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Video Tutorial. I have taught Deep Learning Foundations with Alfredo Canziani at Rwanda, Africa for the African Masters in Machine Intelligence (AMMI) in 2018 supported by Google and Facebook. I was previously conducting research in meta-learning for hyperparameter optimization for deep learning algorithms in NExT Search Centre that is jointly setup between National University of Singapore (NUS), Tsinghua University and University of Southampton led by co-directors Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). I graduated from NUS where I was an NUS Global Merit Scholar, Chua Thian Poh Community Leadership Programme Fellow, Philip Yeo Innovation Fellow, and NUS Enterprise I&E Praticum Award recipient. Check out my profile link at ritchieng.com Jie Fu I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal. I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low. I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner. Check out my profile link at bigaidream.github.io","title":"Experienced Research and Applied Team"},{"location":"#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"review/","text":"Reviews \u00b6 To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors. Roberto Trevi\u00f1o Cervantes Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year. Muktabh Mayank This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch. Charles Neiswender I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing. Ian Lipton This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math. And check out hundreds of more reviews for our video course !","title":"Reviews"},{"location":"review/#reviews","text":"To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors. Roberto Trevi\u00f1o Cervantes Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year. Muktabh Mayank This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch. Charles Neiswender I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing. Ian Lipton This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math. And check out hundreds of more reviews for our video course !","title":"Reviews"},{"location":"supporters/","text":"Supporters \u00b6 More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings. Individuals \u00b6 Alfredo Canziani Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch. He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun. Do check out his latest mini-course on PyTorch that was held in Princeton University and was delivered with Ritchie Ng at AMMI (AIMS) Kigali, Rwanda supported by Google and Facebook. Marek Bardonski Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months. NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation. Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal . Corporations \u00b6 NVIDIA (NVIDIA Inception Partner) Facebook Amazon Research Institutions \u00b6 Montreal Institute of Learning Algorithms (MILA), Montreal, Canada Imperial College London, UK Massachusetts Institute of Technology (MIT), USA National University of Singapore (NUS), Singapore Nanyang Technological University (NTU), Singapore NVIDIA Inception Partner \u00b6 \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.","title":"Supporters"},{"location":"supporters/#supporters","text":"More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.","title":"Supporters"},{"location":"supporters/#individuals","text":"Alfredo Canziani Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch. He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun. Do check out his latest mini-course on PyTorch that was held in Princeton University and was delivered with Ritchie Ng at AMMI (AIMS) Kigali, Rwanda supported by Google and Facebook. Marek Bardonski Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months. NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation. Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal .","title":"Individuals"},{"location":"supporters/#corporations","text":"NVIDIA (NVIDIA Inception Partner) Facebook Amazon","title":"Corporations"},{"location":"supporters/#research-institutions","text":"Montreal Institute of Learning Algorithms (MILA), Montreal, Canada Imperial College London, UK Massachusetts Institute of Technology (MIT), USA National University of Singapore (NUS), Singapore Nanyang Technological University (NTU), Singapore","title":"Research Institutions"},{"location":"supporters/#nvidia-inception-partner","text":"\"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.","title":"NVIDIA Inception Partner"},{"location":"database/intro/","text":"Scalable Database for Deep Learning \u00b6 Our main open-source programming languages and libraries are Python, C++, Apache Cassandra and Bash. This is a critical part of production deployment of deep learning algorithms. Reasons for Using Apache Cassandra \u00b6 It is is an open-source NoSQL database management system (DBMS) that is designed to handle large amounts of data across many servers, providing high availability with no single point of failure. Meaning if one server fails, everything goes as per normal as data is shared amongst nodes (servers). Compared to other master/slave models like MongoDB, Cassandra has a master-less model. It has extremely fast read/write speeds. You read/write millions of rows of data easily. Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Welcome"},{"location":"database/intro/#scalable-database-for-deep-learning","text":"Our main open-source programming languages and libraries are Python, C++, Apache Cassandra and Bash. This is a critical part of production deployment of deep learning algorithms.","title":"Scalable Database for Deep Learning"},{"location":"database/intro/#reasons-for-using-apache-cassandra","text":"It is is an open-source NoSQL database management system (DBMS) that is designed to handle large amounts of data across many servers, providing high availability with no single point of failure. Meaning if one server fails, everything goes as per normal as data is shared amongst nodes (servers). Compared to other master/slave models like MongoDB, Cassandra has a master-less model. It has extremely fast read/write speeds. You read/write millions of rows of data easily. Work in progress This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page.","title":"Reasons for Using Apache Cassandra"},{"location":"database/setting_up_cluster/","text":"Setting up Cassandra Multi-node Cluster \u00b6 Install Apache Cassandra \u00b6 Patience is key initially It will be quite tedious to many people in setting up a multi-node Cassandra cluster. You are likely to face a lot of problems. But with a strong community and this tried-and-proven guide, you should be ok! Just be patient. Remember when you're asking for help on StackOverflow, here or anywhere else, do paste the logs to make everyone's life easier to help you debug quickly. All you need to do is to run this base command and paste the logs. cat /var/log/cassandra/system.log Here, we will install all of the required Debian packages. To ensure you've the latest version of Cassandra, please use the official link at cassandra.apache.org/download . Bash Commands echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - sudo apt-get update sudo apt-get install cassandra This would get you up and running with Cassandra v3.11. Install DataStax Python Cassandra Driver \u00b6 We need to install the Python Cassandra Driver so we can easily interact with our Cassandra Database without using the CQLSH (this is Cassandra's shell to run CQL commands to perform CRUD operations) commandline directly. What is CRUD? CRUD is the acronym of CREATE, READ, UPDATE, and DELETE. For databases, we typically have these 4 categories of operations so we can create new data points, update, read or delete them in our database. Bash Commands pip install cassandra-driver This is strictly for installation on Linux platforms, refer to the official website for more details. Single Node Cassandra Cluster \u00b6 Before we venture into the cool world of multi-node cluster requiring many servers, we will start with a single node cluster that only requires a single server (desktop/laptop). Once you've installed everything so far, you should run the following. Bash commands sudo service cassandra start sudo nodetool status And this will print out something like that: Datacenter : datacenter1 ============================= Status = Up / Down |/ State = Normal /Leaving/Joining/ Moving -- Address Load Tokens Owns ( effective ) Host ID Rack UN 127.0 . 0.1 318.78 KiB 256 100.0 % g5ac4c9 - 99 b7 - 65 d - 24 cfd82524f9 rack1 That's it, we've built our single node Cassandra database. Multi-node Cassandra Cluster \u00b6 Remember it's standard to have at least 3 nodes, and in a basic 3 separate server configuration (3 separate desktops for non-enterprise users). Because of this, we will base this tutorial on setting up a 3-node cluster. But the same steps apply if you want to even do 10000 nodes. Installing Sublime critical Sublime is a text editor, and it would help if you're not familiar with VIM which I use frequently. Honestly using Sublime to edit all the following files we will edit is much easier, trust me! To install Sublime run the following bash commands in sequence: wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add - echo \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list sudo apt-get update sudo apt-get install sublime-text Now check if it works by running this command in your bash: subl This should open Sublime text editor! You're now ready. Before we move on to the steps to set up each node, we need the IP address of all 3 servers. This is simple, just run the following bash command: ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p' This would return the server's IP address that we need for example: server_1_ip : 112.522 . 6.61 server_2_ip : 112.522 . 6.62 server_3_ip : 112.522 . 6.63 We will be using these IPs as a base for configuration in the subsequent sections. Take note yours would differ and you need to change accordingly. Steps Per Node \u00b6 Critical Section You Need To Repeat This is a critcal section. On EVERY server, you need to repeat all the steps shown in this section. In our case of a 3 node cluster, using 3 servers, we need to repeat this 3 times. Step 1: Modify Cassandra Configuration Settings \u00b6 Run the following bash to edit the configuration file for server 1 (112.522.6.61): cd /etc/cassandra subl cassandra.yaml Now you need to find the following fields and change them accordingly. cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" listen_address: 112.522.6.61 rpc_address: 112.522.6.61 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true Step 2: Modify Rack Details \u00b6 Run the following bash command to edit the rack details: cd /etc/cassandra subl cassandra-rackdc.properties And change the following fields to this: dc=datacenter1 Step 3: Check Status \u00b6 Restart service with the following bash commands: sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status Here you would see an output like this: Datacenter : datacenter ============================= Status = Up / Down |/ State = Normal /Leaving/Joining/ Moving -- Address Load Tokens Owns ( effective ) Host ID Rack UN 112.522 . 6.61 318.78 KiB 256 100.0 % f5c84c9 - 99 b7 - 45 d - 8856 - 24 cfd82523f9 rack1 UN 112.522 . 6.62 206.42 KiB 256 100.0 % ac2f24da - 1 b2c - 4 e3 - 8 a75 - 0 e28ec366a4 rack1 UN 112.522 . 6.63 338.34 KiB 256 100.0 % 55 a227d - ffbe - 45 d5 - 8698 - 60 c374b3a6b rack1 Step 4: Configure Firewall IP Settings \u00b6 Run the following bash commands sudo apt-get install iptables-persistent sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status Repeat Steps 1 to 4 \u00b6 You have to repeat all 4 steps for the other 2 servers but step 1 and step 4 requires slightly different settings. For the other servers, you need to basically change the IP to the server's IP. And for the IP firewall settings, you need to allow entry from the other 2 servers instead of itself. Server 2 Example for Step 2 \u00b6 cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.62 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.62 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true Server 3 Example for Step 2 \u00b6 cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.63 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.63 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true Server 2 Example for Step 4 \u00b6 sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 3 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status Server 3 Example for Step 4 \u00b6 sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 2 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status Dead Node Fix \u00b6 When your server/desktop restarts, you may face a dead node. You need replace the address with the same IP for it to work. cd /etc/cassandra subtl cassandra-env.sh Add the following line in the last row assuming server 1 is dead where the IP is 112.522.6.61 JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=112.522.6.61\" Then run the following bash commands sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status You might have to wait awhile and re-run sudo nodetool status for the DB to get up and running. Summary \u00b6 We have successfully set up a 3-node Cassandra cluster DB after all these steps. We will move on to interacting with the cluster with CQLSH and the Python Driver in subsequent guides. Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Cassandra Cluster Setup"},{"location":"database/setting_up_cluster/#setting-up-cassandra-multi-node-cluster","text":"","title":"Setting up Cassandra Multi-node Cluster"},{"location":"database/setting_up_cluster/#install-apache-cassandra","text":"Patience is key initially It will be quite tedious to many people in setting up a multi-node Cassandra cluster. You are likely to face a lot of problems. But with a strong community and this tried-and-proven guide, you should be ok! Just be patient. Remember when you're asking for help on StackOverflow, here or anywhere else, do paste the logs to make everyone's life easier to help you debug quickly. All you need to do is to run this base command and paste the logs. cat /var/log/cassandra/system.log Here, we will install all of the required Debian packages. To ensure you've the latest version of Cassandra, please use the official link at cassandra.apache.org/download . Bash Commands echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - sudo apt-get update sudo apt-get install cassandra This would get you up and running with Cassandra v3.11.","title":"Install Apache Cassandra"},{"location":"database/setting_up_cluster/#install-datastax-python-cassandra-driver","text":"We need to install the Python Cassandra Driver so we can easily interact with our Cassandra Database without using the CQLSH (this is Cassandra's shell to run CQL commands to perform CRUD operations) commandline directly. What is CRUD? CRUD is the acronym of CREATE, READ, UPDATE, and DELETE. For databases, we typically have these 4 categories of operations so we can create new data points, update, read or delete them in our database. Bash Commands pip install cassandra-driver This is strictly for installation on Linux platforms, refer to the official website for more details.","title":"Install DataStax Python Cassandra Driver"},{"location":"database/setting_up_cluster/#single-node-cassandra-cluster","text":"Before we venture into the cool world of multi-node cluster requiring many servers, we will start with a single node cluster that only requires a single server (desktop/laptop). Once you've installed everything so far, you should run the following. Bash commands sudo service cassandra start sudo nodetool status And this will print out something like that: Datacenter : datacenter1 ============================= Status = Up / Down |/ State = Normal /Leaving/Joining/ Moving -- Address Load Tokens Owns ( effective ) Host ID Rack UN 127.0 . 0.1 318.78 KiB 256 100.0 % g5ac4c9 - 99 b7 - 65 d - 24 cfd82524f9 rack1 That's it, we've built our single node Cassandra database.","title":"Single Node Cassandra Cluster"},{"location":"database/setting_up_cluster/#multi-node-cassandra-cluster","text":"Remember it's standard to have at least 3 nodes, and in a basic 3 separate server configuration (3 separate desktops for non-enterprise users). Because of this, we will base this tutorial on setting up a 3-node cluster. But the same steps apply if you want to even do 10000 nodes. Installing Sublime critical Sublime is a text editor, and it would help if you're not familiar with VIM which I use frequently. Honestly using Sublime to edit all the following files we will edit is much easier, trust me! To install Sublime run the following bash commands in sequence: wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add - echo \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list sudo apt-get update sudo apt-get install sublime-text Now check if it works by running this command in your bash: subl This should open Sublime text editor! You're now ready. Before we move on to the steps to set up each node, we need the IP address of all 3 servers. This is simple, just run the following bash command: ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\\.){3}[0-9]*).*/\\2/p' This would return the server's IP address that we need for example: server_1_ip : 112.522 . 6.61 server_2_ip : 112.522 . 6.62 server_3_ip : 112.522 . 6.63 We will be using these IPs as a base for configuration in the subsequent sections. Take note yours would differ and you need to change accordingly.","title":"Multi-node Cassandra Cluster"},{"location":"database/setting_up_cluster/#steps-per-node","text":"Critical Section You Need To Repeat This is a critcal section. On EVERY server, you need to repeat all the steps shown in this section. In our case of a 3 node cluster, using 3 servers, we need to repeat this 3 times.","title":"Steps Per Node"},{"location":"database/setting_up_cluster/#step-1-modify-cassandra-configuration-settings","text":"Run the following bash to edit the configuration file for server 1 (112.522.6.61): cd /etc/cassandra subl cassandra.yaml Now you need to find the following fields and change them accordingly. cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" listen_address: 112.522.6.61 rpc_address: 112.522.6.61 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true","title":"Step 1: Modify Cassandra Configuration Settings"},{"location":"database/setting_up_cluster/#step-2-modify-rack-details","text":"Run the following bash command to edit the rack details: cd /etc/cassandra subl cassandra-rackdc.properties And change the following fields to this: dc=datacenter1","title":"Step 2: Modify Rack Details"},{"location":"database/setting_up_cluster/#step-3-check-status","text":"Restart service with the following bash commands: sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status Here you would see an output like this: Datacenter : datacenter ============================= Status = Up / Down |/ State = Normal /Leaving/Joining/ Moving -- Address Load Tokens Owns ( effective ) Host ID Rack UN 112.522 . 6.61 318.78 KiB 256 100.0 % f5c84c9 - 99 b7 - 45 d - 8856 - 24 cfd82523f9 rack1 UN 112.522 . 6.62 206.42 KiB 256 100.0 % ac2f24da - 1 b2c - 4 e3 - 8 a75 - 0 e28ec366a4 rack1 UN 112.522 . 6.63 338.34 KiB 256 100.0 % 55 a227d - ffbe - 45 d5 - 8698 - 60 c374b3a6b rack1","title":"Step 3: Check Status"},{"location":"database/setting_up_cluster/#step-4-configure-firewall-ip-settings","text":"Run the following bash commands sudo apt-get install iptables-persistent sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status","title":"Step 4: Configure Firewall IP Settings"},{"location":"database/setting_up_cluster/#repeat-steps-1-to-4","text":"You have to repeat all 4 steps for the other 2 servers but step 1 and step 4 requires slightly different settings. For the other servers, you need to basically change the IP to the server's IP. And for the IP firewall settings, you need to allow entry from the other 2 servers instead of itself.","title":"Repeat Steps 1 to 4"},{"location":"database/setting_up_cluster/#server-2-example-for-step-2","text":"cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.62 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.62 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true","title":"Server 2 Example for Step 2"},{"location":"database/setting_up_cluster/#server-3-example-for-step-2","text":"cluster_name: 'CassandraDBCluster' seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"112.522.6.61, 112.522.6.62, 112.522.6.63\" # ONLY THIS LINE CHANGES listen_address: 112.522.6.63 # ONLY THIS LINE CHANGES rpc_address: 112.522.6.63 endpoint_snitch: GossipingPropertyFileSnitch auto_bootstrap: true","title":"Server 3 Example for Step 2"},{"location":"database/setting_up_cluster/#server-2-example-for-step-4","text":"sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 3 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.63 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status","title":"Server 2 Example for Step 4"},{"location":"database/setting_up_cluster/#server-3-example-for-step-4","text":"sudo apt-get install iptables-persistent # ONLY THESE LINES CHANGE TO ALLOW COMMUNICATION WITH SERVER 1 AND 2 sudo iptables -A INPUT -p tcp -s 112.522.6.61 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp -s 112.522.6.62 -m multiport --dports 7000,9042 -m state --state NEW,ESTABLISHED -j ACCEPT sudo bash -c \"iptables-save > /etc/iptables/rules.v4\" sudo netfilter-persistent reload sudo nodetool status","title":"Server 3 Example for Step 4"},{"location":"database/setting_up_cluster/#dead-node-fix","text":"When your server/desktop restarts, you may face a dead node. You need replace the address with the same IP for it to work. cd /etc/cassandra subtl cassandra-env.sh Add the following line in the last row assuming server 1 is dead where the IP is 112.522.6.61 JVM_OPTS=\"$JVM_OPTS -Dcassandra.replace_address=112.522.6.61\" Then run the following bash commands sudo rm -rf /var/lib/cassandra/data/system/* sudo service cassandra restart sudo nodetool status You might have to wait awhile and re-run sudo nodetool status for the DB to get up and running.","title":"Dead Node Fix"},{"location":"database/setting_up_cluster/#summary","text":"We have successfully set up a 3-node Cassandra cluster DB after all these steps. We will move on to interacting with the cluster with CQLSH and the Python Driver in subsequent guides.","title":"Summary"},{"location":"database/setting_up_cluster/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/course_progression/","text":"Course Progression \u00b6 If you would like a smooth transition in learning deep learning concepts, you need to follow the materials in a sequential order. Some sections are still pending as I am working on them. 1. Practical Deep Learning with PyTorch \u00b6 Matrices Gradients Linear Regression Logistic Regression Feedforward Neural Networks (FNN) Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) Long Short Term Memory Neural Networks (LSTM) Autoencoders (AE) Variational Autoencoders (VAEs) Adversarial Autoencoders (AAEs) Generative Adversarial Networks (GANs) 2. Boosting Deep Learning Models with PyTorch \u00b6 Derivatives, Gradients and Jacobian Gradient Descent and Backpropagation Learning Rate Scheduling Optimizers Advanced Learning Rate Optimization Weight Initializations and Activation Functions Overfitting Prevention Loss, Accuracy and Weight Visualizations","title":"Course Progression"},{"location":"deep_learning/course_progression/#course-progression","text":"If you would like a smooth transition in learning deep learning concepts, you need to follow the materials in a sequential order. Some sections are still pending as I am working on them.","title":"Course Progression"},{"location":"deep_learning/course_progression/#1-practical-deep-learning-with-pytorch","text":"Matrices Gradients Linear Regression Logistic Regression Feedforward Neural Networks (FNN) Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) Long Short Term Memory Neural Networks (LSTM) Autoencoders (AE) Variational Autoencoders (VAEs) Adversarial Autoencoders (AAEs) Generative Adversarial Networks (GANs)","title":"1. Practical Deep Learning with PyTorch"},{"location":"deep_learning/course_progression/#2-boosting-deep-learning-models-with-pytorch","text":"Derivatives, Gradients and Jacobian Gradient Descent and Backpropagation Learning Rate Scheduling Optimizers Advanced Learning Rate Optimization Weight Initializations and Activation Functions Overfitting Prevention Loss, Accuracy and Weight Visualizations","title":"2. Boosting Deep Learning Models with PyTorch"},{"location":"deep_learning/intro/","text":"Deep Learning Theory and Programming Tutorials \u00b6 Our main open-source programming languages and libraries are Python, PyTorch, NumPy and C++. If you would like a more visual and guided experience, feel free to take our video course . Work in progress This open-source portion is still a work in progress, it is very sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page. Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier, proven-to-work, experience. All of our code allows you to run in a notebook for this deep learning section. Please use a jupyter notebook and run the examples from the start of the page to the end. Remember to use CTRL + if you would like to zoom into the diagrams if you find them too small or blur. You can also just right click and open the image in a new tab if you prefer. Errors to be corrected As we are rapidly prototyping there may be some errors. For these errors stated here, they will be corrected very soon. Feel free to report bugs, corrections or improvements on our Github repositor . If you did not go through all the materials, you would not be familiar with these, so go through them and come back to review these changes. For all diagrams that says dot product, they refer to matrix product. For all diagrams that says valid padding, they refer to no padding such that your output size will be smaller than your input size. For all diagrams that says same padding, they refer to zero padding (padding your input with zeroes) such that your output size will be equal to your input size. When we state linear function, more specifically we meant affine function that comprises a linear function and a constant.","title":"Introduction"},{"location":"deep_learning/intro/#deep-learning-theory-and-programming-tutorials","text":"Our main open-source programming languages and libraries are Python, PyTorch, NumPy and C++. If you would like a more visual and guided experience, feel free to take our video course . Work in progress This open-source portion is still a work in progress, it is very sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page. Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier, proven-to-work, experience. All of our code allows you to run in a notebook for this deep learning section. Please use a jupyter notebook and run the examples from the start of the page to the end. Remember to use CTRL + if you would like to zoom into the diagrams if you find them too small or blur. You can also just right click and open the image in a new tab if you prefer. Errors to be corrected As we are rapidly prototyping there may be some errors. For these errors stated here, they will be corrected very soon. Feel free to report bugs, corrections or improvements on our Github repositor . If you did not go through all the materials, you would not be familiar with these, so go through them and come back to review these changes. For all diagrams that says dot product, they refer to matrix product. For all diagrams that says valid padding, they refer to no padding such that your output size will be smaller than your input size. For all diagrams that says same padding, they refer to zero padding (padding your input with zeroes) such that your output size will be equal to your input size. When we state linear function, more specifically we meant affine function that comprises a linear function and a constant.","title":"Deep Learning Theory and Programming Tutorials"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/","text":"Derivative, Gradient and Jacobian \u00b6 Simplified Equation \u00b6 This is the simplified equation we have been using on how we update our parameters to reach good values (good local or global minima) \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation in English: parameters = parameters - learning_rate * parameters_gradients This process can be broken down into 2 sequential parts Backpropagation Gradient descent Simplified Equation Breakdown \u00b6 Our simplified equation can be broken down into 2 parts Backpropagation: getting our gradients Our partial derivatives of loss (scalar number) with respect to (w.r.t.) our model's parameters and w.r.t. our input Backpropagation gets us \\nabla_\\theta \\nabla_\\theta which is our gradient Gradient descent: using our gradients to update our parameters Somehow, the terms backpropagation and gradient descent are often mixed together. But they're totally different. Gradient descent relates to using our gradients obtained from backpropagation to update our weights. Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta Steps \u00b6 Derivatives Partial Derivatives Gradients Gradient, Jacobian and Generalized Jacobian Differences Backpropagation: computing gradients Gradient descent: using gradients to update parameters Derivative \u00b6 Given a simple cubic equation: f(x) = 2x^3 + 5 f(x) = 2x^3 + 5 Calculating the derivative \\frac{df(x)}{dx} \\frac{df(x)}{dx} is simply calculating the difference in values of y y for an extremely small (infinitesimally) change in value of x x which is frequently labelled as h h \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} is the slope formula similar to what you may be familiar with: Change in y y over change in x x : \\frac{\\Delta y}{\\Delta x} \\frac{\\Delta y}{\\Delta x} And the derivative is the slope when h \\rightarrow 0 h \\rightarrow 0 , in essence a super teeny small h h Let's break down \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2 \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2 Partial Derivative \u00b6 Ok, it's simple to calculate our derivative when we've only one variable in our function. If we've more than one (as with our parameters in our models), we need to calculate our partial derivatives of our function with respect to our variables Given a simple equation f(x, z) = 4x^4z^3 f(x, z) = 4x^4z^3 , let us get our partial derivatives 2 parts: partial derivative of our function w.r.t. x and z Partial derivative of our function w.r.t. x: \\frac{\\delta f(x, z)}{\\delta x} \\frac{\\delta f(x, z)}{\\delta x} Let z z term be a constant, a a f(x, z) = 4x^4a f(x, z) = 4x^4a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a Now we substitute a a with our z term, a = z^3 a = z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 Partial derivative of our function w.r.t. z: \\frac{\\delta f(x, z)}{\\delta z} \\frac{\\delta f(x, z)}{\\delta z} Let x x term be a constant, a a f(x, z) = 4az^3 f(x, z) = 4az^3 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 Now we substitute a a with our x x term, a = x^4 a = x^4 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 Ta da! We made it, we calculated our partial derivatives of our function w.r.t. the different variables Gradient \u00b6 We can now put all our partial derivatives into a vector of partial derivatives Also called \"gradient\" Represented by \\nabla_{(x,z)} \\nabla_{(x,z)} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} It is critical to note that the term gradient applies for f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Where our function maps a vector input to a scalar output: in deep learning, our loss function that produces a scalar loss Gradient, Jacobian, and Generalized Jacobian \u00b6 In the case where we have non-scalar outputs, these are the right terms of matrices or vectors containing our partial derivatives Gradient: vector input to scalar output f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Jacobian: vector input to vector output f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M Generalized Jacobian: tensor input to tensor output In this case, a tensor can be any number of dimensions. Summary \u00b6 We've learnt to... Success Calculate derivatives Calculate partial derivatives Get gradients Differentiate the concepts amongst gradients, Jacobian and Generalized Jacobian Now it is time to move on to backpropagation and gradient descent for a simple 1 hidden layer FNN with all these concepts in mind. Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Derivative, Gradient and Jacobian"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#derivative-gradient-and-jacobian","text":"","title":"Derivative, Gradient and Jacobian"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#simplified-equation","text":"This is the simplified equation we have been using on how we update our parameters to reach good values (good local or global minima) \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation in English: parameters = parameters - learning_rate * parameters_gradients This process can be broken down into 2 sequential parts Backpropagation Gradient descent","title":"Simplified Equation"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#simplified-equation-breakdown","text":"Our simplified equation can be broken down into 2 parts Backpropagation: getting our gradients Our partial derivatives of loss (scalar number) with respect to (w.r.t.) our model's parameters and w.r.t. our input Backpropagation gets us \\nabla_\\theta \\nabla_\\theta which is our gradient Gradient descent: using our gradients to update our parameters Somehow, the terms backpropagation and gradient descent are often mixed together. But they're totally different. Gradient descent relates to using our gradients obtained from backpropagation to update our weights. Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta","title":"Simplified Equation Breakdown"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#steps","text":"Derivatives Partial Derivatives Gradients Gradient, Jacobian and Generalized Jacobian Differences Backpropagation: computing gradients Gradient descent: using gradients to update parameters","title":"Steps"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#derivative","text":"Given a simple cubic equation: f(x) = 2x^3 + 5 f(x) = 2x^3 + 5 Calculating the derivative \\frac{df(x)}{dx} \\frac{df(x)}{dx} is simply calculating the difference in values of y y for an extremely small (infinitesimally) change in value of x x which is frequently labelled as h h \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} \\frac{f(x + h) - f(x)}{h} is the slope formula similar to what you may be familiar with: Change in y y over change in x x : \\frac{\\Delta y}{\\Delta x} \\frac{\\Delta y}{\\Delta x} And the derivative is the slope when h \\rightarrow 0 h \\rightarrow 0 , in essence a super teeny small h h Let's break down \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}} \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2 \\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2","title":"Derivative"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#partial-derivative","text":"Ok, it's simple to calculate our derivative when we've only one variable in our function. If we've more than one (as with our parameters in our models), we need to calculate our partial derivatives of our function with respect to our variables Given a simple equation f(x, z) = 4x^4z^3 f(x, z) = 4x^4z^3 , let us get our partial derivatives 2 parts: partial derivative of our function w.r.t. x and z Partial derivative of our function w.r.t. x: \\frac{\\delta f(x, z)}{\\delta x} \\frac{\\delta f(x, z)}{\\delta x} Let z z term be a constant, a a f(x, z) = 4x^4a f(x, z) = 4x^4a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a \\frac{\\delta f(x, z)}{\\delta x} = 16x^3a Now we substitute a a with our z term, a = z^3 a = z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 \\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3 Partial derivative of our function w.r.t. z: \\frac{\\delta f(x, z)}{\\delta z} \\frac{\\delta f(x, z)}{\\delta z} Let x x term be a constant, a a f(x, z) = 4az^3 f(x, z) = 4az^3 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 \\frac{\\delta f(x, z)}{\\delta z} = 12az^2 Now we substitute a a with our x x term, a = x^4 a = x^4 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 \\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2 Ta da! We made it, we calculated our partial derivatives of our function w.r.t. the different variables","title":"Partial Derivative"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#gradient","text":"We can now put all our partial derivatives into a vector of partial derivatives Also called \"gradient\" Represented by \\nabla_{(x,z)} \\nabla_{(x,z)} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} \\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix} It is critical to note that the term gradient applies for f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Where our function maps a vector input to a scalar output: in deep learning, our loss function that produces a scalar loss","title":"Gradient"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#gradient-jacobian-and-generalized-jacobian","text":"In the case where we have non-scalar outputs, these are the right terms of matrices or vectors containing our partial derivatives Gradient: vector input to scalar output f : \\mathbb{R}^N \\rightarrow \\mathbb{R} f : \\mathbb{R}^N \\rightarrow \\mathbb{R} Jacobian: vector input to vector output f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M Generalized Jacobian: tensor input to tensor output In this case, a tensor can be any number of dimensions.","title":"Gradient, Jacobian, and Generalized Jacobian"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#summary","text":"We've learnt to... Success Calculate derivatives Calculate partial derivatives Get gradients Differentiate the concepts amongst gradients, Jacobian and Generalized Jacobian Now it is time to move on to backpropagation and gradient descent for a simple 1 hidden layer FNN with all these concepts in mind.","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/derivative_gradient_jacobian/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/","text":"Forwardpropagation, Backpropagation and Gradient Descent with PyTorch \u00b6 Transiting to Backpropagation \u00b6 Let's go back to our simple FNN to put things in perspective Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): \\hat y = \\theta x \\hat y = \\theta x \\theta \\theta is our parameters x x is our input \\hat y \\hat y is our prediction Then we have our MSE loss function L = \\frac{1}{2} (\\hat y - y)^2 L = \\frac{1}{2} (\\hat y - y)^2 We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} With chain rule we have \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta \\hat y}{\\delta \\theta} is our partial derivatives of y y w.r.t. our parameters (our gradient) as we have covered previously Forward Propagation, Backward Propagation and Gradient Descent \u00b6 All right, now let's put together what we have learnt on backpropagation and apply it on a simple feedforward neural network (FNN) Let us assume the following simple FNN architecture and take note that we do not have bias here to keep things simple FNN architecture Linear function: hidden size = 32 Non-linear function: sigmoid Linear function: output size = 1 Non-linear function: sigmoid We will be going through a binary classification problem classifying 2 types of flowers Output size: 1 (represented by 0 or 1 depending on the flower) Input size: 2 (features of the flower) Number of training samples: 100 Load 3-class dataset We want to set a seed to encourage reproducibility so you can match our loss numbers. import torch import torch.nn as nn # Set manual seed torch . manual_seed ( 2 ) Here we want to load our flower classification dataset of 150 samples. There are 2 features, hence the input size would be 150x2. There is no one-hot encoding so the output would not be a size of 150x3 but a size of 150x1. from sklearn import datasets from sklearn import preprocessing iris = datasets . load_iris () X = torch . tensor ( preprocessing . normalize ( iris . data [:, : 2 ]), dtype = torch . float ) y = torch . tensor ( iris . target . reshape ( - 1 , 1 ), dtype = torch . float ) print ( X . size ()) print ( y . size ()) torch . Size ([ 150 , 2 ]) torch . Size ([ 150 , 1 ]) From 3 class dataset to 2 class dataset We only want 2 classes because we want a binary classification problem. As mentioned, there is no one-hot encoding, so each class is represented by 0, 1, or 2. All we need to do is to filter out all samples with a label of 2 to have 2 classes. # We only take 2 classes to make a binary classification problem X = X [: y [ y < 2 ] . size ()[ 0 ]] y = y [: y [ y < 2 ] . size ()[ 0 ]] ```` `` ` python print ( X . size ()) print ( y . size ()) torch . Size ([ 100 , 2 ]) torch . Size ([ 100 , 1 ]) Building our FNN model class from scratch class FNN ( nn . Module ): def __init__ ( self , ): super () . __init__ () # Dimensions for input, hidden and output self . input_dim = 2 self . hidden_dim = 32 self . output_dim = 1 # Learning rate definition self . learning_rate = 0.001 # Our parameters (weights) # w1: 2 x 32 self . w1 = torch . randn ( self . input_dim , self . hidden_dim ) # w2: 32 x 1 self . w2 = torch . randn ( self . hidden_dim , self . output_dim ) def sigmoid ( self , s ): return 1 / ( 1 + torch . exp ( - s )) def sigmoid_first_order_derivative ( self , s ): return s * ( 1 - s ) # Forward propagation def forward ( self , X ): # First linear layer self . y1 = torch . matmul ( X , self . w1 ) # 3 X 3 \".dot\" does not broadcast in PyTorch # First non-linearity self . y2 = self . sigmoid ( self . y1 ) # Second linear layer self . y3 = torch . matmul ( self . y2 , self . w2 ) # Second non-linearity y4 = self . sigmoid ( self . y3 ) return y4 # Backward propagation def backward ( self , X , l , y4 ): # Derivative of binary cross entropy cost w.r.t. final output y4 self . dC_dy4 = y4 - l ''' Gradients for w2: partial derivative of cost w.r.t. w2 dC/dw2 ''' self . dy4_dy3 = self . sigmoid_first_order_derivative ( y4 ) self . dy3_dw2 = self . y2 # Y4 delta: dC_dy4 dy4_dy3 self . y4_delta = self . dC_dy4 * self . dy4_dy3 # This is our gradients for w1: dC_dy4 dy4_dy3 dy3_dw2 self . dC_dw2 = torch . matmul ( torch . t ( self . dy3_dw2 ), self . y4_delta ) ''' Gradients for w1: partial derivative of cost w.r.t w1 dC/dw1 ''' self . dy3_dy2 = self . w2 self . dy2_dy1 = self . sigmoid_first_order_derivative ( self . y2 ) # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 self . y2_delta = torch . matmul ( self . y4_delta , torch . t ( self . dy3_dy2 )) * self . dy2_dy1 # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1 self . dC_dw1 = torch . matmul ( torch . t ( X ), self . y2_delta ) # Gradient descent on the weights from our 2 linear layers self . w1 -= self . learning_rate * self . dC_dw1 self . w2 -= self . learning_rate * self . dC_dw2 def train ( self , X , l ): # Forward propagation y4 = self . forward ( X ) # Backward propagation and gradient descent self . backward ( X , l , y4 ) Training our FNN model # Instantiate our model class and assign it to our model object model = FNN () # Loss list for plotting of loss behaviour loss_lst = [] # Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x num_epochs = 101 # Let's train our model with 100 epochs for epoch in range ( num_epochs ): # Get our predictions y_hat = model ( X ) # Cross entropy loss, remember this can never be negative by nature of the equation # But it does not mean the loss can't be negative for other loss functions cross_entropy_loss = - ( y * torch . log ( y_hat ) + ( 1 - y ) * torch . log ( 1 - y_hat )) # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset mean_cross_entropy_loss = torch . mean ( cross_entropy_loss ) . detach () . item () # Print our mean cross entropy loss if epoch % 20 == 0 : print ( 'Epoch {} | Loss: {}' . format ( epoch , mean_cross_entropy_loss )) loss_lst . append ( mean_cross_entropy_loss ) # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients) # (3) Gradient Descent: update our weights with our gradients model . train ( X , y ) Epoch 0 | Loss : 0.9228229522705078 Epoch 20 | Loss : 0.6966760754585266 Epoch 40 | Loss : 0.6714916229248047 Epoch 60 | Loss : 0.6686137914657593 Epoch 80 | Loss : 0.666690468788147 Epoch 100 | Loss : 0.6648102402687073 Our loss is decreasing gradually, so it's learning. It has a possibility of reducing to almost 0 (overfitting) with sufficient model capacity (more layers or wider layers). We will explore overfitting and learning rate optimization subsequently. Summary \u00b6 We've learnt... Success The math behind forwardpropagation, backwardpropagation and gradient descent for FNN Implement a basic FNN from scratch with PyTorch Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Forward- and Backward-propagation and Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#forwardpropagation-backpropagation-and-gradient-descent-with-pytorch","text":"","title":"Forwardpropagation, Backpropagation and Gradient Descent with PyTorch"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#transiting-to-backpropagation","text":"Let's go back to our simple FNN to put things in perspective Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): \\hat y = \\theta x \\hat y = \\theta x \\theta \\theta is our parameters x x is our input \\hat y \\hat y is our prediction Then we have our MSE loss function L = \\frac{1}{2} (\\hat y - y)^2 L = \\frac{1}{2} (\\hat y - y)^2 We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} \\nabla_{\\theta} = \\frac{\\delta L}{\\delta \\theta} With chain rule we have \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\theta} = \\frac{\\delta L}{\\delta \\hat y} \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta L}{\\delta \\hat y} = (\\hat y - y) \\frac{\\delta \\hat y}{\\delta \\theta} \\frac{\\delta \\hat y}{\\delta \\theta} is our partial derivatives of y y w.r.t. our parameters (our gradient) as we have covered previously","title":"Transiting to Backpropagation"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#forward-propagation-backward-propagation-and-gradient-descent","text":"All right, now let's put together what we have learnt on backpropagation and apply it on a simple feedforward neural network (FNN) Let us assume the following simple FNN architecture and take note that we do not have bias here to keep things simple FNN architecture Linear function: hidden size = 32 Non-linear function: sigmoid Linear function: output size = 1 Non-linear function: sigmoid We will be going through a binary classification problem classifying 2 types of flowers Output size: 1 (represented by 0 or 1 depending on the flower) Input size: 2 (features of the flower) Number of training samples: 100 Load 3-class dataset We want to set a seed to encourage reproducibility so you can match our loss numbers. import torch import torch.nn as nn # Set manual seed torch . manual_seed ( 2 ) Here we want to load our flower classification dataset of 150 samples. There are 2 features, hence the input size would be 150x2. There is no one-hot encoding so the output would not be a size of 150x3 but a size of 150x1. from sklearn import datasets from sklearn import preprocessing iris = datasets . load_iris () X = torch . tensor ( preprocessing . normalize ( iris . data [:, : 2 ]), dtype = torch . float ) y = torch . tensor ( iris . target . reshape ( - 1 , 1 ), dtype = torch . float ) print ( X . size ()) print ( y . size ()) torch . Size ([ 150 , 2 ]) torch . Size ([ 150 , 1 ]) From 3 class dataset to 2 class dataset We only want 2 classes because we want a binary classification problem. As mentioned, there is no one-hot encoding, so each class is represented by 0, 1, or 2. All we need to do is to filter out all samples with a label of 2 to have 2 classes. # We only take 2 classes to make a binary classification problem X = X [: y [ y < 2 ] . size ()[ 0 ]] y = y [: y [ y < 2 ] . size ()[ 0 ]] ```` `` ` python print ( X . size ()) print ( y . size ()) torch . Size ([ 100 , 2 ]) torch . Size ([ 100 , 1 ]) Building our FNN model class from scratch class FNN ( nn . Module ): def __init__ ( self , ): super () . __init__ () # Dimensions for input, hidden and output self . input_dim = 2 self . hidden_dim = 32 self . output_dim = 1 # Learning rate definition self . learning_rate = 0.001 # Our parameters (weights) # w1: 2 x 32 self . w1 = torch . randn ( self . input_dim , self . hidden_dim ) # w2: 32 x 1 self . w2 = torch . randn ( self . hidden_dim , self . output_dim ) def sigmoid ( self , s ): return 1 / ( 1 + torch . exp ( - s )) def sigmoid_first_order_derivative ( self , s ): return s * ( 1 - s ) # Forward propagation def forward ( self , X ): # First linear layer self . y1 = torch . matmul ( X , self . w1 ) # 3 X 3 \".dot\" does not broadcast in PyTorch # First non-linearity self . y2 = self . sigmoid ( self . y1 ) # Second linear layer self . y3 = torch . matmul ( self . y2 , self . w2 ) # Second non-linearity y4 = self . sigmoid ( self . y3 ) return y4 # Backward propagation def backward ( self , X , l , y4 ): # Derivative of binary cross entropy cost w.r.t. final output y4 self . dC_dy4 = y4 - l ''' Gradients for w2: partial derivative of cost w.r.t. w2 dC/dw2 ''' self . dy4_dy3 = self . sigmoid_first_order_derivative ( y4 ) self . dy3_dw2 = self . y2 # Y4 delta: dC_dy4 dy4_dy3 self . y4_delta = self . dC_dy4 * self . dy4_dy3 # This is our gradients for w1: dC_dy4 dy4_dy3 dy3_dw2 self . dC_dw2 = torch . matmul ( torch . t ( self . dy3_dw2 ), self . y4_delta ) ''' Gradients for w1: partial derivative of cost w.r.t w1 dC/dw1 ''' self . dy3_dy2 = self . w2 self . dy2_dy1 = self . sigmoid_first_order_derivative ( self . y2 ) # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 self . y2_delta = torch . matmul ( self . y4_delta , torch . t ( self . dy3_dy2 )) * self . dy2_dy1 # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1 self . dC_dw1 = torch . matmul ( torch . t ( X ), self . y2_delta ) # Gradient descent on the weights from our 2 linear layers self . w1 -= self . learning_rate * self . dC_dw1 self . w2 -= self . learning_rate * self . dC_dw2 def train ( self , X , l ): # Forward propagation y4 = self . forward ( X ) # Backward propagation and gradient descent self . backward ( X , l , y4 ) Training our FNN model # Instantiate our model class and assign it to our model object model = FNN () # Loss list for plotting of loss behaviour loss_lst = [] # Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x num_epochs = 101 # Let's train our model with 100 epochs for epoch in range ( num_epochs ): # Get our predictions y_hat = model ( X ) # Cross entropy loss, remember this can never be negative by nature of the equation # But it does not mean the loss can't be negative for other loss functions cross_entropy_loss = - ( y * torch . log ( y_hat ) + ( 1 - y ) * torch . log ( 1 - y_hat )) # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset mean_cross_entropy_loss = torch . mean ( cross_entropy_loss ) . detach () . item () # Print our mean cross entropy loss if epoch % 20 == 0 : print ( 'Epoch {} | Loss: {}' . format ( epoch , mean_cross_entropy_loss )) loss_lst . append ( mean_cross_entropy_loss ) # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients) # (3) Gradient Descent: update our weights with our gradients model . train ( X , y ) Epoch 0 | Loss : 0.9228229522705078 Epoch 20 | Loss : 0.6966760754585266 Epoch 40 | Loss : 0.6714916229248047 Epoch 60 | Loss : 0.6686137914657593 Epoch 80 | Loss : 0.666690468788147 Epoch 100 | Loss : 0.6648102402687073 Our loss is decreasing gradually, so it's learning. It has a possibility of reducing to almost 0 (overfitting) with sufficient model capacity (more layers or wider layers). We will explore overfitting and learning rate optimization subsequently.","title":"Forward Propagation, Backward Propagation and Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#summary","text":"We've learnt... Success The math behind forwardpropagation, backwardpropagation and gradient descent for FNN Implement a basic FNN from scratch with PyTorch","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/","text":"Learning Rate Scheduling \u00b6 Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD) \u00b6 We will be using mini-batch gradient descent in all our examples here when scheduling our learning rate Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Typically in deep learning, some variation of mini-batch gradient is used where the batch size is a hyperparameter to be determined Learning Intuition Recap \u00b6 Learning process Original parameters \\rightarrow \\rightarrow given input, get output \\rightarrow \\rightarrow compare with labels \\rightarrow \\rightarrow get loss with comparison of input/output \\rightarrow \\rightarrow get gradients of loss w.r.t parameters \\rightarrow \\rightarrow update parameters so model can churn output closer to labels \\rightarrow \\rightarrow repeat For a detailed mathematical account of how this works and how to implement from scratch in Python and PyTorch, you can read our forward- and back-propagation and gradient descent post . Learning Rate Pointers \u00b6 Update parameters so model can churn output closer to labels, lower loss \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) May take too long or unable to convert to a good local minima Need for Learning Rate Schedules \u00b6 Benefits Converge faster Higher accuracy Top Basic Learning Rate Schedules \u00b6 Step-wise Decay Reduce on Loss Plateau Decay Step-wise Learning Rate Decay \u00b6 Step-wise Decay: Every Epoch \u00b6 At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and $ \\gamma = 0.01$ Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 (0.1) = 0.01 \\eta_{t+1} = 0.1 (0.1) = 0.01 Epoch 2: \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 Epoch n: \\eta_{t+n} = 0.1 (0.1)^n \\eta_{t+n} = 0.1 (0.1)^n Code for step-wise learning rate decay at every epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 1 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.010000000000000002 ] Iteration : 1000. Loss : 0.1207798570394516 . Accuracy : 97 Epoch : 2 LR : [ 0.0010000000000000002 ] Iteration : 1500. Loss : 0.12287932634353638 . Accuracy : 97 Epoch : 3 LR : [ 0.00010000000000000003 ] Iteration : 2000. Loss : 0.05614742264151573 . Accuracy : 97 Epoch : 4 LR : [ 1.0000000000000003e-05 ] Iteration : 2500. Loss : 0.06775809079408646 . Accuracy : 97 Iteration : 3000. Loss : 0.03737065941095352 . Accuracy : 97 Step-wise Decay: Every 2 Epochs \u00b6 At every 2 epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.01 \\gamma = 0.01 Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 \\eta_{t+1} = 0.1 Epoch 2: \\eta_{t+2} = 0.1 (0.1) = 0.01 \\eta_{t+2} = 0.1 (0.1) = 0.01 Code for step-wise learning rate decay at every 2 epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.010000000000000002 ] Iteration : 1500. Loss : 0.14498558640480042 . Accuracy : 97 Epoch : 3 LR : [ 0.010000000000000002 ] Iteration : 2000. Loss : 0.03691177815198898 . Accuracy : 97 Epoch : 4 LR : [ 0.0010000000000000002 ] Iteration : 2500. Loss : 0.03511016443371773 . Accuracy : 97 Iteration : 3000. Loss : 0.029424520209431648 . Accuracy : 97 Step-wise Decay: Every Epoch, Larger Gamma \u00b6 At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.96 \\gamma = 0.96 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.96 \\gamma = 0.96 Epoch 1: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 2: \\eta_{t+1} = 0.1 (0.96) = 0.096 \\eta_{t+1} = 0.1 (0.96) = 0.096 Epoch 3: \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 Epoch n: \\eta_{t+n} = 0.1 (0.96)^n \\eta_{t+n} = 0.1 (0.96)^n Code for step-wise learning rate decay at every epoch with larger gamma import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.96 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.096 ] Iteration : 1500. Loss : 0.11864850670099258 . Accuracy : 97 Epoch : 3 LR : [ 0.096 ] Iteration : 2000. Loss : 0.030942382290959358 . Accuracy : 97 Epoch : 4 LR : [ 0.09216 ] Iteration : 2500. Loss : 0.04521659016609192 . Accuracy : 97 Iteration : 3000. Loss : 0.027839098125696182 . Accuracy : 97 Pointers on Step-wise Decay \u00b6 You would want to decay your LR gradually when you're training more epochs Converge too fast, to a crappy loss/accuracy, if you decay rapidly To decay slower Larger \\gamma \\gamma Larger interval of decay Reduce on Loss Plateau Decay \u00b6 Reduce on Loss Plateau Decay, Patience=0, Factor=0.1 \u00b6 Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.1 Optimization Algorithm: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay of factor 0.1 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.1 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {}. Accuracy: {}' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 1.0000e-02 . Epoch 3 completed Loss : 0.0324370414018631 . Accuracy : 97.7 -------------------- Epoch 4 completed Loss : 0.022194599732756615 . Accuracy : 98.02 -------------------- Epoch 5 completed Loss : 0.007145566865801811 . Accuracy : 98.03 -------------------- Epoch 6 completed Loss : 0.01673538237810135 . Accuracy : 98.05 -------------------- Epoch 7 completed Loss : 0.025424446910619736 . Accuracy : 98.01 -------------------- Epoch 7 : reducing learning rate of group 0 to 1.0000e-03 . Epoch 8 completed Loss : 0.014696130529046059 . Accuracy : 98.05 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.0000e-04 . Epoch 9 completed Loss : 0.00573748117312789 . Accuracy : 98.04 -------------------- Epoch 9 : reducing learning rate of group 0 to 1.0000e-05 . Reduce on Loss Plateau Decay, Patience=0, Factor=0.5 \u00b6 Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.5 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay with factor 0.5 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.5 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {}. Accuracy: {}' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 5.0000e-02 . Epoch 3 completed Loss : 0.04893001914024353 . Accuracy : 97.62 -------------------- Epoch 4 completed Loss : 0.020584167912602425 . Accuracy : 97.86 -------------------- Epoch 5 completed Loss : 0.006022400688380003 . Accuracy : 97.95 -------------------- Epoch 6 completed Loss : 0.028374142944812775 . Accuracy : 97.87 -------------------- Epoch 6 : reducing learning rate of group 0 to 2.5000e-02 . Epoch 7 completed Loss : 0.013204765506088734 . Accuracy : 98.0 -------------------- Epoch 8 completed Loss : 0.010137186385691166 . Accuracy : 97.95 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.2500e-02 . Epoch 9 completed Loss : 0.0035198689438402653 . Accuracy : 98.01 -------------------- Pointers on Reduce on Loss Pleateau Decay \u00b6 In these examples, we used patience=1 because we are running few epochs You should look at a larger patience such as 5 if for example you ran 500 epochs. You should experiment with 2 properties Patience Decay factor Summary \u00b6 We've learnt... Success Learning Rate Intuition Update parameters so model can churn output closer to labels Gradual parameter updates Learning Rate Pointers If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) Learning Rate Schedules Step-wise Decay Reduce on Loss Plateau Decay Step-wise Decay Every 1 epoch Every 2 epoch Every 1 epoch, larger gamma Step-wise Decay Pointers Decay LR gradually Larger \\gamma \\gamma Larger interval of decay (increase epoch) Reduce on Loss Plateau Decay Patience=0, Factor=1 Patience=0, Factor=0.5 Pointers on Reduce on Loss Plateau Decay Larger patience with more epochs 2 hyperparameters to experiment Patience Decay factor Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Learning Rate Scheduling"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-rate-scheduling","text":"","title":"Learning Rate Scheduling"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#optimization-algorithm-mini-batch-stochastic-gradient-descent-sgd","text":"We will be using mini-batch gradient descent in all our examples here when scheduling our learning rate Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Typically in deep learning, some variation of mini-batch gradient is used where the batch size is a hyperparameter to be determined","title":"Optimization Algorithm: Mini-batch Stochastic Gradient Descent (SGD)"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-intuition-recap","text":"Learning process Original parameters \\rightarrow \\rightarrow given input, get output \\rightarrow \\rightarrow compare with labels \\rightarrow \\rightarrow get loss with comparison of input/output \\rightarrow \\rightarrow get gradients of loss w.r.t parameters \\rightarrow \\rightarrow update parameters so model can churn output closer to labels \\rightarrow \\rightarrow repeat For a detailed mathematical account of how this works and how to implement from scratch in Python and PyTorch, you can read our forward- and back-propagation and gradient descent post .","title":"Learning Intuition Recap"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#learning-rate-pointers","text":"Update parameters so model can churn output closer to labels, lower loss \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) May take too long or unable to convert to a good local minima","title":"Learning Rate Pointers"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#need-for-learning-rate-schedules","text":"Benefits Converge faster Higher accuracy","title":"Need for Learning Rate Schedules"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#top-basic-learning-rate-schedules","text":"Step-wise Decay Reduce on Loss Plateau Decay","title":"Top Basic Learning Rate Schedules"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-learning-rate-decay","text":"","title":"Step-wise Learning Rate Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-epoch","text":"At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and $ \\gamma = 0.01$ Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 (0.1) = 0.01 \\eta_{t+1} = 0.1 (0.1) = 0.01 Epoch 2: \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 \\eta_{t+2} = 0.1 (0.1)^2 = 0.001 Epoch n: \\eta_{t+n} = 0.1 (0.1)^n \\eta_{t+n} = 0.1 (0.1)^n Code for step-wise learning rate decay at every epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 1 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.010000000000000002 ] Iteration : 1000. Loss : 0.1207798570394516 . Accuracy : 97 Epoch : 2 LR : [ 0.0010000000000000002 ] Iteration : 1500. Loss : 0.12287932634353638 . Accuracy : 97 Epoch : 3 LR : [ 0.00010000000000000003 ] Iteration : 2000. Loss : 0.05614742264151573 . Accuracy : 97 Epoch : 4 LR : [ 1.0000000000000003e-05 ] Iteration : 2500. Loss : 0.06775809079408646 . Accuracy : 97 Iteration : 3000. Loss : 0.03737065941095352 . Accuracy : 97","title":"Step-wise Decay: Every Epoch"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-2-epochs","text":"At every 2 epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.1 \\gamma = 0.1 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.01 \\gamma = 0.01 Epoch 0: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 1: \\eta_{t+1} = 0.1 \\eta_{t+1} = 0.1 Epoch 2: \\eta_{t+2} = 0.1 (0.1) = 0.01 \\eta_{t+2} = 0.1 (0.1) = 0.01 Code for step-wise learning rate decay at every 2 epoch import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.010000000000000002 ] Iteration : 1500. Loss : 0.14498558640480042 . Accuracy : 97 Epoch : 3 LR : [ 0.010000000000000002 ] Iteration : 2000. Loss : 0.03691177815198898 . Accuracy : 97 Epoch : 4 LR : [ 0.0010000000000000002 ] Iteration : 2500. Loss : 0.03511016443371773 . Accuracy : 97 Iteration : 3000. Loss : 0.029424520209431648 . Accuracy : 97","title":"Step-wise Decay: Every 2 Epochs"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#step-wise-decay-every-epoch-larger-gamma","text":"At every epoch, \\eta_t = \\eta_{t-1}\\gamma \\eta_t = \\eta_{t-1}\\gamma \\gamma = 0.96 \\gamma = 0.96 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Practical example Given \\eta_t = 0.1 \\eta_t = 0.1 and \\gamma = 0.96 \\gamma = 0.96 Epoch 1: \\eta_t = 0.1 \\eta_t = 0.1 Epoch 2: \\eta_{t+1} = 0.1 (0.96) = 0.096 \\eta_{t+1} = 0.1 (0.96) = 0.096 Epoch 3: \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 \\eta_{t+2} = 0.1 (0.96)^2 = 0.092 Epoch n: \\eta_{t+n} = 0.1 (0.96)^n \\eta_{t+n} = 0.1 (0.96)^n Code for step-wise learning rate decay at every epoch with larger gamma import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 2 , gamma = 0.96 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500. Loss : 0.15292978286743164 . Accuracy : 96 Epoch : 1 LR : [ 0.1 ] Iteration : 1000. Loss : 0.11253029108047485 . Accuracy : 96 Epoch : 2 LR : [ 0.096 ] Iteration : 1500. Loss : 0.11864850670099258 . Accuracy : 97 Epoch : 3 LR : [ 0.096 ] Iteration : 2000. Loss : 0.030942382290959358 . Accuracy : 97 Epoch : 4 LR : [ 0.09216 ] Iteration : 2500. Loss : 0.04521659016609192 . Accuracy : 97 Iteration : 3000. Loss : 0.027839098125696182 . Accuracy : 97","title":"Step-wise Decay: Every Epoch, Larger Gamma"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#pointers-on-step-wise-decay","text":"You would want to decay your LR gradually when you're training more epochs Converge too fast, to a crappy loss/accuracy, if you decay rapidly To decay slower Larger \\gamma \\gamma Larger interval of decay","title":"Pointers on Step-wise Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay","text":"","title":"Reduce on Loss Plateau Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor01","text":"Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.1 Optimization Algorithm: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay of factor 0.1 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.1 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {}. Accuracy: {}' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 1.0000e-02 . Epoch 3 completed Loss : 0.0324370414018631 . Accuracy : 97.7 -------------------- Epoch 4 completed Loss : 0.022194599732756615 . Accuracy : 98.02 -------------------- Epoch 5 completed Loss : 0.007145566865801811 . Accuracy : 98.03 -------------------- Epoch 6 completed Loss : 0.01673538237810135 . Accuracy : 98.05 -------------------- Epoch 7 completed Loss : 0.025424446910619736 . Accuracy : 98.01 -------------------- Epoch 7 : reducing learning rate of group 0 to 1.0000e-03 . Epoch 8 completed Loss : 0.014696130529046059 . Accuracy : 98.05 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.0000e-04 . Epoch 9 completed Loss : 0.00573748117312789 . Accuracy : 98.04 -------------------- Epoch 9 : reducing learning rate of group 0 to 1.0000e-05 .","title":"Reduce on Loss Plateau Decay, Patience=0, Factor=0.1"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor05","text":"Reduce learning rate whenever loss plateaus Patience: number of epochs with no improvement after which learning rate will be reduced Patience = 0 Factor: multiplier to decrease learning rate, lr = lr*factor = \\gamma lr = lr*factor = \\gamma Factor = 0.5 Optimization Algorithm 4: SGD Nesterov Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Code for reduce on loss plateau learning rate decay with factor 0.5 and 0 patience import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) # Where to add a new import from torch.optim.lr_scheduler import ReduceLROnPlateau ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 6000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # lr = lr * factor # mode='max': look for the maximum validation accuracy to track # patience: number of epochs - 1 where loss plateaus before decreasing LR # patience = 0, after 1 bad epoch, reduce LR # factor = decaying factor scheduler = ReduceLROnPlateau ( optimizer , mode = 'max' , factor = 0.5 , patience = 0 , verbose = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions # Without .item(), it is a uint8 tensor which will not work when you pass this number to the scheduler correct += ( predicted == labels ) . sum () . item () accuracy = 100 * correct / total # Print Loss # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy)) # Decay Learning Rate, pass validation accuracy for tracking at every epoch print ( 'Epoch {} completed' . format ( epoch )) print ( 'Loss: {}. Accuracy: {}' . format ( loss . item (), accuracy )) print ( '-' * 20 ) scheduler . step ( accuracy ) Epoch 0 completed Loss : 0.17087846994400024 . Accuracy : 96.26 -------------------- Epoch 1 completed Loss : 0.11688263714313507 . Accuracy : 96.96 -------------------- Epoch 2 completed Loss : 0.035437121987342834 . Accuracy : 96.78 -------------------- Epoch 2 : reducing learning rate of group 0 to 5.0000e-02 . Epoch 3 completed Loss : 0.04893001914024353 . Accuracy : 97.62 -------------------- Epoch 4 completed Loss : 0.020584167912602425 . Accuracy : 97.86 -------------------- Epoch 5 completed Loss : 0.006022400688380003 . Accuracy : 97.95 -------------------- Epoch 6 completed Loss : 0.028374142944812775 . Accuracy : 97.87 -------------------- Epoch 6 : reducing learning rate of group 0 to 2.5000e-02 . Epoch 7 completed Loss : 0.013204765506088734 . Accuracy : 98.0 -------------------- Epoch 8 completed Loss : 0.010137186385691166 . Accuracy : 97.95 -------------------- Epoch 8 : reducing learning rate of group 0 to 1.2500e-02 . Epoch 9 completed Loss : 0.0035198689438402653 . Accuracy : 98.01 --------------------","title":"Reduce on Loss Plateau Decay, Patience=0, Factor=0.5"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#pointers-on-reduce-on-loss-pleateau-decay","text":"In these examples, we used patience=1 because we are running few epochs You should look at a larger patience such as 5 if for example you ran 500 epochs. You should experiment with 2 properties Patience Decay factor","title":"Pointers on Reduce on Loss Pleateau Decay"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#summary","text":"We've learnt... Success Learning Rate Intuition Update parameters so model can churn output closer to labels Gradual parameter updates Learning Rate Pointers If we set \\eta \\eta to be a large value \\rightarrow \\rightarrow learn too much (rapid learning) If we set \\eta \\eta to be a small value \\rightarrow \\rightarrow learn too little (slow learning) Learning Rate Schedules Step-wise Decay Reduce on Loss Plateau Decay Step-wise Decay Every 1 epoch Every 2 epoch Every 1 epoch, larger gamma Step-wise Decay Pointers Decay LR gradually Larger \\gamma \\gamma Larger interval of decay (increase epoch) Reduce on Loss Plateau Decay Patience=0, Factor=1 Patience=0, Factor=0.5 Pointers on Reduce on Loss Plateau Decay Larger patience with more epochs 2 hyperparameters to experiment Patience Decay factor","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/lr_scheduling/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/optimizers/","text":"Optimization Algorithms \u00b6 Introduction to Gradient-descent Optimizers \u00b6 Model Recap: 1 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95 Non-Technical Process \u00b6 Convert inputs/labels to variables Clear gradient buffers Get output given inputs Get loss by comparing with labels Get gradients w.r.t. parameters (backpropagation) Update parameters using gradients (gradient descent) parameters = parameters - learning_rate * parameters_gradients REPEAT Why is it called Gradient Descent? \u00b6 Use gradients (calculated through backpropagation) \\rightarrow \\rightarrow update parameters to minimize our loss (descent) \\rightarrow \\rightarrow better predictive accuracy Mathematical Interpretation of Gradient Descent \u00b6 Model's parameters: \\theta \\in \u211d^d \\theta \\in \u211d^d Loss function: J(\\theta) J(\\theta) Gradient w.r.t. parameters: \\nabla J(\\theta) \\nabla J(\\theta) Learning rate: \\eta \\eta Batch Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Optimization Algorithm 1: Batch Gradient Descent \u00b6 What we've covered so far: batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Characteristics Compute the gradient of the lost function w.r.t. parameters for the entire training data, \\nabla J(\\theta) \\nabla J(\\theta) Use this to update our parameters at every iteration Problems Unable to fit whole datasets in memory Computationally slow as we attempt to compute a large gradient matrix \\rightarrow \\rightarrow first order derivative, \\nabla J(\\theta) \\nabla J(\\theta) Conceptually easy to understand \\rightarrow \\rightarrow rarely used Optimization Algorithm 2: Stochastic Gradient Descent \u00b6 Modification of batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) Characteristics Compute the gradient of the lost function w.r.t. parameters for the one set of training sample (1 input and 1 label) , \\nabla J(\\theta, x^{i}, y^{i}) \\nabla J(\\theta, x^{i}, y^{i}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Problems Updating very frequently \\rightarrow \\rightarrow huge variance in parameter updates \\rightarrow \\rightarrow may overshoot local minima Can be solved by carefully decaying your learning rate \\rightarrow \\rightarrow take smaller steps in incorporating gradients to improve the parameters Optimization Algorithm 3: Mini-batch Gradient Descent \u00b6 Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Lower variance of parameter updates This is often called SGD in deep learning frameworks .__. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95 Optimization Algorithm 4: SGD Momentum \u00b6 Modification of SGD v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Parameters updated with update vector, v_t v_t that incorporates previous update vector \\gamma v_{t} \\gamma v_{t} increases if gradient same sign/direction as v_{t-1} v_{t-1} Gives SGD the push when it is going in the right direction (minimizing loss) Accelerated convergence \\gamma v_{t} \\gamma v_{t} decreases if gradient different sign/direction as v_{t-1} v_{t-1} Dampens SGD when it is going in a different direction Lower variation in loss minimization Problems It might go the wrong direction (higher loss) \\rightarrow \\rightarrow continue to be accelerated to the wrong direction (higher loss) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.16120098531246185 . Accuracy : 96 Iteration : 1000 . Loss : 0.15727552771568298 . Accuracy : 96 Iteration : 1500 . Loss : 0.1303034871816635 . Accuracy : 96 Iteration : 2000 . Loss : 0.022178759798407555 . Accuracy : 97 Iteration : 2500 . Loss : 0.07027597725391388 . Accuracy : 97 Iteration : 3000 . Loss : 0.02519878000020981 . Accuracy : 97 Optimization Algorithm 4: SGD Nesterov \u00b6 Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. future approximate parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Gradients w.r.t. future approximate parameters \\rightarrow \\rightarrow sense of where we will be \\rightarrow \\rightarrow anticipate if we are going in the wrong direction in the next step \\rightarrow \\rightarrow slow down accordingly import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.15292978286743164 . Accuracy : 96 Iteration : 1000 . Loss : 0.11253029108047485 . Accuracy : 96 Iteration : 1500 . Loss : 0.11986596137285233 . Accuracy : 96 Iteration : 2000 . Loss : 0.016192540526390076 . Accuracy : 97 Iteration : 2500 . Loss : 0.06744947284460068 . Accuracy : 97 Iteration : 3000 . Loss : 0.03692319989204407 . Accuracy : 97 Optimization Algorithm 4: Adam \u00b6 Adaptive Learning Rates m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t Keeping track of decaying gradient Estimate of the mean of gradients v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 Keeping track of decaying squared gradient Estimate of the variance of gradients When m_t, v_t m_t, v_t initializes as 0, m_t, v_t \\rightarrow 0 m_t, v_t \\rightarrow 0 initially when decay rates small, \\beta_1, \\beta_2 \\rightarrow 1 \\beta_1, \\beta_2 \\rightarrow 1 Need to correct this with: \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t Default recommended values \\beta_1 = 0.9 \\beta_1 = 0.9 \\beta_2 = 0.999 \\beta_2 = 0.999 \\epsilon = 10^{-8} \\epsilon = 10^{-8} Instead of learning rate \\rightarrow \\rightarrow equations account for estimates of mean/variance of gradients to determine the next learning rate import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adam ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2703690826892853 . Accuracy : 93 Iteration : 1000 . Loss : 0.15547044575214386 . Accuracy : 95 Iteration : 1500 . Loss : 0.17266806960105896 . Accuracy : 95 Iteration : 2000 . Loss : 0.0865858644247055 . Accuracy : 96 Iteration : 2500 . Loss : 0.07156120240688324 . Accuracy : 96 Iteration : 3000 . Loss : 0.04664849117398262 . Accuracy : 97 Other Adaptive Algorithms \u00b6 Other adaptive algorithms (like Adam, adapting learning rates) Adagrad Adadelta Adamax RMSProp Optimization Algorithm 5: Adagrad \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adagrad ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2757369875907898 . Accuracy : 92 Iteration : 1000 . Loss : 0.1992958039045334 . Accuracy : 93 Iteration : 1500 . Loss : 0.2227272093296051 . Accuracy : 94 Iteration : 2000 . Loss : 0.18628711998462677 . Accuracy : 94 Iteration : 2500 . Loss : 0.1470586657524109 . Accuracy : 95 Iteration : 3000 . Loss : 0.11748368293046951 . Accuracy : 95 Optimization Algorithm 6: Adadelta \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adadelta ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = Variable ( images . view ( - 1 , 28 * 28 )) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . data [ 0 ], accuracy )) Iteration : 500 . Loss : 0.26303035020828247 . Accuracy : 93.95 Iteration : 1000 . Loss : 0.08731874823570251 . Accuracy : 95.83 Iteration : 1500 . Loss : 0.11502093076705933 . Accuracy : 96.87 Iteration : 2000 . Loss : 0.03550947830080986 . Accuracy : 97.12 Iteration : 2500 . Loss : 0.042649827897548676 . Accuracy : 97.54 Iteration : 3000 . Loss : 0.03061559610068798 . Accuracy : 97.45 Optimization Algorithm 6: Adamax \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adamax ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.29930350184440613 . Accuracy : 92 Iteration : 1000 . Loss : 0.18749120831489563 . Accuracy : 93 Iteration : 1500 . Loss : 0.21887679398059845 . Accuracy : 95 Iteration : 2000 . Loss : 0.14390651881694794 . Accuracy : 95 Iteration : 2500 . Loss : 0.10771607607603073 . Accuracy : 96 Iteration : 3000 . Loss : 0.0839928686618805 . Accuracy : 96 Optimization Algorithm 7: RMSProp \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . RMSprop ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.25550296902656555 . Accuracy : 95 Iteration : 1000 . Loss : 0.17357593774795532 . Accuracy : 93 Iteration : 1500 . Loss : 0.10597744584083557 . Accuracy : 96 Iteration : 2000 . Loss : 0.03807783126831055 . Accuracy : 96 Iteration : 2500 . Loss : 0.10654022544622421 . Accuracy : 96 Iteration : 3000 . Loss : 0.05745543912053108 . Accuracy : 96 Summary of Optimization Algorithms Performance \u00b6 SGD: 95.78% SGD Momentum: 97.69% SGD Nesterov: 97.58% Adam: 97.20% Adagrad: 95.51% Adadelta: 97.45% Adamax: 96.58% RMSProp: 97.1% Performance is not definitive here I have used a seed to ensure you can reproduce results here. However, if you change the seed number you would realize that the performance of these optimization algorithms would change. A solution is to run each optimization on many seeds and get the average performance. Then you can compare the mean performance across all optimization algorithms. There are a lot of other factors like how Adam and SGD Momentum may have different ideal starting learning rates and require different learning rate scheduling. But off the hand, SGD and Adam are very robust optimization algorithms that you can rely on. Subsequently, we will look into more advanced optimization algorithms that are based mainly on SGD and Adam. Simple Suggestions \u00b6 Momentum/Nesterov Powerful if we control the learning rate schedule Adam Lazy to control the learning rate schedule Summary \u00b6 We've learnt... Success Recap of 7 step process Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 6 Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients Gradient descent Using gradients (error signals from loss class) to update parameters Mathematical interpretation: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Optimisation Algorithms Batch gradient descent Stochastic gradient descent Mini-batch gradient descent (SGD) SGD + Momentum SGD + Nesterov Adam Other adaptive algorithms: adagrad, adamax, adadelta, RMSProp Recommendations SGD+M SGD+N Adam Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Optimization Algorithms"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithms","text":"","title":"Optimization Algorithms"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#introduction-to-gradient-descent-optimizers","text":"","title":"Introduction to Gradient-descent Optimizers"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#model-recap-1-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model Recap: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#non-technical-process","text":"Convert inputs/labels to variables Clear gradient buffers Get output given inputs Get loss by comparing with labels Get gradients w.r.t. parameters (backpropagation) Update parameters using gradients (gradient descent) parameters = parameters - learning_rate * parameters_gradients REPEAT","title":"Non-Technical Process"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#why-is-it-called-gradient-descent","text":"Use gradients (calculated through backpropagation) \\rightarrow \\rightarrow update parameters to minimize our loss (descent) \\rightarrow \\rightarrow better predictive accuracy","title":"Why is it called Gradient Descent?"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#mathematical-interpretation-of-gradient-descent","text":"Model's parameters: \\theta \\in \u211d^d \\theta \\in \u211d^d Loss function: J(\\theta) J(\\theta) Gradient w.r.t. parameters: \\nabla J(\\theta) \\nabla J(\\theta) Learning rate: \\eta \\eta Batch Gradient descent: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta)","title":"Mathematical Interpretation of Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-1-batch-gradient-descent","text":"What we've covered so far: batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Characteristics Compute the gradient of the lost function w.r.t. parameters for the entire training data, \\nabla J(\\theta) \\nabla J(\\theta) Use this to update our parameters at every iteration Problems Unable to fit whole datasets in memory Computationally slow as we attempt to compute a large gradient matrix \\rightarrow \\rightarrow first order derivative, \\nabla J(\\theta) \\nabla J(\\theta) Conceptually easy to understand \\rightarrow \\rightarrow rarely used","title":"Optimization Algorithm 1: Batch Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-2-stochastic-gradient-descent","text":"Modification of batch gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i}, y^{i}) Characteristics Compute the gradient of the lost function w.r.t. parameters for the one set of training sample (1 input and 1 label) , \\nabla J(\\theta, x^{i}, y^{i}) \\nabla J(\\theta, x^{i}, y^{i}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Problems Updating very frequently \\rightarrow \\rightarrow huge variance in parameter updates \\rightarrow \\rightarrow may overshoot local minima Can be solved by carefully decaying your learning rate \\rightarrow \\rightarrow take smaller steps in incorporating gradients to improve the parameters","title":"Optimization Algorithm 2: Stochastic Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-3-mini-batch-gradient-descent","text":"Combination of batch gradient descent & stochastic gradient descent \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to update our parameters at every iteration Benefits Able to fit large datasets Computationally faster \\rightarrow \\rightarrow instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample Lower variance of parameter updates This is often called SGD in deep learning frameworks .__. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.3440718352794647 . Accuracy : 91 Iteration : 1000 . Loss : 0.2057694047689438 . Accuracy : 93 Iteration : 1500 . Loss : 0.2646750807762146 . Accuracy : 94 Iteration : 2000 . Loss : 0.17563636600971222 . Accuracy : 94 Iteration : 2500 . Loss : 0.1361844837665558 . Accuracy : 95 Iteration : 3000 . Loss : 0.11089023947715759 . Accuracy : 95","title":"Optimization Algorithm 3: Mini-batch Gradient Descent"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-sgd-momentum","text":"Modification of SGD v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Parameters updated with update vector, v_t v_t that incorporates previous update vector \\gamma v_{t} \\gamma v_{t} increases if gradient same sign/direction as v_{t-1} v_{t-1} Gives SGD the push when it is going in the right direction (minimizing loss) Accelerated convergence \\gamma v_{t} \\gamma v_{t} decreases if gradient different sign/direction as v_{t-1} v_{t-1} Dampens SGD when it is going in a different direction Lower variation in loss minimization Problems It might go the wrong direction (higher loss) \\rightarrow \\rightarrow continue to be accelerated to the wrong direction (higher loss) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.16120098531246185 . Accuracy : 96 Iteration : 1000 . Loss : 0.15727552771568298 . Accuracy : 96 Iteration : 1500 . Loss : 0.1303034871816635 . Accuracy : 96 Iteration : 2000 . Loss : 0.022178759798407555 . Accuracy : 97 Iteration : 2500 . Loss : 0.07027597725391388 . Accuracy : 97 Iteration : 3000 . Loss : 0.02519878000020981 . Accuracy : 97","title":"Optimization Algorithm 4: SGD Momentum"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-sgd-nesterov","text":"Modification of SGD Momentum v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\theta = \\theta - v_t \\theta = \\theta - v_t Characteristics Compute the gradient of the lost function w.r.t. future approximate parameters for n sets of training sample (n input and n label) , \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n}) Use this to add to the previous update vector v_{t-1} v_{t-1} Momentum, usually set to \\gamma = 0.9 \\gamma = 0.9 Gradients w.r.t. future approximate parameters \\rightarrow \\rightarrow sense of where we will be \\rightarrow \\rightarrow anticipate if we are going in the wrong direction in the next step \\rightarrow \\rightarrow slow down accordingly import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.15292978286743164 . Accuracy : 96 Iteration : 1000 . Loss : 0.11253029108047485 . Accuracy : 96 Iteration : 1500 . Loss : 0.11986596137285233 . Accuracy : 96 Iteration : 2000 . Loss : 0.016192540526390076 . Accuracy : 97 Iteration : 2500 . Loss : 0.06744947284460068 . Accuracy : 97 Iteration : 3000 . Loss : 0.03692319989204407 . Accuracy : 97","title":"Optimization Algorithm 4: SGD Nesterov"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-4-adam","text":"Adaptive Learning Rates m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t Keeping track of decaying gradient Estimate of the mean of gradients v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 Keeping track of decaying squared gradient Estimate of the variance of gradients When m_t, v_t m_t, v_t initializes as 0, m_t, v_t \\rightarrow 0 m_t, v_t \\rightarrow 0 initially when decay rates small, \\beta_1, \\beta_2 \\rightarrow 1 \\beta_1, \\beta_2 \\rightarrow 1 Need to correct this with: \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat m_t = \\frac{m_t}{1- \\beta_1} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\hat v_t = \\frac{v_t}{1- \\beta_2} \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t Default recommended values \\beta_1 = 0.9 \\beta_1 = 0.9 \\beta_2 = 0.999 \\beta_2 = 0.999 \\epsilon = 10^{-8} \\epsilon = 10^{-8} Instead of learning rate \\rightarrow \\rightarrow equations account for estimates of mean/variance of gradients to determine the next learning rate import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adam ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2703690826892853 . Accuracy : 93 Iteration : 1000 . Loss : 0.15547044575214386 . Accuracy : 95 Iteration : 1500 . Loss : 0.17266806960105896 . Accuracy : 95 Iteration : 2000 . Loss : 0.0865858644247055 . Accuracy : 96 Iteration : 2500 . Loss : 0.07156120240688324 . Accuracy : 96 Iteration : 3000 . Loss : 0.04664849117398262 . Accuracy : 97","title":"Optimization Algorithm 4: Adam"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#other-adaptive-algorithms","text":"Other adaptive algorithms (like Adam, adapting learning rates) Adagrad Adadelta Adamax RMSProp","title":"Other Adaptive Algorithms"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-5-adagrad","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adagrad ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.2757369875907898 . Accuracy : 92 Iteration : 1000 . Loss : 0.1992958039045334 . Accuracy : 93 Iteration : 1500 . Loss : 0.2227272093296051 . Accuracy : 94 Iteration : 2000 . Loss : 0.18628711998462677 . Accuracy : 94 Iteration : 2500 . Loss : 0.1470586657524109 . Accuracy : 95 Iteration : 3000 . Loss : 0.11748368293046951 . Accuracy : 95","title":"Optimization Algorithm 5: Adagrad"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-6-adadelta","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adadelta ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = Variable ( images . view ( - 1 , 28 * 28 )) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . data [ 0 ], accuracy )) Iteration : 500 . Loss : 0.26303035020828247 . Accuracy : 93.95 Iteration : 1000 . Loss : 0.08731874823570251 . Accuracy : 95.83 Iteration : 1500 . Loss : 0.11502093076705933 . Accuracy : 96.87 Iteration : 2000 . Loss : 0.03550947830080986 . Accuracy : 97.12 Iteration : 2500 . Loss : 0.042649827897548676 . Accuracy : 97.54 Iteration : 3000 . Loss : 0.03061559610068798 . Accuracy : 97.45","title":"Optimization Algorithm 6: Adadelta"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-6-adamax","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . Adamax ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.29930350184440613 . Accuracy : 92 Iteration : 1000 . Loss : 0.18749120831489563 . Accuracy : 93 Iteration : 1500 . Loss : 0.21887679398059845 . Accuracy : 95 Iteration : 2000 . Loss : 0.14390651881694794 . Accuracy : 95 Iteration : 2500 . Loss : 0.10771607607603073 . Accuracy : 96 Iteration : 3000 . Loss : 0.0839928686618805 . Accuracy : 96","title":"Optimization Algorithm 6: Adamax"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#optimization-algorithm-7-rmsprop","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets # Set seed torch . manual_seed ( 0 ) ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' # learning_rate = 0.001 optimizer = torch . optim . RMSprop ( model . parameters ()) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.25550296902656555 . Accuracy : 95 Iteration : 1000 . Loss : 0.17357593774795532 . Accuracy : 93 Iteration : 1500 . Loss : 0.10597744584083557 . Accuracy : 96 Iteration : 2000 . Loss : 0.03807783126831055 . Accuracy : 96 Iteration : 2500 . Loss : 0.10654022544622421 . Accuracy : 96 Iteration : 3000 . Loss : 0.05745543912053108 . Accuracy : 96","title":"Optimization Algorithm 7: RMSProp"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#summary-of-optimization-algorithms-performance","text":"SGD: 95.78% SGD Momentum: 97.69% SGD Nesterov: 97.58% Adam: 97.20% Adagrad: 95.51% Adadelta: 97.45% Adamax: 96.58% RMSProp: 97.1% Performance is not definitive here I have used a seed to ensure you can reproduce results here. However, if you change the seed number you would realize that the performance of these optimization algorithms would change. A solution is to run each optimization on many seeds and get the average performance. Then you can compare the mean performance across all optimization algorithms. There are a lot of other factors like how Adam and SGD Momentum may have different ideal starting learning rates and require different learning rate scheduling. But off the hand, SGD and Adam are very robust optimization algorithms that you can rely on. Subsequently, we will look into more advanced optimization algorithms that are based mainly on SGD and Adam.","title":"Summary of Optimization Algorithms Performance"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#simple-suggestions","text":"Momentum/Nesterov Powerful if we control the learning rate schedule Adam Lazy to control the learning rate schedule","title":"Simple Suggestions"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#summary","text":"We've learnt... Success Recap of 7 step process Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 6 Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients Gradient descent Using gradients (error signals from loss class) to update parameters Mathematical interpretation: \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta) Optimisation Algorithms Batch gradient descent Stochastic gradient descent Mini-batch gradient descent (SGD) SGD + Momentum SGD + Nesterov Adam Other adaptive algorithms: adagrad, adamax, adadelta, RMSProp Recommendations SGD+M SGD+N Adam","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/optimizers/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/","text":"Weight Initializations & Activation Functions \u00b6 Recap of Logistic Regression \u00b6 Recap of Feedforward Neural Network Activation Function \u00b6 Sigmoid (Logistic) \u00b6 \\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this import matplotlib.pyplot as plt % matplotlib inline import numpy as np def sigmoid ( x ): a = [] for item in x : a . append ( 1 / ( 1 + np . exp ( - item ))) return a x = np . arange ( - 10. , 10. , 0.2 ) sig = sigmoid ( x ) plt . style . use ( 'ggplot' ) plt . plot ( x , sig , linewidth = 3.0 ) Tanh \u00b6 \\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this x = np . arange ( - 10. , 10. , 0.2 ) tanh = np . dot ( 2 , sigmoid ( np . dot ( 2 , x ))) - 1 plt . plot ( x , tanh , linewidth = 3.0 ) ReLUs \u00b6 f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate and weight initialization choice x = np . arange ( - 10. , 10. , 0.2 ) relu = np . maximum ( x , 0 ) plt . plot ( x , relu , linewidth = 3.0 ) Why do we need weight initializations or new activation functions? \u00b6 To prevent vanishing/exploding gradients Case 1: Sigmoid/Tanh \u00b6 Problem If variance of input too large: gradients = 0 (vanishing gradients) If variance of input too small: linear \\rightarrow \\rightarrow gradients = constant value Solutions Want a constant variance of input to achieve non-linearity \\rightarrow \\rightarrow unique gradients for unique updates Xavier Initialization (good constant variance for Sigmoid/Tanh) ReLU or Leaky ReLU Case 2: ReLU \u00b6 Solution to Case 1 Regardless of variance of input: gradients = 0 or 1 Problem But those with 0: no updates (\"dead ReLU units\") Has unlimited output size with input > 0 (explodes gradients subsequently) Solutions He Initialization (good constant variance) Leaky ReLU Case 3: Leaky ReLU \u00b6 Solution to Case 2 Solves the 0 signal issue when input < 0 Problem Has unlimited output size with input > 0 (explodes) Solution He Initialization (good constant variance) Summary of weight initialization solutions to activations \u00b6 Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization Good range of constant variance ReLU/Leaky ReLU exploding gradients can be solved with He initialization Good range of constant variance Types of weight intializations \u00b6 Zero Initialization: set all weights to 0 \u00b6 Every neuron in the network computes the same output \\rightarrow \\rightarrow computes the same gradient \\rightarrow \\rightarrow same parameter updates Normal Initialization: set all weights to random small numbers \u00b6 Every neuron in the network computes different output \\rightarrow \\rightarrow computes different gradient \\rightarrow \\rightarrow different parameter updates \"Symmetry breaking\" Problem: variance that grows with the number of inputs Lecun Initialization: normalize variance \u00b6 Solves growing variance with the number of inputs \\rightarrow \\rightarrow constant variance Look at a simple feedforward neural network Equations for Lecun Initialization \u00b6 Y = AX + B Y = AX + B y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) General term, you might be more familiar with the following Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) E(x_i) E(x_i) : expectation/mean of x_i x_i E(a_i) E(a_i) : expectation/mean of a_i a_i Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0 E(x_i) = E(a_i) = 0 E(x_i) = E(a_i) = 0 Var(a_i x_i) = Var(a_i)Var(x_i) Var(a_i x_i) = Var(a_i)Var(x_i) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Since the bias, b, is a constant, Var(b) = 0 Var(b) = 0 Since i.i.d. Var(y) = n \\times Var(a_i)Var(x_i) Var(y) = n \\times Var(a_i)Var(x_i) Since we want constant variance where Var(y) = Var(x_i) Var(y) = Var(x_i) 1 = nVar(a_i) 1 = nVar(a_i) Var(a_i) = \\frac{1}{n} Var(a_i) = \\frac{1}{n} This is essentially Lecun initialization, from his paper titled \"Efficient Backpropagation\" We draw our weights i.i.d. with mean=0 and variance = \\frac{1}{n} \\frac{1}{n} Where n n is the number of input units in the weight tensor Improvements to Lecun Intialization \u00b6 They are essentially slight modifications to Lecun'98 initialization Xavier Intialization Works better for layers with Sigmoid activations var(a_i) = \\frac{1}{n_{in} + n_{out}} var(a_i) = \\frac{1}{n_{in} + n_{out}} Where n_{in} n_{in} and n_{out} n_{out} are the number of input and output units in the weight tensor respectively Kaiming Initialization Works better for layers with ReLU or LeakyReLU activations var(a_i) = \\frac{2}{n_{in}} var(a_i) = \\frac{2}{n_{in}} Summary of weight initializations \u00b6 Normal Distribution Lecun Normal Distribution Xavier (Glorot) Normal Distribution Kaiming (He) Normal Distribution Weight Initializations with PyTorch \u00b6 Normal Initialization: Tanh Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . normal_ ( self . fc1 . weight , mean = 0 , std = 1 ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . normal_ ( self . fc2 . weight , mean = 0 , std = 1 ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.5192779302597046 . Accuracy : 87.9 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.4060308337211609 . Accuracy : 90.15 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.2880493104457855 . Accuracy : 90.71 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.23173095285892487 . Accuracy : 91.99 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.23814399540424347 . Accuracy : 92.32 Iteration : 3000 . Loss : 0.19513173401355743 . Accuracy : 92.55 Lecun Initialization: Tanh Activation \u00b6 By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.20123475790023804 . Accuracy : 95.63 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.10885068774223328 . Accuracy : 96.48 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.1296212077140808 . Accuracy : 97.22 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.05178885534405708 . Accuracy : 97.36 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.02619408629834652 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.02096685953438282 . Accuracy : 97.7 Xavier Initialization: Tanh Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.14800140261650085 . Accuracy : 95.43 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.17138008773326874 . Accuracy : 96.58 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.07987994700670242 . Accuracy : 96.95 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.07756654918193817 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.05563584715127945 . Accuracy : 97.6 Iteration : 3000 . Loss : 0.07122127711772919 . Accuracy : 97.49 Xavier Initialization: ReLU Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.1245984435081482 . Accuracy : 95.82 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.14348150789737701 . Accuracy : 96.72 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.10421314090490341 . Accuracy : 97.3 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04693891853094101 . Accuracy : 97.29 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.06869587302207947 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.056865859776735306 . Accuracy : 97.48 He Initialization: ReLU Activation \u00b6 import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . kaiming_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . kaiming_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.11658752709627151 . Accuracy : 95.7 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.15525035560131073 . Accuracy : 96.65 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.09970294684171677 . Accuracy : 97.07 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04063304886221886 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.0719323456287384 . Accuracy : 97.7 Iteration : 3000 . Loss : 0.04470040276646614 . Accuracy : 97.39 Initialization Performance \u00b6 Initialization: Activation Test Accuracy Normal: Tanh 92.55 Lecun: Tanh 97.7 Xavier: Tanh 97.49 Xavier: ReLU 97.48 He: ReLU 97.39 Interpreting the Validation Accuracy Table Take note that these numbers would fluctuate slightly when you change seeds. However, the key point here is that all the other intializations are clearly much better than a basic normal distribution. Whether He, Xavier, or Lecun intialization is better or any other initializations depends on the overall model's architecture (RNN/LSTM/CNN/FNN etc.), activation functions (ReLU, Sigmoid, Tanh etc.) and more. For example, more advanced initializations we will cover subsequently is orthogonal initialization that works better for RNN/LSTM. But due to the math involved in that, we will be covering such advanced initializations in a separate section. Summary \u00b6 We've learnt... Success Recap of LG Recap of FNN Recap of Activation Functions Sigmoid (Logistic) Tanh ReLU Need for Weight Initializations Sigmoid/Tanh: vanishing gradients Constant Variance initialization with Lecun or Xavier ReLU: exploding gradients with dead units He Initialization Leaky ReLU: exploding gradients only He Initialization Types of weight initialisations Zero Normal: growing weight variance Lecun: constant variance Xavier: constant variance for Sigmoid/Tanh Kaiming He: constant variance for ReLU activations PyTorch implementation Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Weight Initialization & Activation Functions"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#weight-initializations-activation-functions","text":"","title":"Weight Initializations &amp; Activation Functions"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#recap-of-logistic-regression","text":"","title":"Recap of Logistic Regression"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#recap-of-feedforward-neural-network-activation-function","text":"","title":"Recap of Feedforward Neural Network Activation Function"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#sigmoid-logistic","text":"\\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this import matplotlib.pyplot as plt % matplotlib inline import numpy as np def sigmoid ( x ): a = [] for item in x : a . append ( 1 / ( 1 + np . exp ( - item ))) return a x = np . arange ( - 10. , 10. , 0.2 ) sig = sigmoid ( x ) plt . style . use ( 'ggplot' ) plt . plot ( x , sig , linewidth = 3.0 )","title":"Sigmoid (Logistic)"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#tanh","text":"\\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this x = np . arange ( - 10. , 10. , 0.2 ) tanh = np . dot ( 2 , sigmoid ( np . dot ( 2 , x ))) - 1 plt . plot ( x , tanh , linewidth = 3.0 )","title":"Tanh"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#relus","text":"f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate and weight initialization choice x = np . arange ( - 10. , 10. , 0.2 ) relu = np . maximum ( x , 0 ) plt . plot ( x , relu , linewidth = 3.0 )","title":"ReLUs"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#why-do-we-need-weight-initializations-or-new-activation-functions","text":"To prevent vanishing/exploding gradients","title":"Why do we need weight initializations or new activation functions?"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-1-sigmoidtanh","text":"Problem If variance of input too large: gradients = 0 (vanishing gradients) If variance of input too small: linear \\rightarrow \\rightarrow gradients = constant value Solutions Want a constant variance of input to achieve non-linearity \\rightarrow \\rightarrow unique gradients for unique updates Xavier Initialization (good constant variance for Sigmoid/Tanh) ReLU or Leaky ReLU","title":"Case 1: Sigmoid/Tanh"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-2-relu","text":"Solution to Case 1 Regardless of variance of input: gradients = 0 or 1 Problem But those with 0: no updates (\"dead ReLU units\") Has unlimited output size with input > 0 (explodes gradients subsequently) Solutions He Initialization (good constant variance) Leaky ReLU","title":"Case 2: ReLU"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#case-3-leaky-relu","text":"Solution to Case 2 Solves the 0 signal issue when input < 0 Problem Has unlimited output size with input > 0 (explodes) Solution He Initialization (good constant variance)","title":"Case 3: Leaky ReLU"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary-of-weight-initialization-solutions-to-activations","text":"Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization Good range of constant variance ReLU/Leaky ReLU exploding gradients can be solved with He initialization Good range of constant variance","title":"Summary of weight initialization solutions to activations"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#types-of-weight-intializations","text":"","title":"Types of weight intializations"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#zero-initialization-set-all-weights-to-0","text":"Every neuron in the network computes the same output \\rightarrow \\rightarrow computes the same gradient \\rightarrow \\rightarrow same parameter updates","title":"Zero Initialization: set all weights to 0"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#normal-initialization-set-all-weights-to-random-small-numbers","text":"Every neuron in the network computes different output \\rightarrow \\rightarrow computes different gradient \\rightarrow \\rightarrow different parameter updates \"Symmetry breaking\" Problem: variance that grows with the number of inputs","title":"Normal Initialization: set all weights to random small numbers"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#lecun-initialization-normalize-variance","text":"Solves growing variance with the number of inputs \\rightarrow \\rightarrow constant variance Look at a simple feedforward neural network","title":"Lecun Initialization: normalize variance"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#equations-for-lecun-initialization","text":"Y = AX + B Y = AX + B y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i) General term, you might be more familiar with the following Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y) E(x_i) E(x_i) : expectation/mean of x_i x_i E(a_i) E(a_i) : expectation/mean of a_i a_i Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0 E(x_i) = E(a_i) = 0 E(x_i) = E(a_i) = 0 Var(a_i x_i) = Var(a_i)Var(x_i) Var(a_i x_i) = Var(a_i)Var(x_i) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) Since the bias, b, is a constant, Var(b) = 0 Var(b) = 0 Since i.i.d. Var(y) = n \\times Var(a_i)Var(x_i) Var(y) = n \\times Var(a_i)Var(x_i) Since we want constant variance where Var(y) = Var(x_i) Var(y) = Var(x_i) 1 = nVar(a_i) 1 = nVar(a_i) Var(a_i) = \\frac{1}{n} Var(a_i) = \\frac{1}{n} This is essentially Lecun initialization, from his paper titled \"Efficient Backpropagation\" We draw our weights i.i.d. with mean=0 and variance = \\frac{1}{n} \\frac{1}{n} Where n n is the number of input units in the weight tensor","title":"Equations for Lecun Initialization"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#improvements-to-lecun-intialization","text":"They are essentially slight modifications to Lecun'98 initialization Xavier Intialization Works better for layers with Sigmoid activations var(a_i) = \\frac{1}{n_{in} + n_{out}} var(a_i) = \\frac{1}{n_{in} + n_{out}} Where n_{in} n_{in} and n_{out} n_{out} are the number of input and output units in the weight tensor respectively Kaiming Initialization Works better for layers with ReLU or LeakyReLU activations var(a_i) = \\frac{2}{n_{in}} var(a_i) = \\frac{2}{n_{in}}","title":"Improvements to Lecun Intialization"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary-of-weight-initializations","text":"Normal Distribution Lecun Normal Distribution Xavier (Glorot) Normal Distribution Kaiming (He) Normal Distribution","title":"Summary of  weight initializations"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#weight-initializations-with-pytorch","text":"","title":"Weight Initializations with PyTorch"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#normal-initialization-tanh-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . normal_ ( self . fc1 . weight , mean = 0 , std = 1 ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . normal_ ( self . fc2 . weight , mean = 0 , std = 1 ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.5192779302597046 . Accuracy : 87.9 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.4060308337211609 . Accuracy : 90.15 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.2880493104457855 . Accuracy : 90.71 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.23173095285892487 . Accuracy : 91.99 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.23814399540424347 . Accuracy : 92.32 Iteration : 3000 . Loss : 0.19513173401355743 . Accuracy : 92.55","title":"Normal Initialization: Tanh Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#lecun-initialization-tanh-activation","text":"By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.20123475790023804 . Accuracy : 95.63 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.10885068774223328 . Accuracy : 96.48 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.1296212077140808 . Accuracy : 97.22 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.05178885534405708 . Accuracy : 97.36 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.02619408629834652 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.02096685953438282 . Accuracy : 97.7","title":"Lecun Initialization: Tanh Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#xavier-initialization-tanh-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.14800140261650085 . Accuracy : 95.43 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.17138008773326874 . Accuracy : 96.58 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.07987994700670242 . Accuracy : 96.95 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.07756654918193817 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.05563584715127945 . Accuracy : 97.6 Iteration : 3000 . Loss : 0.07122127711772919 . Accuracy : 97.49","title":"Xavier Initialization: Tanh Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#xavier-initialization-relu-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . xavier_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . xavier_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.1245984435081482 . Accuracy : 95.82 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.14348150789737701 . Accuracy : 96.72 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.10421314090490341 . Accuracy : 97.3 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04693891853094101 . Accuracy : 97.29 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.06869587302207947 . Accuracy : 97.61 Iteration : 3000 . Loss : 0.056865859776735306 . Accuracy : 97.48","title":"Xavier Initialization: ReLU Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#he-initialization-relu-activation","text":"import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets from torch.autograd import Variable # Set seed torch . manual_seed ( 0 ) # Scheduler import from torch.optim.lr_scheduler import StepLR ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Linear weight, W, Y = WX + B nn . init . kaiming_normal_ ( self . fc1 . weight ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) nn . init . kaiming_normal_ ( self . fc2 . weight ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate , momentum = 0.9 , nesterov = True ) ''' STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS ''' # step_size: at how many multiples of epoch you decay # step_size = 1, after every 2 epoch, new_lr = lr*gamma # step_size = 2, after every 2 epoch, new_lr = lr*gamma # gamma = decaying factor scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.96 ) ''' STEP 8: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): # Decay Learning Rate scheduler . step () # Print Learning Rate print ( 'Epoch:' , epoch , 'LR:' , scheduler . get_lr ()) for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted . type ( torch . FloatTensor ) . cpu () == labels . type ( torch . FloatTensor )) . sum () accuracy = 100. * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Epoch : 0 LR : [ 0.1 ] Iteration : 500 . Loss : 0.11658752709627151 . Accuracy : 95.7 Epoch : 1 LR : [ 0.096 ] Iteration : 1000 . Loss : 0.15525035560131073 . Accuracy : 96.65 Epoch : 2 LR : [ 0.09216 ] Iteration : 1500 . Loss : 0.09970294684171677 . Accuracy : 97.07 Epoch : 3 LR : [ 0.08847359999999999 ] Iteration : 2000 . Loss : 0.04063304886221886 . Accuracy : 97.23 Epoch : 4 LR : [ 0.084934656 ] Iteration : 2500 . Loss : 0.0719323456287384 . Accuracy : 97.7 Iteration : 3000 . Loss : 0.04470040276646614 . Accuracy : 97.39","title":"He Initialization: ReLU Activation"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#initialization-performance","text":"Initialization: Activation Test Accuracy Normal: Tanh 92.55 Lecun: Tanh 97.7 Xavier: Tanh 97.49 Xavier: ReLU 97.48 He: ReLU 97.39 Interpreting the Validation Accuracy Table Take note that these numbers would fluctuate slightly when you change seeds. However, the key point here is that all the other intializations are clearly much better than a basic normal distribution. Whether He, Xavier, or Lecun intialization is better or any other initializations depends on the overall model's architecture (RNN/LSTM/CNN/FNN etc.), activation functions (ReLU, Sigmoid, Tanh etc.) and more. For example, more advanced initializations we will cover subsequently is orthogonal initialization that works better for RNN/LSTM. But due to the math involved in that, we will be covering such advanced initializations in a separate section.","title":"Initialization Performance"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#summary","text":"We've learnt... Success Recap of LG Recap of FNN Recap of Activation Functions Sigmoid (Logistic) Tanh ReLU Need for Weight Initializations Sigmoid/Tanh: vanishing gradients Constant Variance initialization with Lecun or Xavier ReLU: exploding gradients with dead units He Initialization Leaky ReLU: exploding gradients only He Initialization Types of weight initialisations Zero Normal: growing weight variance Lecun: constant variance Xavier: constant variance for Sigmoid/Tanh Kaiming He: constant variance for ReLU activations PyTorch implementation","title":"Summary"},{"location":"deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/","text":"Convolutional Neural Network with PyTorch \u00b6 About Convolutional Neural Network \u00b6 Transition From Feedforward Neural Network \u00b6 Hidden Layer Feedforward Neural Network \u00b6 Recap of FNN So let's do a recap of what we covered in the Feedforward Neural Network (FNN) section using a simple FNN with 1 hidden layer (a pair of affine function and non-linear function) [Yellow box] Pass input into an affine function \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} [Pink box] Pass logits to non-linear function, for example sigmoid, tanh (hyperbolic tangent), ReLU, or LeakyReLU [Blue box] Pass output of non-linear function to another affine function [Red box] Pass output of final affine function to softmax function to get our probability distribution over K classes [Purple box] Finally we can get our loss by using our cross entropy function Basic Convolutional Neural Network (CNN) \u00b6 A basic CNN just requires 2 additional layers! Convolution and pooling layers before our feedforward neural network Fully Connected (FC) Layer A layer with an affine function & non-linear function is called a Fully Connected (FC) layer One Convolutional Layer: High Level View \u00b6 One Convolutional Layer: High Level View Summary \u00b6 As the kernel is sliding/convolving across the image \\rightarrow \\rightarrow 2 operations done per patch Element-wise multiplication Summation More kernels = = more feature map channels Can capture more information about the input Multiple Convolutional Layers: High Level View \u00b6 Pooling Layer: High Level View \u00b6 2 Common Types Max Pooling Average Pooling Multiple Pooling Layers: High Level View \u00b6 Padding \u00b6 Padding Summary \u00b6 Valid Padding (No Padding) Output size < Input Size Same Padding (Zero Padding) Output size = Input Size Dimension Calculations \u00b6 O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) P P : padding P = \\frac{K - 1}{2} P = \\frac{K - 1}{2} S S : stride Example 1: Output Dimension Calculation for Valid Padding \u00b6 W = 4 W = 4 K = 3 K = 3 P = 0 P = 0 S = 1 S = 1 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 Example 2: Output Dimension Calculation for Same Padding \u00b6 W = 5 W = 5 K = 3 K = 3 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 S = 1 S = 1 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5 Building a Convolutional Neural Network with PyTorch \u00b6 Model A: \u00b6 2 Convolutional Layers Same Padding (same output size) 2 Max Pooling Layers 1 Fully Connected Layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 MNIST Dataset and Size of Training Dataset (Excluding Labels) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) torch.Size ([ 60000 , 28 , 28 ]) Size of our training dataset labels print ( train_dataset . train_labels . size ()) torch.Size ([ 60000 ]) Size of our testing dataset (excluding labels) print ( test_dataset . test_data . size ()) torch.Size ([ 10000 , 28 , 28 ]) Size of our testing dataset labels print ( test_dataset . test_labels . size ()) torch.Size ([ 10000 ]) Step 2: Make Dataset Iterable \u00b6 Load Dataset into Dataloader batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Output Formula for Convolution \u00b6 O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) = 5 P P : same padding (non-zero) P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 S S : stride = 1 Output Formula for Pooling \u00b6 O = \\frac {W - K}{S} + 1 O = \\frac {W - K}{S} + 1 W: input height/width K: filter size = 2 S: stride size = filter size , PyTorch defaults the stride to kernel filter size If using PyTorch default stride, this will result in the formula O = \\frac {W}{K} O = \\frac {W}{K} By default, in our tutorials, we do this for simplicity. Define our simple 2 convolutional layer CNN class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out Step 4: Instantiate Model Class \u00b6 Our model model = CNNModel () Step 5: Instantiate Loss Class \u00b6 Convolutional Neural Network: Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Our cross entropy loss criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Print model's parameter print ( model . parameters ()) print ( len ( list ( model . parameters ()))) # Convolution 1: 16 Kernels print ( list ( model . parameters ())[ 0 ] . size ()) # Convolution 1 Bias: 16 Kernels print ( list ( model . parameters ())[ 1 ] . size ()) # Convolution 2: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 2 ] . size ()) # Convolution 2 Bias: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 3 ] . size ()) # Fully Connected Layer 1 print ( list ( model . parameters ())[ 4 ] . size ()) # Fully Connected Layer Bias print ( list ( model . parameters ())[ 5 ] . size ()) <generator object Module.parameters at 0x7f9864363c50> 6 torch.Size ([ 16 , 1 , 5 , 5 ]) torch.Size ([ 16 ]) torch.Size ([ 32 , 16 , 5 , 5 ]) torch.Size ([ 32 ]) torch.Size ([ 10 , 1568 ]) torch.Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs to tensors with gradient accumulation abilities CNN Input: (1, 28, 28) Feedforward NN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Model training iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.43324267864227295 . Accuracy : 90 Iteration : 1000 . Loss : 0.2511480152606964 . Accuracy : 92 Iteration : 1500 . Loss : 0.13431282341480255 . Accuracy : 94 Iteration : 2000 . Loss : 0.11173319816589355 . Accuracy : 95 Iteration : 2500 . Loss : 0.06409914791584015 . Accuracy : 96 Iteration : 3000 . Loss : 0.14377528429031372 . Accuracy : 96 Model B: \u00b6 2 Convolutional Layers Same Padding (same output size) 2 Average Pooling Layers 1 Fully Connected Layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Average Pool + 1 FC (Zero Padding, Same Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Average pool 1 self . avgpool1 = nn . AvgPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Average pool 2 self . avgpool2 = nn . AvgPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Average pool 1 out = self . avgpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . avgpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .6850348711013794. Accuracy: 85 Iteration: 1000 . Loss: 0 .36549052596092224. Accuracy: 88 Iteration: 1500 . Loss: 0 .31540098786354065. Accuracy: 89 Iteration: 2000 . Loss: 0 .3522164225578308. Accuracy: 90 Iteration: 2500 . Loss: 0 .2680729925632477. Accuracy: 91 Iteration: 3000 . Loss: 0 .26440390944480896. Accuracy: 92 Comparison of accuracies It seems like average pooling test accuracy is less than the max pooling accuracy! Does this mean average pooling is better? This is not definitive and depends on a lot of factors including the model's architecture, seed (that affects random weight initialization) and more. Model C: \u00b6 2 Convolutional Layers Valid Padding (smaller output size) 2 Max Pooling Layers 1 Fully Connected Layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pool + 1 FC (Valid Padding, No Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .5153220295906067. Accuracy: 88 Iteration: 1000 . Loss: 0 .28784745931625366. Accuracy: 92 Iteration: 1500 . Loss: 0 .4086027443408966. Accuracy: 94 Iteration: 2000 . Loss: 0 .09390712529420853. Accuracy: 95 Iteration: 2500 . Loss: 0 .07138358801603317. Accuracy: 95 Iteration: 3000 . Loss: 0 .05396252125501633. Accuracy: 96 Summary of Results \u00b6 Model A Model B Model C Max Pooling Average Pooling Max Pooling Same Padding Same Padding Valid Padding 97.04% 93.59% 96.5% All Models INPUT \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow FC Convolution Kernel Size = 5 x 5 Convolution Kernel Stride = 1 Pooling Kernel Size = 2 x 2 General Deep Learning Notes on CNN and FNN \u00b6 3 ways to expand a convolutional neural network More convolutional layers Less aggressive downsampling Smaller kernel size for pooling (gradually downsampling) More fully connected layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Convolutional Neural Network with PyTorch (GPU) \u00b6 Model A \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pooling + 1 FC (Same Padding, Zero Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .36831170320510864. Accuracy: 88 Iteration: 1000 . Loss: 0 .31790846586227417. Accuracy: 92 Iteration: 1500 . Loss: 0 .1510857343673706. Accuracy: 94 Iteration: 2000 . Loss: 0 .08368007838726044. Accuracy: 95 Iteration: 2500 . Loss: 0 .13419771194458008. Accuracy: 96 Iteration: 3000 . Loss: 0 .16750787198543549. Accuracy: 96 More Efficient Convolutions via Toeplitz Matrices This is beyond the scope of this particular lesson. But now that we understand how convolutions work, it is critical to know that it is quite an inefficient operation if we use for-loops to perform our 2D convolutions (5 x 5 convolution kernel size for example) on our 2D images (28 x 28 MNIST image for example). A more efficient implementation is in converting our convolution kernel into a Toeplitz matrix and our image into a vector. Then, we will do just one matrix operation using our Toeplitz matrix and vector. There will be a whole lesson dedicated to this operation released down the road. Summary \u00b6 We've learnt to... Success Transition from Feedforward Neural Network Addition of Convolutional & Pooling Layers before Linear Layers One Convolutional Layer Basics One Pooling Layer Basics Max pooling Average pooling Padding Output Dimension Calculations and Examples O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 Convolutional Neural Networks Model A : 2 Conv + 2 Max pool + 1 FC Same Padding Model B : 2 Conv + 2 Average pool + 1 FC Same Padding Model C : 2 Conv + 2 Max pool + 1 FC Valid Padding Model Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More convolutions Gradual pooling More fully connected layers GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Convolutional Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#convolutional-neural-network-with-pytorch","text":"","title":"Convolutional Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#about-convolutional-neural-network","text":"","title":"About Convolutional Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#transition-from-feedforward-neural-network","text":"","title":"Transition From Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#hidden-layer-feedforward-neural-network","text":"Recap of FNN So let's do a recap of what we covered in the Feedforward Neural Network (FNN) section using a simple FNN with 1 hidden layer (a pair of affine function and non-linear function) [Yellow box] Pass input into an affine function \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} [Pink box] Pass logits to non-linear function, for example sigmoid, tanh (hyperbolic tangent), ReLU, or LeakyReLU [Blue box] Pass output of non-linear function to another affine function [Red box] Pass output of final affine function to softmax function to get our probability distribution over K classes [Purple box] Finally we can get our loss by using our cross entropy function","title":"Hidden Layer Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#basic-convolutional-neural-network-cnn","text":"A basic CNN just requires 2 additional layers! Convolution and pooling layers before our feedforward neural network Fully Connected (FC) Layer A layer with an affine function & non-linear function is called a Fully Connected (FC) layer","title":"Basic Convolutional Neural Network (CNN)"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#one-convolutional-layer-high-level-view","text":"","title":"One Convolutional Layer: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#one-convolutional-layer-high-level-view-summary","text":"As the kernel is sliding/convolving across the image \\rightarrow \\rightarrow 2 operations done per patch Element-wise multiplication Summation More kernels = = more feature map channels Can capture more information about the input","title":"One Convolutional Layer: High Level View Summary"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#multiple-convolutional-layers-high-level-view","text":"","title":"Multiple Convolutional Layers: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#pooling-layer-high-level-view","text":"2 Common Types Max Pooling Average Pooling","title":"Pooling Layer: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#multiple-pooling-layers-high-level-view","text":"","title":"Multiple Pooling Layers: High Level View"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#padding","text":"","title":"Padding"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#padding-summary","text":"Valid Padding (No Padding) Output size < Input Size Same Padding (Zero Padding) Output size = Input Size","title":"Padding Summary"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#dimension-calculations","text":"O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) P P : padding P = \\frac{K - 1}{2} P = \\frac{K - 1}{2} S S : stride","title":"Dimension Calculations"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#example-1-output-dimension-calculation-for-valid-padding","text":"W = 4 W = 4 K = 3 K = 3 P = 0 P = 0 S = 1 S = 1 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2 O = \\frac {4 - 3 + 2*0}{1} + 1 = \\frac {1}{1} + 1 = 1 + 1 = 2","title":"Example 1: Output Dimension Calculation for Valid Padding"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#example-2-output-dimension-calculation-for-same-padding","text":"W = 5 W = 5 K = 3 K = 3 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 P = \\frac{3 - 1}{2} = \\frac{2}{2} = 1 S = 1 S = 1 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5 O = \\frac {5 - 3 + 2*1}{1} + 1 = \\frac {4}{1} + 1 = 5","title":"Example 2: Output Dimension Calculation for Same Padding"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#building-a-convolutional-neural-network-with-pytorch","text":"","title":"Building a Convolutional Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-a","text":"2 Convolutional Layers Same Padding (same output size) 2 Max Pooling Layers 1 Fully Connected Layer","title":"Model A:"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 MNIST Dataset and Size of Training Dataset (Excluding Labels) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) torch.Size ([ 60000 , 28 , 28 ]) Size of our training dataset labels print ( train_dataset . train_labels . size ()) torch.Size ([ 60000 ]) Size of our testing dataset (excluding labels) print ( test_dataset . test_data . size ()) torch.Size ([ 10000 , 28 , 28 ]) Size of our testing dataset labels print ( test_dataset . test_labels . size ()) torch.Size ([ 10000 ])","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-2-make-dataset-iterable","text":"Load Dataset into Dataloader batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-3-create-model-class","text":"","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#output-formula-for-convolution","text":"O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 O O : output height/length W W : input height/length K K : filter size (kernel size) = 5 P P : same padding (non-zero) P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 P = \\frac{K - 1}{2} = \\frac{5 - 1}{2} = 2 S S : stride = 1","title":"Output Formula for Convolution"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#output-formula-for-pooling","text":"O = \\frac {W - K}{S} + 1 O = \\frac {W - K}{S} + 1 W: input height/width K: filter size = 2 S: stride size = filter size , PyTorch defaults the stride to kernel filter size If using PyTorch default stride, this will result in the formula O = \\frac {W}{K} O = \\frac {W}{K} By default, in our tutorials, we do this for simplicity. Define our simple 2 convolutional layer CNN class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out","title":"Output Formula for Pooling"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-4-instantiate-model-class","text":"Our model model = CNNModel ()","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-5-instantiate-loss-class","text":"Convolutional Neural Network: Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Our cross entropy loss criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#parameters-in-depth","text":"Print model's parameter print ( model . parameters ()) print ( len ( list ( model . parameters ()))) # Convolution 1: 16 Kernels print ( list ( model . parameters ())[ 0 ] . size ()) # Convolution 1 Bias: 16 Kernels print ( list ( model . parameters ())[ 1 ] . size ()) # Convolution 2: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 2 ] . size ()) # Convolution 2 Bias: 32 Kernels with depth = 16 print ( list ( model . parameters ())[ 3 ] . size ()) # Fully Connected Layer 1 print ( list ( model . parameters ())[ 4 ] . size ()) # Fully Connected Layer Bias print ( list ( model . parameters ())[ 5 ] . size ()) <generator object Module.parameters at 0x7f9864363c50> 6 torch.Size ([ 16 , 1 , 5 , 5 ]) torch.Size ([ 16 ]) torch.Size ([ 32 , 16 , 5 , 5 ]) torch.Size ([ 32 ]) torch.Size ([ 10 , 1568 ]) torch.Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#step-7-train-model","text":"Process Convert inputs to tensors with gradient accumulation abilities CNN Input: (1, 28, 28) Feedforward NN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Model training iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500 . Loss : 0.43324267864227295 . Accuracy : 90 Iteration : 1000 . Loss : 0.2511480152606964 . Accuracy : 92 Iteration : 1500 . Loss : 0.13431282341480255 . Accuracy : 94 Iteration : 2000 . Loss : 0.11173319816589355 . Accuracy : 95 Iteration : 2500 . Loss : 0.06409914791584015 . Accuracy : 96 Iteration : 3000 . Loss : 0.14377528429031372 . Accuracy : 96","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-b","text":"2 Convolutional Layers Same Padding (same output size) 2 Average Pooling Layers 1 Fully Connected Layer","title":"Model B:"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Average Pool + 1 FC (Zero Padding, Same Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu1 = nn . ReLU () # Average pool 1 self . avgpool1 = nn . AvgPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 2 ) self . relu2 = nn . ReLU () # Average pool 2 self . avgpool2 = nn . AvgPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 7 * 7 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Average pool 1 out = self . avgpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . avgpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .6850348711013794. Accuracy: 85 Iteration: 1000 . Loss: 0 .36549052596092224. Accuracy: 88 Iteration: 1500 . Loss: 0 .31540098786354065. Accuracy: 89 Iteration: 2000 . Loss: 0 .3522164225578308. Accuracy: 90 Iteration: 2500 . Loss: 0 .2680729925632477. Accuracy: 91 Iteration: 3000 . Loss: 0 .26440390944480896. Accuracy: 92 Comparison of accuracies It seems like average pooling test accuracy is less than the max pooling accuracy! Does this mean average pooling is better? This is not definitive and depends on a lot of factors including the model's architecture, seed (that affects random weight initialization) and more.","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-c","text":"2 Convolutional Layers Valid Padding (smaller output size) 2 Max Pooling Layers 1 Fully Connected Layer","title":"Model C:"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pool + 1 FC (Valid Padding, No Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to tensors with gradient accumulation abilities images = images . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .5153220295906067. Accuracy: 88 Iteration: 1000 . Loss: 0 .28784745931625366. Accuracy: 92 Iteration: 1500 . Loss: 0 .4086027443408966. Accuracy: 94 Iteration: 2000 . Loss: 0 .09390712529420853. Accuracy: 95 Iteration: 2500 . Loss: 0 .07138358801603317. Accuracy: 95 Iteration: 3000 . Loss: 0 .05396252125501633. Accuracy: 96","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#summary-of-results","text":"Model A Model B Model C Max Pooling Average Pooling Max Pooling Same Padding Same Padding Valid Padding 97.04% 93.59% 96.5% All Models INPUT \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow CONV \\rightarrow \\rightarrow POOL \\rightarrow \\rightarrow FC Convolution Kernel Size = 5 x 5 Convolution Kernel Stride = 1 Pooling Kernel Size = 2 x 2","title":"Summary of Results"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#general-deep-learning-notes-on-cnn-and-fnn","text":"3 ways to expand a convolutional neural network More convolutional layers Less aggressive downsampling Smaller kernel size for pooling (gradually downsampling) More fully connected layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Deep Learning Notes on CNN and FNN"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#3-building-a-convolutional-neural-network-with-pytorch-gpu","text":"","title":"3. Building a Convolutional Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#model-a_1","text":"GPU: 2 things must be on GPU - model - tensors","title":"Model A"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Conv + 2 Max Pooling + 1 FC (Same Padding, Zero Padding) import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class CNNModel ( nn . Module ): def __init__ ( self ): super ( CNNModel , self ) . __init__ () # Convolution 1 self . cnn1 = nn . Conv2d ( in_channels = 1 , out_channels = 16 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu1 = nn . ReLU () # Max pool 1 self . maxpool1 = nn . MaxPool2d ( kernel_size = 2 ) # Convolution 2 self . cnn2 = nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 5 , stride = 1 , padding = 0 ) self . relu2 = nn . ReLU () # Max pool 2 self . maxpool2 = nn . MaxPool2d ( kernel_size = 2 ) # Fully connected 1 (readout) self . fc1 = nn . Linear ( 32 * 4 * 4 , 10 ) def forward ( self , x ): # Convolution 1 out = self . cnn1 ( x ) out = self . relu1 ( out ) # Max pool 1 out = self . maxpool1 ( out ) # Convolution 2 out = self . cnn2 ( out ) out = self . relu2 ( out ) # Max pool 2 out = self . maxpool2 ( out ) # Resize # Original size: (100, 32, 7, 7) # out.size(0): 100 # New out size: (100, 32*7*7) out = out . view ( out . size ( 0 ), - 1 ) # Linear function (readout) out = self . fc1 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' model = CNNModel () ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration: 500 . Loss: 0 .36831170320510864. Accuracy: 88 Iteration: 1000 . Loss: 0 .31790846586227417. Accuracy: 92 Iteration: 1500 . Loss: 0 .1510857343673706. Accuracy: 94 Iteration: 2000 . Loss: 0 .08368007838726044. Accuracy: 95 Iteration: 2500 . Loss: 0 .13419771194458008. Accuracy: 96 Iteration: 3000 . Loss: 0 .16750787198543549. Accuracy: 96 More Efficient Convolutions via Toeplitz Matrices This is beyond the scope of this particular lesson. But now that we understand how convolutions work, it is critical to know that it is quite an inefficient operation if we use for-loops to perform our 2D convolutions (5 x 5 convolution kernel size for example) on our 2D images (28 x 28 MNIST image for example). A more efficient implementation is in converting our convolution kernel into a Toeplitz matrix and our image into a vector. Then, we will do just one matrix operation using our Toeplitz matrix and vector. There will be a whole lesson dedicated to this operation released down the road.","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#summary","text":"We've learnt to... Success Transition from Feedforward Neural Network Addition of Convolutional & Pooling Layers before Linear Layers One Convolutional Layer Basics One Pooling Layer Basics Max pooling Average pooling Padding Output Dimension Calculations and Examples O = \\frac {W - K + 2P}{S} + 1 O = \\frac {W - K + 2P}{S} + 1 Convolutional Neural Networks Model A : 2 Conv + 2 Max pool + 1 FC Same Padding Model B : 2 Conv + 2 Average pool + 1 FC Same Padding Model C : 2 Conv + 2 Max pool + 1 FC Valid Padding Model Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More convolutions Gradual pooling More fully connected layers GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/","text":"Feedforward Neural Network with PyTorch \u00b6 About Feedforward Neural Network \u00b6 Logistic Regression Transition to Neural Networks \u00b6 Logistic Regression Review \u00b6 Define logistic regression model Import our relevant torch modules. import torch import torch.nn as nn Define our model class. class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate the logistic regression model. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). print ( model ) LogisticRegressionModel ( ( linear ): Linear ( in_features = 784 , out_features = 10 , bias = True ) ) Logistic Regression Problems \u00b6 Can represent linear functions well y = 2x + 3 y = 2x + 3 y = x_1 + x_2 y = x_1 + x_2 y = x_1 + 3x_2 + 4x_3 y = x_1 + 3x_2 + 4x_3 Cannot represent non-linear functions y = 4x_1 + 2x_2^2 +3x_3^3 y = 4x_1 + 2x_2^2 +3x_3^3 y = x_1x_2 y = x_1x_2 Introducing a Non-linear Function \u00b6 Non-linear Function In-Depth \u00b6 Function: takes a number & perform mathematical operation Common Types of Non-linearity ReLUs (Rectified Linear Units) Sigmoid Tanh Sigmoid (Logistic) \u00b6 \\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this Outputs not centered around 0 If output always positive \\rightarrow \\rightarrow gradients always positive or negative \\rightarrow \\rightarrow bad for gradient updates Tanh \u00b6 \\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this ReLUs \u00b6 f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate choice Building a Feedforward Neural Network with PyTorch \u00b6 Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) Step 2: Make Dataset Iterable \u00b6 Batch sizes and iterations Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently. There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would: (1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. (2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests! If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. 60000 / 100 600.0 Epochs An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations. If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). 600 * 5 3000.0 Bringing batch size, iterations and epochs together As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100. Because we want 5 epochs, we need a total of 3000 iterations. batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Creating our feedforward neural network Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. This translates to just 4 more lines of code! class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . sigmoid = nn . Sigmoid () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function # LINEAR out = self . fc1 ( x ) # Non-linearity # NON-LINEAR out = self . sigmoid ( out ) # Linear function (readout) # LINEAR out = self . fc2 ( out ) return out Step 4: Instantiate Model Class \u00b6 Input dimension: 784 Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Hidden dimension: 100 Can be any number Similar term Number of neurons Number of non-linear activation functions Instantiating our model class Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28). Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10. Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results. On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Feedforward Neural Network: Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Loss class This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels. criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation capabilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer class Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities. Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section. learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Linear layers' parameters In a simple linear layer it's Y = AX + B Y = AX + B , and our parameters are A A and bias B B . Hence, each linear layer would have 2 groups of parameters A A and B B . It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on Y Y , the output of our linear layers. This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. print ( model . parameters ()) Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters A A and b b . print ( len ( list ( model . parameters ()))) Our first linear layer parameters, A_1 A_1 , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) Our first linear layer bias parameters, B_1 B_1 , would be of size 100 which is our hidden size. # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) Our second linear layer is our readout layer, where the parameters A_2 A_2 would be of size 10 x 100. This is because our output size is 10 and hidden size is 100. # FC 2 Parameters print ( list ( model . parameters ())[ 2 ] . size ()) Likewise our readout layer's bias B_1 B_1 would just be 10, the size of our output. # FC 2 Bias Parameters print ( list ( model . parameters ())[ 3 ] . size ()) The diagram below shows the interaction amongst our input X X and our linear layers' parameters A_1 A_1 , B_1 B_1 , A_2 A_2 , and B_2 B_2 to reach to the final size of 10 x 1. If you're still unfamiliar with matrix product, go ahead and review the previous quick lesson where we covered it in logistic regression . < generator object Module . parameters at 0x7f1d530fa678 > 4 torch . Size ([ 100 , 784 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs to tensors with gradient accumulation capabilities Clear gradient buffers Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT 7-step training process iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.6457265615463257 . Accuracy : 85 Iteration : 1000. Loss : 0.39627206325531006 . Accuracy : 89 Iteration : 1500. Loss : 0.2831554412841797 . Accuracy : 90 Iteration : 2000. Loss : 0.4409525394439697 . Accuracy : 91 Iteration : 2500. Loss : 0.2397005707025528 . Accuracy : 91 Iteration : 3000. Loss : 0.3160165846347809 . Accuracy : 91 Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with Tanh Activation The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.4128190577030182 . Accuracy : 91 Iteration : 1000. Loss : 0.14497484266757965 . Accuracy : 92 Iteration : 1500. Loss : 0.272532194852829 . Accuracy : 93 Iteration : 2000. Loss : 0.2758277952671051 . Accuracy : 94 Iteration : 2500. Loss : 0.1603182554244995 . Accuracy : 94 Iteration : 3000. Loss : 0.08848697692155838 . Accuracy : 95 Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with ReLU Activation The only difference again is in using ReLU activation and it affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3179700970649719 . Accuracy : 91 Iteration : 1000. Loss : 0.17288273572921753 . Accuracy : 93 Iteration : 1500. Loss : 0.16829034686088562 . Accuracy : 94 Iteration : 2000. Loss : 0.25494423508644104 . Accuracy : 94 Iteration : 2500. Loss : 0.16818439960479736 . Accuracy : 95 Iteration : 3000. Loss : 0.11110792309045792 . Accuracy : 95 Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2-layer FNN with ReLU Activation This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3 (readout): 100 --> 10 self . fc3 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 3 (readout) out = self . fc3 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.2995373010635376 . Accuracy : 91 Iteration : 1000. Loss : 0.3924565613269806 . Accuracy : 93 Iteration : 1500. Loss : 0.1283276081085205 . Accuracy : 94 Iteration : 2000. Loss : 0.10905527323484421 . Accuracy : 95 Iteration : 2500. Loss : 0.11943754553794861 . Accuracy : 96 Iteration : 3000. Loss : 0.15632082521915436 . Accuracy : 96 Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality? import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.33234935998916626 . Accuracy : 89 Iteration : 1000. Loss : 0.3098006248474121 . Accuracy : 94 Iteration : 1500. Loss : 0.12461677193641663 . Accuracy : 95 Iteration : 2000. Loss : 0.14346086978912354 . Accuracy : 96 Iteration : 2500. Loss : 0.03763459622859955 . Accuracy : 96 Iteration : 3000. Loss : 0.1397182047367096 . Accuracy : 97 General Comments on FNNs \u00b6 2 ways to expand a neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Feedforward Neural Network with PyTorch (GPU) \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation on GPU Only step 4 and 7 of the CPU code will be affected and it's a simple change. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3877025246620178 . Accuracy : 90 Iteration : 1000. Loss : 0.1337055265903473 . Accuracy : 93 Iteration : 1500. Loss : 0.2038637101650238 . Accuracy : 95 Iteration : 2000. Loss : 0.17892278730869293 . Accuracy : 95 Iteration : 2500. Loss : 0.14455552399158478 . Accuracy : 96 Iteration : 3000. Loss : 0.024540524929761887 . Accuracy : 96 Alternative Term of Neural Network The alternative term is Universal Function Approximator . This is because ultimately we are trying to find a function that maps our input, X X , to our output, y y . Summary \u00b6 We've learnt to... Success Logistic Regression Problems for Non-Linear Functions Representation Cannot represent non-linear functions $ y = 4x_1 + 2x_2^2 +3x_3^3 $ $ y = x_1x_2$ Introduced Non-Linearity to Logistic Regression to form a Neural Network Types of Non-Linearity Sigmoid Tanh ReLU Feedforward Neural Network Models Model A: 1 hidden layer ( sigmoid activation) Model B: 1 hidden layer ( tanh activation) Model C: 1 hidden layer ( ReLU activation) Model D: 2 hidden layers (ReLU activation) Model E: 3 hidden layers (ReLU activation) Models Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation capabilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Feedforward Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#feedforward-neural-network-with-pytorch","text":"","title":"Feedforward Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#about-feedforward-neural-network","text":"","title":"About Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-transition-to-neural-networks","text":"","title":"Logistic Regression Transition to Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-review","text":"Define logistic regression model Import our relevant torch modules. import torch import torch.nn as nn Define our model class. class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate the logistic regression model. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). print ( model ) LogisticRegressionModel ( ( linear ): Linear ( in_features = 784 , out_features = 10 , bias = True ) )","title":"Logistic Regression Review"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-problems","text":"Can represent linear functions well y = 2x + 3 y = 2x + 3 y = x_1 + x_2 y = x_1 + x_2 y = x_1 + 3x_2 + 4x_3 y = x_1 + 3x_2 + 4x_3 Cannot represent non-linear functions y = 4x_1 + 2x_2^2 +3x_3^3 y = 4x_1 + 2x_2^2 +3x_3^3 y = x_1x_2 y = x_1x_2","title":"Logistic Regression Problems"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#introducing-a-non-linear-function","text":"","title":"Introducing a Non-linear Function"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#non-linear-function-in-depth","text":"Function: takes a number & perform mathematical operation Common Types of Non-linearity ReLUs (Rectified Linear Units) Sigmoid Tanh","title":"Non-linear Function In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#sigmoid-logistic","text":"\\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this Outputs not centered around 0 If output always positive \\rightarrow \\rightarrow gradients always positive or negative \\rightarrow \\rightarrow bad for gradient updates","title":"Sigmoid (Logistic)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#tanh","text":"\\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this","title":"Tanh"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#relus","text":"f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate choice","title":"ReLUs"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#building-a-feedforward-neural-network-with-pytorch","text":"","title":"Building a Feedforward Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-a-1-hidden-layer-feedforward-neural-network-sigmoid-activation","text":"","title":"Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ())","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-2-make-dataset-iterable","text":"Batch sizes and iterations Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently. There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would: (1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. (2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests! If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. 60000 / 100 600.0 Epochs An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations. If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). 600 * 5 3000.0 Bringing batch size, iterations and epochs together As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100. Because we want 5 epochs, we need a total of 3000 iterations. batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-3-create-model-class","text":"Creating our feedforward neural network Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. This translates to just 4 more lines of code! class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . sigmoid = nn . Sigmoid () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function # LINEAR out = self . fc1 ( x ) # Non-linearity # NON-LINEAR out = self . sigmoid ( out ) # Linear function (readout) # LINEAR out = self . fc2 ( out ) return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-4-instantiate-model-class","text":"Input dimension: 784 Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Hidden dimension: 100 Can be any number Similar term Number of neurons Number of non-linear activation functions Instantiating our model class Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28). Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10. Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results. On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-5-instantiate-loss-class","text":"Feedforward Neural Network: Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Loss class This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels. criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation capabilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer class Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities. Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section. learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#parameters-in-depth","text":"Linear layers' parameters In a simple linear layer it's Y = AX + B Y = AX + B , and our parameters are A A and bias B B . Hence, each linear layer would have 2 groups of parameters A A and B B . It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on Y Y , the output of our linear layers. This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. print ( model . parameters ()) Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters A A and b b . print ( len ( list ( model . parameters ()))) Our first linear layer parameters, A_1 A_1 , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) Our first linear layer bias parameters, B_1 B_1 , would be of size 100 which is our hidden size. # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) Our second linear layer is our readout layer, where the parameters A_2 A_2 would be of size 10 x 100. This is because our output size is 10 and hidden size is 100. # FC 2 Parameters print ( list ( model . parameters ())[ 2 ] . size ()) Likewise our readout layer's bias B_1 B_1 would just be 10, the size of our output. # FC 2 Bias Parameters print ( list ( model . parameters ())[ 3 ] . size ()) The diagram below shows the interaction amongst our input X X and our linear layers' parameters A_1 A_1 , B_1 B_1 , A_2 A_2 , and B_2 B_2 to reach to the final size of 10 x 1. If you're still unfamiliar with matrix product, go ahead and review the previous quick lesson where we covered it in logistic regression . < generator object Module . parameters at 0x7f1d530fa678 > 4 torch . Size ([ 100 , 784 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-7-train-model","text":"Process Convert inputs to tensors with gradient accumulation capabilities Clear gradient buffers Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT 7-step training process iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.6457265615463257 . Accuracy : 85 Iteration : 1000. Loss : 0.39627206325531006 . Accuracy : 89 Iteration : 1500. Loss : 0.2831554412841797 . Accuracy : 90 Iteration : 2000. Loss : 0.4409525394439697 . Accuracy : 91 Iteration : 2500. Loss : 0.2397005707025528 . Accuracy : 91 Iteration : 3000. Loss : 0.3160165846347809 . Accuracy : 91","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-b-1-hidden-layer-feedforward-neural-network-tanh-activation","text":"","title":"Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with Tanh Activation The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.4128190577030182 . Accuracy : 91 Iteration : 1000. Loss : 0.14497484266757965 . Accuracy : 92 Iteration : 1500. Loss : 0.272532194852829 . Accuracy : 93 Iteration : 2000. Loss : 0.2758277952671051 . Accuracy : 94 Iteration : 2500. Loss : 0.1603182554244995 . Accuracy : 94 Iteration : 3000. Loss : 0.08848697692155838 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-c-1-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with ReLU Activation The only difference again is in using ReLU activation and it affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3179700970649719 . Accuracy : 91 Iteration : 1000. Loss : 0.17288273572921753 . Accuracy : 93 Iteration : 1500. Loss : 0.16829034686088562 . Accuracy : 94 Iteration : 2000. Loss : 0.25494423508644104 . Accuracy : 94 Iteration : 2500. Loss : 0.16818439960479736 . Accuracy : 95 Iteration : 3000. Loss : 0.11110792309045792 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-d-2-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2-layer FNN with ReLU Activation This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3 (readout): 100 --> 10 self . fc3 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 3 (readout) out = self . fc3 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.2995373010635376 . Accuracy : 91 Iteration : 1000. Loss : 0.3924565613269806 . Accuracy : 93 Iteration : 1500. Loss : 0.1283276081085205 . Accuracy : 94 Iteration : 2000. Loss : 0.10905527323484421 . Accuracy : 95 Iteration : 2500. Loss : 0.11943754553794861 . Accuracy : 96 Iteration : 3000. Loss : 0.15632082521915436 . Accuracy : 96","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-e-3-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_4","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality? import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.33234935998916626 . Accuracy : 89 Iteration : 1000. Loss : 0.3098006248474121 . Accuracy : 94 Iteration : 1500. Loss : 0.12461677193641663 . Accuracy : 95 Iteration : 2000. Loss : 0.14346086978912354 . Accuracy : 96 Iteration : 2500. Loss : 0.03763459622859955 . Accuracy : 96 Iteration : 3000. Loss : 0.1397182047367096 . Accuracy : 97","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#general-comments-on-fnns","text":"2 ways to expand a neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Comments on FNNs"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#3-building-a-feedforward-neural-network-with-pytorch-gpu","text":"GPU: 2 things must be on GPU - model - tensors","title":"3. Building a Feedforward Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_5","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation on GPU Only step 4 and 7 of the CPU code will be affected and it's a simple change. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3877025246620178 . Accuracy : 90 Iteration : 1000. Loss : 0.1337055265903473 . Accuracy : 93 Iteration : 1500. Loss : 0.2038637101650238 . Accuracy : 95 Iteration : 2000. Loss : 0.17892278730869293 . Accuracy : 95 Iteration : 2500. Loss : 0.14455552399158478 . Accuracy : 96 Iteration : 3000. Loss : 0.024540524929761887 . Accuracy : 96 Alternative Term of Neural Network The alternative term is Universal Function Approximator . This is because ultimately we are trying to find a function that maps our input, X X , to our output, y y .","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#summary","text":"We've learnt to... Success Logistic Regression Problems for Non-Linear Functions Representation Cannot represent non-linear functions $ y = 4x_1 + 2x_2^2 +3x_3^3 $ $ y = x_1x_2$ Introduced Non-Linearity to Logistic Regression to form a Neural Network Types of Non-Linearity Sigmoid Tanh ReLU Feedforward Neural Network Models Model A: 1 hidden layer ( sigmoid activation) Model B: 1 hidden layer ( tanh activation) Model C: 1 hidden layer ( ReLU activation) Model D: 2 hidden layers (ReLU activation) Model E: 3 hidden layers (ReLU activation) Models Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation capabilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/","text":"Gradients with PyTorch \u00b6 Tensors with Gradients \u00b6 Creating Tensors with Gradients \u00b6 Allows accumulation of gradients Method 1: Create tensor with gradients It is very similar to creating a tensor, all you need to do is to add an additional argument. import torch a = torch . ones (( 2 , 2 ), requires_grad = True ) a tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Check if tensor requires gradients This should return True otherwise you've not done it right. a . requires_grad True Method 2: Create tensor with gradients This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients. # Normal way of creating gradients a = torch . ones (( 2 , 2 )) # Requires gradient a . requires_grad_ () # Check if requires gradient a . requires_grad True A tensor without gradients just for comparison If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable no_gradient = torch . ones ( 2 , 2 ) no_gradient . requires_grad False Tensor with gradients addition operation # Behaves similarly to tensors b = torch . ones (( 2 , 2 ), requires_grad = True ) print ( a + b ) print ( torch . add ( a , b )) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) Tensor with gradients multiplication operation As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a * b ) print ( torch . mul ( a , b )) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Manually and Automatically Calculating Gradients \u00b6 What exactly is requires_grad ? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Create tensor of size 2x1 filled with 1's that requires gradient x = torch . ones ( 2 , requires_grad = True ) x tensor ([ 1. , 1. ]) Simple linear equation with x tensor created y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 We should get a value of 20 by replicating this simple equation y = 5 * ( x + 1 ) ** 2 y tensor ([ 20. , 20. ]) Simple equation with y tensor Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable Let's reduce y to a scalar then... o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i As you can see above, we've a tensor filled with 20's, so average them would return 20 o = ( 1 / 2 ) * torch . sum ( y ) o tensor ( 20. ) Calculating first derivative Recap y equation : y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Recap o equation : o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i Substitute y into o equation : o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 We should expect to get 10, and it's so simple to do this with PyTorch with the following line... Get first derivative: o . backward () Print out first derivative: x . grad tensor ([ 10. , 10. ]) If x requires gradient and you create new objects with it, you get all gradients print ( x . requires_grad ) print ( y . requires_grad ) print ( o . requires_grad ) True True True Summary \u00b6 We've learnt to... Success Tensor with Gradients Wraps a tensor for gradient accumulation Gradients Define original equation Substitute equation with x values Reduce to scalar output, o through mean Calculate gradients with o.backward() Then access gradients of the x tensor with requires_grad through x.grad Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#gradients-with-pytorch","text":"","title":"Gradients with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients","text":"","title":"Tensors with Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients","text":"Allows accumulation of gradients Method 1: Create tensor with gradients It is very similar to creating a tensor, all you need to do is to add an additional argument. import torch a = torch . ones (( 2 , 2 ), requires_grad = True ) a tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Check if tensor requires gradients This should return True otherwise you've not done it right. a . requires_grad True Method 2: Create tensor with gradients This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients. # Normal way of creating gradients a = torch . ones (( 2 , 2 )) # Requires gradient a . requires_grad_ () # Check if requires gradient a . requires_grad True A tensor without gradients just for comparison If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable no_gradient = torch . ones ( 2 , 2 ) no_gradient . requires_grad False Tensor with gradients addition operation # Behaves similarly to tensors b = torch . ones (( 2 , 2 ), requires_grad = True ) print ( a + b ) print ( torch . add ( a , b )) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) Tensor with gradients multiplication operation As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a * b ) print ( torch . mul ( a , b )) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) tensor ([[ 1. , 1. ], [ 1. , 1. ]])","title":"Creating Tensors with Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients","text":"What exactly is requires_grad ? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Create tensor of size 2x1 filled with 1's that requires gradient x = torch . ones ( 2 , requires_grad = True ) x tensor ([ 1. , 1. ]) Simple linear equation with x tensor created y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 We should get a value of 20 by replicating this simple equation y = 5 * ( x + 1 ) ** 2 y tensor ([ 20. , 20. ]) Simple equation with y tensor Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable Let's reduce y to a scalar then... o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i As you can see above, we've a tensor filled with 20's, so average them would return 20 o = ( 1 / 2 ) * torch . sum ( y ) o tensor ( 20. ) Calculating first derivative Recap y equation : y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Recap o equation : o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i Substitute y into o equation : o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 We should expect to get 10, and it's so simple to do this with PyTorch with the following line... Get first derivative: o . backward () Print out first derivative: x . grad tensor ([ 10. , 10. ]) If x requires gradient and you create new objects with it, you get all gradients print ( x . requires_grad ) print ( y . requires_grad ) print ( o . requires_grad ) True True True","title":"Manually and Automatically Calculating Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#summary","text":"We've learnt to... Success Tensor with Gradients Wraps a tensor for gradient accumulation Gradients Define original equation Substitute equation with x values Reduce to scalar output, o through mean Calculate gradients with o.backward() Then access gradients of the x tensor with requires_grad through x.grad","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/","text":"Linear Regression with PyTorch \u00b6 About Linear Regression \u00b6 Simple Linear Regression Basics \u00b6 Allows us to understand relationship between two continuous variables Example x: independent variable weight y: dependent variable height y = \\alpha x + \\beta y = \\alpha x + \\beta Example of simple linear regression \u00b6 Create plot for simple linear regression Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists. import numpy as np import matplotlib.pyplot as plt % matplotlib inline # Creates 50 random x and y numbers np . random . seed ( 1 ) n = 50 x = np . random . randn ( n ) y = x * np . random . randn ( n ) # Makes the dots colorful colors = np . random . rand ( n ) # Plots best-fit line via polyfit plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) # Plots the random x and y data points we created # Interestingly, alpha makes it more aesthetically pleasing plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Aim of Linear Regression \u00b6 Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta ) Adjusting Coefficient: \\alpha \\alpha Bias/intercept: \\beta \\beta Building a Linear Regression Model with PyTorch \u00b6 Example \u00b6 Coefficient: \\alpha = 2 \\alpha = 2 Bias/intercept: \\beta = 1 \\beta = 1 Equation: y = 2x + 1 y = 2x + 1 Building a Toy Dataset \u00b6 Create a list of values from 0 to 11 x_values = [ i for i in range ( 11 )] x_values [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Convert list of numbers to numpy array # Convert to numpy x_train = np . array ( x_values , dtype = np . float32 ) x_train . shape ( 11 ,) Convert to 2-dimensional array If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required x_train = x_train . reshape ( - 1 , 1 ) x_train . shape ( 11 , 1 ) Create list of y values We want y values for every x value we have above. y = 2x + 1 y = 2x + 1 y_values = [ 2 * i + 1 for i in x_values ] y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Alternative to create list of y values If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators... y_values = [] for i in x_values : result = 2 * i + 1 y_values . append ( result ) y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Convert to numpy array You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good. y_train = np . array ( y_values , dtype = np . float32 ) y_train . shape ( 11 ,) Reshape y numpy array to 2-dimension # IMPORTANT: 2D required y_train = y_train . reshape ( - 1 , 1 ) y_train . shape ( 11 , 1 ) Building Model \u00b6 Critical Imports import torch import torch.nn as nn Create Model Linear model True Equation: y = 2x + 1 y = 2x + 1 Forward Example Input x = 1 x = 1 Output \\hat y = ? \\hat y = ? # Create class class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate Model Class input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21] input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) Instantiate Loss Class MSE Loss: Mean Squared Error MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) \\hat y \\hat y : prediction y y : true value criterion = nn . MSELoss () Instantiate Optimizer Class Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients parameters: \\alpha \\alpha and \\beta \\beta in y = \\alpha x + \\beta y = \\alpha x + \\beta desired parameters: \\alpha = 2 \\alpha = 2 and \\beta = 1 \\beta = 1 in y = 2x + 1 y = 2x + 1 learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Train Model 1 epoch: going through the whole x_train data once 100 epochs: 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 140.58143615722656 epoch 2 , loss 11.467253684997559 epoch 3 , loss 0.9358152747154236 epoch 4 , loss 0.07679400593042374 epoch 5 , loss 0.0067212567664682865 epoch 6 , loss 0.0010006226366385818 epoch 7 , loss 0.0005289533291943371 epoch 8 , loss 0.0004854927829001099 epoch 9 , loss 0.00047700389404781163 epoch 10 , loss 0.0004714332753792405 epoch 11 , loss 0.00046614606981165707 epoch 12 , loss 0.0004609318566508591 epoch 13 , loss 0.0004557870561257005 epoch 14 , loss 0.00045069155748933554 epoch 15 , loss 0.00044567222357727587 epoch 16 , loss 0.00044068993884138763 epoch 17 , loss 0.00043576463940553367 epoch 18 , loss 0.00043090470717288554 epoch 19 , loss 0.00042609183583408594 epoch 20 , loss 0.0004213254142086953 epoch 21 , loss 0.0004166301223449409 epoch 22 , loss 0.0004119801160413772 epoch 23 , loss 0.00040738462121225893 epoch 24 , loss 0.0004028224211651832 epoch 25 , loss 0.0003983367350883782 epoch 26 , loss 0.0003938761365134269 epoch 27 , loss 0.000389480876037851 epoch 28 , loss 0.00038514015614055097 epoch 29 , loss 0.000380824290914461 epoch 30 , loss 0.00037657516077160835 epoch 31 , loss 0.000372376263840124 epoch 32 , loss 0.0003682126116473228 epoch 33 , loss 0.0003640959912445396 epoch 34 , loss 0.00036003670538775623 epoch 35 , loss 0.00035601368290372193 epoch 36 , loss 0.00035203873994760215 epoch 37 , loss 0.00034810820943675935 epoch 38 , loss 0.000344215368386358 epoch 39 , loss 0.0003403784066904336 epoch 40 , loss 0.00033658024040050805 epoch 41 , loss 0.0003328165039420128 epoch 42 , loss 0.0003291067841928452 epoch 43 , loss 0.0003254293987993151 epoch 44 , loss 0.0003217888588551432 epoch 45 , loss 0.0003182037326041609 epoch 46 , loss 0.0003146533854305744 epoch 47 , loss 0.00031113551813177764 epoch 48 , loss 0.0003076607536058873 epoch 49 , loss 0.00030422292184084654 epoch 50 , loss 0.00030083119054324925 epoch 51 , loss 0.00029746422660537064 epoch 52 , loss 0.0002941471466328949 epoch 53 , loss 0.00029085995629429817 epoch 54 , loss 0.0002876132493838668 epoch 55 , loss 0.00028440452297218144 epoch 56 , loss 0.00028122696676291525 epoch 57 , loss 0.00027808290906250477 epoch 58 , loss 0.00027497278642840683 epoch 59 , loss 0.00027190230321139097 epoch 60 , loss 0.00026887087733484805 epoch 61 , loss 0.0002658693410921842 epoch 62 , loss 0.0002629039518069476 epoch 63 , loss 0.00025996880140155554 epoch 64 , loss 0.0002570618235040456 epoch 65 , loss 0.00025419273879379034 epoch 66 , loss 0.00025135406758636236 epoch 67 , loss 0.0002485490695107728 epoch 68 , loss 0.0002457649679854512 epoch 69 , loss 0.0002430236927466467 epoch 70 , loss 0.00024031475186347961 epoch 71 , loss 0.00023762597993481904 epoch 72 , loss 0.00023497406800743192 epoch 73 , loss 0.0002323519001947716 epoch 74 , loss 0.00022976362379267812 epoch 75 , loss 0.0002271933335578069 epoch 76 , loss 0.00022465786605607718 epoch 77 , loss 0.00022214400814846158 epoch 78 , loss 0.00021966728672850877 epoch 79 , loss 0.0002172116219298914 epoch 80 , loss 0.00021478648704942316 epoch 81 , loss 0.00021239375928416848 epoch 82 , loss 0.0002100227284245193 epoch 83 , loss 0.00020767028036061674 epoch 84 , loss 0.00020534756185952574 epoch 85 , loss 0.00020305956422816962 epoch 86 , loss 0.0002007894654525444 epoch 87 , loss 0.00019854879064951092 epoch 88 , loss 0.00019633043848443776 epoch 89 , loss 0.00019413618429098278 epoch 90 , loss 0.00019197272195015103 epoch 91 , loss 0.0001898303598864004 epoch 92 , loss 0.00018771187751553953 epoch 93 , loss 0.00018561164324637502 epoch 94 , loss 0.00018354636267758906 epoch 95 , loss 0.00018149390234611928 epoch 96 , loss 0.0001794644631445408 epoch 97 , loss 0.00017746571393217891 epoch 98 , loss 0.00017548113828524947 epoch 99 , loss 0.00017352371651213616 epoch 100 , loss 0.00017157981346827 Looking at predicted values # Purely inference predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () predicted array ([[ 0.9756333 ], [ 2.9791424 ], [ 4.982651 ], [ 6.9861603 ], [ 8.98967 ], [ 10.993179 ], [ 12.996688 ], [ 15.000196 ], [ 17.003706 ], [ 19.007215 ], [ 21.010725 ]], dtype = float32 ) Looking at training values These are the true values, you can see how it's able to predict similar values. # y = 2x + 1 y_train array ([[ 1. ], [ 3. ], [ 5. ], [ 7. ], [ 9. ], [ 11. ], [ 13. ], [ 15. ], [ 17. ], [ 19. ], [ 21. ]], dtype = float32 ) Plot of predicted and actual values # Clear figure plt . clf () # Get predictions predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () # Plot true data plt . plot ( x_train , y_train , 'go' , label = 'True data' , alpha = 0.5 ) # Plot predictions plt . plot ( x_train , predicted , '--' , label = 'Predictions' , alpha = 0.5 ) # Legend and plot plt . legend ( loc = 'best' ) plt . show () Save Model save_model = False if save_model is True : # Saves only parameters # alpha & beta torch . save ( model . state_dict (), 'awesome_model.pkl' ) Load Model load_model = False if load_model is True : model . load_state_dict ( torch . load ( 'awesome_model.pkl' )) Building a Linear Regression Model with PyTorch (GPU) \u00b6 CPU Summary import torch import torch.nn as nn ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () GPU Summary Just remember always 2 things must be on GPU model tensors import torch import torch.nn as nn import numpy as np ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable ####################### # USE GPU FOR MODEL # ####################### inputs = torch . from_numpy ( x_train ) . to ( device ) labels = torch . from_numpy ( y_train ) . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () # Logging print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 336.0314025878906 epoch 2 , loss 27.67657470703125 epoch 3 , loss 2.5220539569854736 epoch 4 , loss 0.46732547879219055 epoch 5 , loss 0.2968060076236725 epoch 6 , loss 0.2800087630748749 epoch 7 , loss 0.27578213810920715 epoch 8 , loss 0.2726128399372101 epoch 9 , loss 0.269561231136322 epoch 10 , loss 0.2665504515171051 epoch 11 , loss 0.2635740041732788 epoch 12 , loss 0.26063060760498047 epoch 13 , loss 0.2577202618122101 epoch 14 , loss 0.2548423111438751 epoch 15 , loss 0.25199657678604126 epoch 16 , loss 0.24918246269226074 epoch 17 , loss 0.24639996886253357 epoch 18 , loss 0.24364829063415527 epoch 19 , loss 0.24092751741409302 epoch 20 , loss 0.2382371574640274 epoch 21 , loss 0.23557686805725098 epoch 22 , loss 0.2329462170600891 epoch 23 , loss 0.2303449958562851 epoch 24 , loss 0.22777271270751953 epoch 25 , loss 0.2252292037010193 epoch 26 , loss 0.22271405160427094 epoch 27 , loss 0.22022713720798492 epoch 28 , loss 0.21776780486106873 epoch 29 , loss 0.21533599495887756 epoch 30 , loss 0.21293145418167114 epoch 31 , loss 0.21055366098880768 epoch 32 , loss 0.20820240676403046 epoch 33 , loss 0.2058774083852768 epoch 34 , loss 0.20357847213745117 epoch 35 , loss 0.20130516588687897 epoch 36 , loss 0.1990572065114975 epoch 37 , loss 0.19683438539505005 epoch 38 , loss 0.19463638961315155 epoch 39 , loss 0.19246290624141693 epoch 40 , loss 0.1903136670589447 epoch 41 , loss 0.1881885528564453 epoch 42 , loss 0.18608702719211578 epoch 43 , loss 0.18400898575782776 epoch 44 , loss 0.18195408582687378 epoch 45 , loss 0.17992223799228668 epoch 46 , loss 0.17791320383548737 epoch 47 , loss 0.17592646181583405 epoch 48 , loss 0.17396186292171478 epoch 49 , loss 0.17201924324035645 epoch 50 , loss 0.17009828984737396 epoch 51 , loss 0.16819894313812256 epoch 52 , loss 0.16632060706615448 epoch 53 , loss 0.16446338593959808 epoch 54 , loss 0.16262666881084442 epoch 55 , loss 0.16081078350543976 epoch 56 , loss 0.15901507437229156 epoch 57 , loss 0.15723931789398193 epoch 58 , loss 0.15548335015773773 epoch 59 , loss 0.15374726057052612 epoch 60 , loss 0.1520303338766098 epoch 61 , loss 0.15033268928527832 epoch 62 , loss 0.14865389466285706 epoch 63 , loss 0.14699392020702362 epoch 64 , loss 0.14535246789455414 epoch 65 , loss 0.14372935891151428 epoch 66 , loss 0.14212435483932495 epoch 67 , loss 0.14053721725940704 epoch 68 , loss 0.13896773755550385 epoch 69 , loss 0.1374160647392273 epoch 70 , loss 0.1358814686536789 epoch 71 , loss 0.13436420261859894 epoch 72 , loss 0.13286370038986206 epoch 73 , loss 0.1313801407814026 epoch 74 , loss 0.12991292774677277 epoch 75 , loss 0.12846232950687408 epoch 76 , loss 0.1270277351140976 epoch 77 , loss 0.12560924887657166 epoch 78 , loss 0.12420656532049179 epoch 79 , loss 0.12281957268714905 epoch 80 , loss 0.1214480847120285 epoch 81 , loss 0.12009195983409882 epoch 82 , loss 0.1187509223818779 epoch 83 , loss 0.11742479354143143 epoch 84 , loss 0.11611353605985641 epoch 85 , loss 0.11481687426567078 epoch 86 , loss 0.11353478580713272 epoch 87 , loss 0.11226697266101837 epoch 88 , loss 0.11101329326629639 epoch 89 , loss 0.10977360606193542 epoch 90 , loss 0.10854770988225937 epoch 91 , loss 0.10733554512262344 epoch 92 , loss 0.10613703727722168 epoch 93 , loss 0.10495180636644363 epoch 94 , loss 0.10377981513738632 epoch 95 , loss 0.10262089222669601 epoch 96 , loss 0.10147502273321152 epoch 97 , loss 0.1003417894244194 epoch 98 , loss 0.09922132641077042 epoch 99 , loss 0.0981132984161377 epoch 100 , loss 0.09701769798994064 Summary \u00b6 We've learnt to... Success Simple linear regression basics y = Ax + B y = Ax + B y = 2x + 1 y = 2x + 1 Example of simple linear regression Aim of linear regression Minimizing distance between the points and the line Calculate \"distance\" through MSE Calculate gradients Update parameters with parameters = parameters - learning_rate * gradients Slowly update parameters A A and B B model the linear relationship between y y and x x of the form y = 2x + 1 y = 2x + 1 Built a linear regression model in CPU and GPU Step 1: Create Model Class Step 2: Instantiate Model Class Step 3: Instantiate Loss Class Step 4: Instantiate Optimizer Class Step 5: Train Model Important things to be on GPU model tensors with gradients How to bring to GPU ? model_name.to(device) variable_name.to(device) Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#linear-regression-with-pytorch","text":"","title":"Linear Regression with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#about-linear-regression","text":"","title":"About Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#simple-linear-regression-basics","text":"Allows us to understand relationship between two continuous variables Example x: independent variable weight y: dependent variable height y = \\alpha x + \\beta y = \\alpha x + \\beta","title":"Simple Linear Regression Basics"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example-of-simple-linear-regression","text":"Create plot for simple linear regression Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists. import numpy as np import matplotlib.pyplot as plt % matplotlib inline # Creates 50 random x and y numbers np . random . seed ( 1 ) n = 50 x = np . random . randn ( n ) y = x * np . random . randn ( n ) # Makes the dots colorful colors = np . random . rand ( n ) # Plots best-fit line via polyfit plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) # Plots the random x and y data points we created # Interestingly, alpha makes it more aesthetically pleasing plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ()","title":"Example of simple linear regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#aim-of-linear-regression","text":"Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta ) Adjusting Coefficient: \\alpha \\alpha Bias/intercept: \\beta \\beta","title":"Aim of Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch","text":"","title":"Building a Linear Regression Model with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example","text":"Coefficient: \\alpha = 2 \\alpha = 2 Bias/intercept: \\beta = 1 \\beta = 1 Equation: y = 2x + 1 y = 2x + 1","title":"Example"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-toy-dataset","text":"Create a list of values from 0 to 11 x_values = [ i for i in range ( 11 )] x_values [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Convert list of numbers to numpy array # Convert to numpy x_train = np . array ( x_values , dtype = np . float32 ) x_train . shape ( 11 ,) Convert to 2-dimensional array If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required x_train = x_train . reshape ( - 1 , 1 ) x_train . shape ( 11 , 1 ) Create list of y values We want y values for every x value we have above. y = 2x + 1 y = 2x + 1 y_values = [ 2 * i + 1 for i in x_values ] y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Alternative to create list of y values If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators... y_values = [] for i in x_values : result = 2 * i + 1 y_values . append ( result ) y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Convert to numpy array You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good. y_train = np . array ( y_values , dtype = np . float32 ) y_train . shape ( 11 ,) Reshape y numpy array to 2-dimension # IMPORTANT: 2D required y_train = y_train . reshape ( - 1 , 1 ) y_train . shape ( 11 , 1 )","title":"Building a Toy Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-model","text":"Critical Imports import torch import torch.nn as nn Create Model Linear model True Equation: y = 2x + 1 y = 2x + 1 Forward Example Input x = 1 x = 1 Output \\hat y = ? \\hat y = ? # Create class class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate Model Class input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21] input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) Instantiate Loss Class MSE Loss: Mean Squared Error MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) \\hat y \\hat y : prediction y y : true value criterion = nn . MSELoss () Instantiate Optimizer Class Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients parameters: \\alpha \\alpha and \\beta \\beta in y = \\alpha x + \\beta y = \\alpha x + \\beta desired parameters: \\alpha = 2 \\alpha = 2 and \\beta = 1 \\beta = 1 in y = 2x + 1 y = 2x + 1 learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Train Model 1 epoch: going through the whole x_train data once 100 epochs: 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 140.58143615722656 epoch 2 , loss 11.467253684997559 epoch 3 , loss 0.9358152747154236 epoch 4 , loss 0.07679400593042374 epoch 5 , loss 0.0067212567664682865 epoch 6 , loss 0.0010006226366385818 epoch 7 , loss 0.0005289533291943371 epoch 8 , loss 0.0004854927829001099 epoch 9 , loss 0.00047700389404781163 epoch 10 , loss 0.0004714332753792405 epoch 11 , loss 0.00046614606981165707 epoch 12 , loss 0.0004609318566508591 epoch 13 , loss 0.0004557870561257005 epoch 14 , loss 0.00045069155748933554 epoch 15 , loss 0.00044567222357727587 epoch 16 , loss 0.00044068993884138763 epoch 17 , loss 0.00043576463940553367 epoch 18 , loss 0.00043090470717288554 epoch 19 , loss 0.00042609183583408594 epoch 20 , loss 0.0004213254142086953 epoch 21 , loss 0.0004166301223449409 epoch 22 , loss 0.0004119801160413772 epoch 23 , loss 0.00040738462121225893 epoch 24 , loss 0.0004028224211651832 epoch 25 , loss 0.0003983367350883782 epoch 26 , loss 0.0003938761365134269 epoch 27 , loss 0.000389480876037851 epoch 28 , loss 0.00038514015614055097 epoch 29 , loss 0.000380824290914461 epoch 30 , loss 0.00037657516077160835 epoch 31 , loss 0.000372376263840124 epoch 32 , loss 0.0003682126116473228 epoch 33 , loss 0.0003640959912445396 epoch 34 , loss 0.00036003670538775623 epoch 35 , loss 0.00035601368290372193 epoch 36 , loss 0.00035203873994760215 epoch 37 , loss 0.00034810820943675935 epoch 38 , loss 0.000344215368386358 epoch 39 , loss 0.0003403784066904336 epoch 40 , loss 0.00033658024040050805 epoch 41 , loss 0.0003328165039420128 epoch 42 , loss 0.0003291067841928452 epoch 43 , loss 0.0003254293987993151 epoch 44 , loss 0.0003217888588551432 epoch 45 , loss 0.0003182037326041609 epoch 46 , loss 0.0003146533854305744 epoch 47 , loss 0.00031113551813177764 epoch 48 , loss 0.0003076607536058873 epoch 49 , loss 0.00030422292184084654 epoch 50 , loss 0.00030083119054324925 epoch 51 , loss 0.00029746422660537064 epoch 52 , loss 0.0002941471466328949 epoch 53 , loss 0.00029085995629429817 epoch 54 , loss 0.0002876132493838668 epoch 55 , loss 0.00028440452297218144 epoch 56 , loss 0.00028122696676291525 epoch 57 , loss 0.00027808290906250477 epoch 58 , loss 0.00027497278642840683 epoch 59 , loss 0.00027190230321139097 epoch 60 , loss 0.00026887087733484805 epoch 61 , loss 0.0002658693410921842 epoch 62 , loss 0.0002629039518069476 epoch 63 , loss 0.00025996880140155554 epoch 64 , loss 0.0002570618235040456 epoch 65 , loss 0.00025419273879379034 epoch 66 , loss 0.00025135406758636236 epoch 67 , loss 0.0002485490695107728 epoch 68 , loss 0.0002457649679854512 epoch 69 , loss 0.0002430236927466467 epoch 70 , loss 0.00024031475186347961 epoch 71 , loss 0.00023762597993481904 epoch 72 , loss 0.00023497406800743192 epoch 73 , loss 0.0002323519001947716 epoch 74 , loss 0.00022976362379267812 epoch 75 , loss 0.0002271933335578069 epoch 76 , loss 0.00022465786605607718 epoch 77 , loss 0.00022214400814846158 epoch 78 , loss 0.00021966728672850877 epoch 79 , loss 0.0002172116219298914 epoch 80 , loss 0.00021478648704942316 epoch 81 , loss 0.00021239375928416848 epoch 82 , loss 0.0002100227284245193 epoch 83 , loss 0.00020767028036061674 epoch 84 , loss 0.00020534756185952574 epoch 85 , loss 0.00020305956422816962 epoch 86 , loss 0.0002007894654525444 epoch 87 , loss 0.00019854879064951092 epoch 88 , loss 0.00019633043848443776 epoch 89 , loss 0.00019413618429098278 epoch 90 , loss 0.00019197272195015103 epoch 91 , loss 0.0001898303598864004 epoch 92 , loss 0.00018771187751553953 epoch 93 , loss 0.00018561164324637502 epoch 94 , loss 0.00018354636267758906 epoch 95 , loss 0.00018149390234611928 epoch 96 , loss 0.0001794644631445408 epoch 97 , loss 0.00017746571393217891 epoch 98 , loss 0.00017548113828524947 epoch 99 , loss 0.00017352371651213616 epoch 100 , loss 0.00017157981346827 Looking at predicted values # Purely inference predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () predicted array ([[ 0.9756333 ], [ 2.9791424 ], [ 4.982651 ], [ 6.9861603 ], [ 8.98967 ], [ 10.993179 ], [ 12.996688 ], [ 15.000196 ], [ 17.003706 ], [ 19.007215 ], [ 21.010725 ]], dtype = float32 ) Looking at training values These are the true values, you can see how it's able to predict similar values. # y = 2x + 1 y_train array ([[ 1. ], [ 3. ], [ 5. ], [ 7. ], [ 9. ], [ 11. ], [ 13. ], [ 15. ], [ 17. ], [ 19. ], [ 21. ]], dtype = float32 ) Plot of predicted and actual values # Clear figure plt . clf () # Get predictions predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () # Plot true data plt . plot ( x_train , y_train , 'go' , label = 'True data' , alpha = 0.5 ) # Plot predictions plt . plot ( x_train , predicted , '--' , label = 'Predictions' , alpha = 0.5 ) # Legend and plot plt . legend ( loc = 'best' ) plt . show () Save Model save_model = False if save_model is True : # Saves only parameters # alpha & beta torch . save ( model . state_dict (), 'awesome_model.pkl' ) Load Model load_model = False if load_model is True : model . load_state_dict ( torch . load ( 'awesome_model.pkl' ))","title":"Building Model"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch-gpu","text":"CPU Summary import torch import torch.nn as nn ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () GPU Summary Just remember always 2 things must be on GPU model tensors import torch import torch.nn as nn import numpy as np ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable ####################### # USE GPU FOR MODEL # ####################### inputs = torch . from_numpy ( x_train ) . to ( device ) labels = torch . from_numpy ( y_train ) . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () # Logging print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 336.0314025878906 epoch 2 , loss 27.67657470703125 epoch 3 , loss 2.5220539569854736 epoch 4 , loss 0.46732547879219055 epoch 5 , loss 0.2968060076236725 epoch 6 , loss 0.2800087630748749 epoch 7 , loss 0.27578213810920715 epoch 8 , loss 0.2726128399372101 epoch 9 , loss 0.269561231136322 epoch 10 , loss 0.2665504515171051 epoch 11 , loss 0.2635740041732788 epoch 12 , loss 0.26063060760498047 epoch 13 , loss 0.2577202618122101 epoch 14 , loss 0.2548423111438751 epoch 15 , loss 0.25199657678604126 epoch 16 , loss 0.24918246269226074 epoch 17 , loss 0.24639996886253357 epoch 18 , loss 0.24364829063415527 epoch 19 , loss 0.24092751741409302 epoch 20 , loss 0.2382371574640274 epoch 21 , loss 0.23557686805725098 epoch 22 , loss 0.2329462170600891 epoch 23 , loss 0.2303449958562851 epoch 24 , loss 0.22777271270751953 epoch 25 , loss 0.2252292037010193 epoch 26 , loss 0.22271405160427094 epoch 27 , loss 0.22022713720798492 epoch 28 , loss 0.21776780486106873 epoch 29 , loss 0.21533599495887756 epoch 30 , loss 0.21293145418167114 epoch 31 , loss 0.21055366098880768 epoch 32 , loss 0.20820240676403046 epoch 33 , loss 0.2058774083852768 epoch 34 , loss 0.20357847213745117 epoch 35 , loss 0.20130516588687897 epoch 36 , loss 0.1990572065114975 epoch 37 , loss 0.19683438539505005 epoch 38 , loss 0.19463638961315155 epoch 39 , loss 0.19246290624141693 epoch 40 , loss 0.1903136670589447 epoch 41 , loss 0.1881885528564453 epoch 42 , loss 0.18608702719211578 epoch 43 , loss 0.18400898575782776 epoch 44 , loss 0.18195408582687378 epoch 45 , loss 0.17992223799228668 epoch 46 , loss 0.17791320383548737 epoch 47 , loss 0.17592646181583405 epoch 48 , loss 0.17396186292171478 epoch 49 , loss 0.17201924324035645 epoch 50 , loss 0.17009828984737396 epoch 51 , loss 0.16819894313812256 epoch 52 , loss 0.16632060706615448 epoch 53 , loss 0.16446338593959808 epoch 54 , loss 0.16262666881084442 epoch 55 , loss 0.16081078350543976 epoch 56 , loss 0.15901507437229156 epoch 57 , loss 0.15723931789398193 epoch 58 , loss 0.15548335015773773 epoch 59 , loss 0.15374726057052612 epoch 60 , loss 0.1520303338766098 epoch 61 , loss 0.15033268928527832 epoch 62 , loss 0.14865389466285706 epoch 63 , loss 0.14699392020702362 epoch 64 , loss 0.14535246789455414 epoch 65 , loss 0.14372935891151428 epoch 66 , loss 0.14212435483932495 epoch 67 , loss 0.14053721725940704 epoch 68 , loss 0.13896773755550385 epoch 69 , loss 0.1374160647392273 epoch 70 , loss 0.1358814686536789 epoch 71 , loss 0.13436420261859894 epoch 72 , loss 0.13286370038986206 epoch 73 , loss 0.1313801407814026 epoch 74 , loss 0.12991292774677277 epoch 75 , loss 0.12846232950687408 epoch 76 , loss 0.1270277351140976 epoch 77 , loss 0.12560924887657166 epoch 78 , loss 0.12420656532049179 epoch 79 , loss 0.12281957268714905 epoch 80 , loss 0.1214480847120285 epoch 81 , loss 0.12009195983409882 epoch 82 , loss 0.1187509223818779 epoch 83 , loss 0.11742479354143143 epoch 84 , loss 0.11611353605985641 epoch 85 , loss 0.11481687426567078 epoch 86 , loss 0.11353478580713272 epoch 87 , loss 0.11226697266101837 epoch 88 , loss 0.11101329326629639 epoch 89 , loss 0.10977360606193542 epoch 90 , loss 0.10854770988225937 epoch 91 , loss 0.10733554512262344 epoch 92 , loss 0.10613703727722168 epoch 93 , loss 0.10495180636644363 epoch 94 , loss 0.10377981513738632 epoch 95 , loss 0.10262089222669601 epoch 96 , loss 0.10147502273321152 epoch 97 , loss 0.1003417894244194 epoch 98 , loss 0.09922132641077042 epoch 99 , loss 0.0981132984161377 epoch 100 , loss 0.09701769798994064","title":"Building a Linear Regression Model with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#summary","text":"We've learnt to... Success Simple linear regression basics y = Ax + B y = Ax + B y = 2x + 1 y = 2x + 1 Example of simple linear regression Aim of linear regression Minimizing distance between the points and the line Calculate \"distance\" through MSE Calculate gradients Update parameters with parameters = parameters - learning_rate * gradients Slowly update parameters A A and B B model the linear relationship between y y and x x of the form y = 2x + 1 y = 2x + 1 Built a linear regression model in CPU and GPU Step 1: Create Model Class Step 2: Instantiate Model Class Step 3: Instantiate Loss Class Step 4: Instantiate Optimizer Class Step 5: Train Model Important things to be on GPU model tensors with gradients How to bring to GPU ? model_name.to(device) variable_name.to(device)","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/","text":"Logistic Regression with PyTorch \u00b6 About Logistic Regression \u00b6 Logistic Regression Basics \u00b6 Classification algorithm \u00b6 Example: Spam vs No Spam Input: Bunch of words Output: Probability spam or not Basic Comparison \u00b6 Linear regression Output: numeric value given inputs Logistic regression : Output: probability [0, 1] given input belonging to a class Input/Output Comparison \u00b6 Linear regression: Multiplication Input: [1] Output: 2 Input: [2] Output: 4 Trying to model the relationship y = 2x Logistic regression: Spam Input: \"Sign up to get 1 million dollars by tonight\" Output: p = 0.8 Input: \"This is a receipt for your recent purchase with Amazon\" Output: p = 0.3 p: probability it is spam Problems of Linear Regression \u00b6 Example Fever Input : temperature Output : fever or no fever Remember Linear regression : minimize error between points and line Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1) If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import numpy as np import matplotlib.pyplot as plt % matplotlib inline x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 100 ,] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Linear Regression Problem 2: Fever points are not predicted with the presence of outliers Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import numpy as np import matplotlib.pyplot as plt x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 300 ] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Logistic Regression In-Depth \u00b6 Predicting Probability \u00b6 Linear regression doesn't work Instead of predicting direct values: predict probability Logistic Function g() \u00b6 \"Two-class logistic regression\" \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 2-class prediction y_0 y_0 and y_1 y_1 Where the labels are y_0 = 0 y_0 = 0 and y_1 = 1 y_1 = 1 Also, it's bolded because it's a vector, not a matrix. g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) g(y_1) = Estimated probability that y = 1 y = 1 g(y_0) = 1 - g(y_1) g(y_0) = 1 - g(y_1) g(y_0) g(y_0) = Estimated probability that y = 0 y = 0 For our illustration above, we have 4 classes, so we have to use softmax function explained below Softmax Function g() \u00b6 \"Multi-class logistic regression\" Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple. \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 4-class prediction y_0, y_1, y_2, y_3 y_0, y_1, y_2, y_3 Where the 4 labels (K = 4) are y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} where K = 4 because we have 4 classes To put numbers to this equation in relation to the illustration above where we've y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 All softmax outputs have to sum to one as they represent a probability distribution over K classes. Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so). y_0 y_0 and y_1 y_1 are approximately similar in values and they return similar probabilities. Similarly, y_2 y_2 and y_3 y_3 are approximately similar in values and they return similar probabilities. Softmax versus Soft(arg)max Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max. This is because soft(arg)max returns the probability distribution over K classes, a vector. However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution. According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip. Cross Entropy Function D() for 2 Class \u00b6 Take note that here, S S is our softmax outputs and L L are our labels D(S, L) = -(L log S + (1-L)log(1-S)) D(S, L) = -(L log S + (1-L)log(1-S)) If L = 0 (label) D(S, 0) = - log(1-S) D(S, 0) = - log(1-S) - log(1-S) - log(1-S) : less positive if S \\longrightarrow 0 S \\longrightarrow 0 - log(1-S) - log(1-S) : more positive if S \\longrightarrow 1 S \\longrightarrow 1 (BIGGER LOSS) If L = 1 (label) D(S, 1) = - log S D(S, 1) = - log S -log(S) -log(S) : less positive if S \\longrightarrow 1 S \\longrightarrow 1 -log(S) -log(S) : more positive if S \\longrightarrow 0 S \\longrightarrow 0 (BIGGER LOSS) Numerical example of bigger or small loss You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import math print ( - math . log ( 1 - 0.00001 )) You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1 - 0.99999 )) You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( - math . log ( 0.99999 )) You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( - math . log ( 0.00001 )) 1.0000050000287824e-05 11.51292546497478 1.0000050000287824e-05 11.512925464970229 Cross Entropy Function D() for More Than 2 Class \u00b6 For the case where we have more than 2 class, we need a more generalized function D(S, L) = - \\sum^K_1 L_i log(S_i) D(S, L) = - \\sum^K_1 L_i log(S_i) K K : number of classes L_i L_i : label of i-th class, 1 if that's the class else 0 S_i S_i : output of softmax for i-th class Cross Entropy Loss over N samples \u00b6 Goal: Minimizing Cross Entropy Loss, L Loss = \\frac {1}{N} \\sum_j^N D_j Loss = \\frac {1}{N} \\sum_j^N D_j D_j D_j : j-th sample of cross entropy function D(S, L) D(S, L) N N : number of samples Loss Loss : average cross entropy loss over N samples Building a Logistic Regression Model with PyTorch \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1a: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Inspect length of training dataset You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) len ( train_dataset ) 60000 Inspecting a single image So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ] ( tensor ([[[ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0118 , 0.0706 , 0.0706 , 0.0706 , 0.4941 , 0.5333 , 0.6863 , 0.1020 , 0.6510 , 1.0000 , 0.9686 , 0.4980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1176 , 0.1412 , 0.3686 , 0.6039 , 0.6667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.8824 , 0.6745 , 0.9922 , 0.9490 , 0.7647 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1922 , 0.9333 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9843 , 0.3647 , 0.3216 , 0.3216 , 0.2196 , 0.1529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.7137 , 0.9686 , 0.9451 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3137 , 0.6118 , 0.4196 , 0.9922 , 0.9922 , 0.8039 , 0.0431 , 0.0000 , 0.1686 , 0.6039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0549 , 0.0039 , 0.6039 , 0.9922 , 0.3529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5451 , 0.9922 , 0.7451 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0431 , 0.7451 , 0.9922 , 0.2745 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1373 , 0.9451 , 0.8824 , 0.6275 , 0.4235 , 0.0039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3176 , 0.9412 , 0.9922 , 0.9922 , 0.4667 , 0.0980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1765 , 0.7294 , 0.9922 , 0.9922 , 0.5882 , 0.1059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0627 , 0.3647 , 0.9882 , 0.9922 , 0.7333 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.9765 , 0.9922 , 0.9765 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1804 , 0.5098 , 0.7176 , 0.9922 , 0.9922 , 0.8118 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1529 , 0.5804 , 0.8980 , 0.9922 , 0.9922 , 0.9922 , 0.9804 , 0.7137 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0941 , 0.4471 , 0.8667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7882 , 0.3059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0902 , 0.2588 , 0.8353 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.3176 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.6706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7647 , 0.3137 , 0.0353 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.2157 , 0.6745 , 0.8863 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9569 , 0.5216 , 0.0431 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5333 , 0.9922 , 0.9922 , 0.9922 , 0.8314 , 0.5294 , 0.5176 , 0.0627 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ]]]), tensor ( 5 )) Inspecting a single data point in the training dataset When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label. type ( train_dataset [ 0 ]) tuple Inspecting training dataset first element of tuple This means to access the image, you need to access the first element in the tuple. # Input Matrix train_dataset [ 0 ][ 0 ] . size () # A 28x28 sized image of a digit torch . Size ([ 1 , 28 , 28 ]) Inspecting training dataset second element of tuple The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Displaying MNIST \u00b6 Verifying shape of MNIST image As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel. import matplotlib.pyplot as plt % matplotlib inline import numpy as np train_dataset [ 0 ][ 0 ] . numpy () . shape ( 1 , 28 , 28 ) Plot image of MNIST image show_img = train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label As you would expect, the label is 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Plot second image of MNIST image show_img = train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label We should see 0 here as the label. # Label train_dataset [ 1 ][ 1 ] tensor ( 0 ) Step 1b: Loading MNIST Test Dataset \u00b6 Show our algorithm works beyond the data we have trained on. Out-of-sample Load test dataset Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) len ( test_dataset ) 10000 Test dataset elements Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ]) tuple Test dataset first element in tuple This contains the image matrix, similar to the training set. # Image matrix test_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Plot image sample from test dataset show_img = test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Test dataset second element in tuple # Label test_dataset [ 0 ][ 1 ] tensor ( 7 ) Step 2: Make Dataset Iterable \u00b6 Aim: make the dataset iterable totaldata : 60000 minibatch : 100 Number of examples in 1 iteration iterations : 3000 1 iteration: one mini-batch forward & backward pass epochs 1 epoch: running through the whole dataset once epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 Recap training dataset Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset ) 60000 Defining epochs When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. batch_size = 100 We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters = 3000 One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) num_epochs 5 Create Iterable Object: Training Dataset train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) Check Iterability import collections isinstance ( train_loader , collections . Iterable ) True Create Iterable Object: Testing Dataset # Iterable object test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Check iterability of testing dataset isinstance ( test_loader , collections . Iterable ) True Iterate through dataset This is just a simplified example of what we're doing above where we're creating an iterable object lst to loop through so we can access all the images img_1 and img_2 . Above, the equivalent of lst is train_loader and test_loader . img_1 = np . ones (( 28 , 28 )) img_2 = np . ones (( 28 , 28 )) lst = [ img_1 , img_2 ] # Need to iterate # Think of numbers as the images for i in lst : print ( i . shape ) ( 28 , 28 ) ( 28 , 28 ) Step 3: Building Model \u00b6 Create model class # Same as linear regression! class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Step 4: Instantiate Model Class \u00b6 Input dimension: Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Check size of dataset This should be 28x28. # Size of images train_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Instantiate model class based on input and out dimensions As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Logistic Regression : Cross Entropy Loss Linear Regression: MSE Create Cross Entry Loss Class Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters. criterion = nn . CrossEntropyLoss () What happens in nn.CrossEntropyLoss()? It does 2 things at the same time. 1. Computes softmax (logistic/softmax function) 2. Computes cross entropy Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Create optimizer Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the y = AX + b y = AX + b equation where X is our input of size 784. We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. # Type of parameter object print ( model . parameters ()) # Length of parameters print ( len ( list ( model . parameters ()))) # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) < generator object Module . parameters at 0x7ff7c884f830 > 2 torch . Size ([ 10 , 784 ]) torch . Size ([ 10 ]) Quick Matrix Product Review Example 1: matrix product A: (100, 10) A: (100, 10) B: (10, 1) B: (10, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) Example 2: matrix product A: (50, 5) A: (50, 5) B: (5, 2) B: (5, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) Example 3: element-wise addition A: (10, 1) A: (10, 1) B: (10, 1) B: (10, 1) A + B = (10, 1) A + B = (10, 1) Step 7: Train Model \u00b6 7 step process for training models Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8513233661651611 . Accuracy : 70 Iteration : 1000. Loss : 1.5732524394989014 . Accuracy : 77 Iteration : 1500. Loss : 1.3840199708938599 . Accuracy : 79 Iteration : 2000. Loss : 1.1711134910583496 . Accuracy : 81 Iteration : 2500. Loss : 1.1094708442687988 . Accuracy : 82 Iteration : 3000. Loss : 1.002761721611023 . Accuracy : 82 Break Down Accuracy Calculation \u00b6 Printing outputs of our model As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model. This would print out the output of the model's predictions on your notebook. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs ) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor ([[ - 0.4181 , - 1.0784 , - 0.4840 , - 0.0985 , - 0.2394 , - 0.1801 , - 1.1639 , 2.9352 , - 0.1552 , 0.8852 ], [ 0.5117 , - 0.1099 , 1.5295 , 0.8863 , - 1.8813 , 0.5967 , 1.3632 , - 1.8977 , 0.4183 , - 1.4990 ], [ - 1.0126 , 2.4112 , 0.2373 , 0.0857 , - 0.7007 , - 0.2015 , - 0.3428 , - 0.2548 , 0.1659 , - 0.4703 ], [ 2.8072 , - 2.2973 , - 0.0984 , - 0.4313 , - 0.9619 , 0.8670 , 1.2201 , 0.3752 , - 0.2873 , - 0.3272 ], [ - 0.0343 , - 2.0043 , 0.5081 , - 0.6452 , 1.8647 , - 0.6924 , 0.1435 , 0.4330 , 0.2958 , 1.0339 ], [ - 1.5392 , 2.9070 , 0.2297 , 0.3139 , - 0.6863 , - 0.2734 , - 0.8377 , - 0.1238 , 0.3285 , - 0.3004 ], [ - 1.2037 , - 1.3739 , - 0.5947 , 0.3530 , 1.4205 , 0.0593 , - 0.7307 , 0.6642 , 0.3937 , 0.8004 ], [ - 1.4439 , - 0.3284 , - 0.7652 , - 0.0952 , 0.9323 , 0.3006 , 0.0238 , - 0.0810 , 0.0612 , 1.3295 ], [ 0.5409 , - 0.5266 , 0.9914 , - 1.2369 , 0.6583 , 0.0992 , 0.8525 , - 1.0562 , 0.2013 , 0.0462 ], [ - 0.6548 , - 0.7253 , - 0.9825 , - 1.1663 , 0.9076 , - 0.0694 , - 0.3708 , 1.8270 , 0.2457 , 1.5921 ], [ 3.2147 , - 1.7689 , 0.8531 , 1.2320 , - 0.8126 , 1.1251 , - 0.2776 , - 1.4244 , 0.5930 , - 1.6183 ], [ 0.7470 , - 0.5545 , 1.0251 , 0.0529 , 0.4384 , - 0.5934 , 0.7666 , - 1.0084 , 0.5313 , - 0.3465 ], [ - 0.7916 , - 1.7064 , - 0.7805 , - 1.1588 , 1.3284 , - 0.1708 , - 0.2092 , 0.9495 , 0.1033 , 2.0208 ], [ 3.0602 , - 2.3578 , - 0.2576 , - 0.2198 , - 0.2372 , 0.9765 , - 0.1514 , - 0.5380 , 0.7970 , 0.1374 ], [ - 1.2613 , 2.8594 , - 0.0874 , 0.1974 , - 1.2018 , - 0.0064 , - 0.0923 , - 0.2142 , 0.2575 , - 0.3218 ], [ 0.4348 , - 0.7216 , 0.0021 , 1.2864 , - 0.5062 , 0.7761 , - 0.3236 , - 0.5667 , 0.5431 , - 0.7781 ], [ - 0.2157 , - 2.0200 , 0.1829 , - 0.6882 , 1.3815 , - 0.7609 , - 0.0902 , 0.8647 , 0.3679 , 1.8843 ], [ 0.0950 , - 1.5009 , - 0.6347 , 0.3662 , - 0.4679 , - 0.0359 , - 0.7671 , 2.7155 , - 0.3991 , 0.5737 ], [ - 0.7005 , - 0.5366 , - 0.0434 , 1.1289 , - 0.5873 , 0.2555 , 0.8187 , - 0.6557 , 0.1241 , - 0.4297 ], [ - 1.0635 , - 1.5991 , - 0.4677 , - 0.1231 , 2.0445 , 0.1128 , - 0.1825 , 0.1075 , 0.0348 , 1.4317 ], [ - 1.0319 , - 0.1595 , - 1.3415 , 0.1095 , 0.5339 , 0.1973 , - 1.3272 , 1.5765 , 0.4784 , 1.4176 ], [ - 0.4928 , - 1.5653 , - 0.0672 , 0.3325 , 0.5359 , 0.5368 , 2.1542 , - 1.4276 , 0.3605 , 0.0587 ], [ - 0.4761 , 0.2958 , 0.6597 , - 0.2658 , 1.1279 , - 1.0676 , 1.2506 , - 0.2059 , - 0.1489 , 0.1051 ], [ - 0.0764 , - 0.9274 , - 0.6838 , 0.3464 , - 0.2656 , 1.4099 , 0.4486 , - 0.9527 , 0.5682 , 0.0156 ], [ - 0.6900 , - 0.9611 , 0.1395 , - 0.0079 , 1.5424 , - 0.3208 , - 0.2682 , 0.3586 , - 0.2771 , 1.0389 ], [ 4.3606 , - 2.8621 , 0.6310 , - 0.9657 , - 0.2486 , 1.2009 , 1.1873 , - 0.8255 , - 0.2103 , - 1.2172 ], [ - 0.1000 , - 1.4268 , - 0.4627 , - 0.1041 , 0.2959 , - 0.1392 , - 0.6855 , 1.8622 , - 0.2580 , 1.1347 ], [ - 0.3625 , - 2.1323 , - 0.2224 , - 0.8754 , 2.4684 , 0.0295 , 0.1161 , - 0.2660 , 0.3037 , 1.4570 ], [ 2.8688 , - 2.4517 , 0.1782 , 1.1149 , - 1.0898 , 1.1062 , - 0.0681 , - 0.5697 , 0.8888 , - 0.6965 ], [ - 1.0429 , 1.4446 , - 0.3349 , 0.1254 , - 0.5017 , 0.2286 , 0.2328 , - 0.3290 , 0.3949 , - 0.2586 ], [ - 0.8476 , - 0.0004 , - 1.1003 , 2.2806 , - 1.2226 , 0.9251 , - 0.3165 , 0.4957 , 0.0690 , 0.0232 ], [ - 0.9108 , 1.1355 , - 0.2715 , 0.2233 , - 0.3681 , 0.1442 , - 0.0001 , - 0.0174 , 0.1454 , 0.2286 ], [ - 1.0663 , - 0.8466 , - 0.7147 , 2.5685 , - 0.2090 , 1.2993 , - 0.3057 , - 0.8314 , 0.7046 , - 0.0176 ], [ 1.7013 , - 1.8051 , 0.7541 , - 1.5248 , 0.8972 , 0.1518 , 1.4876 , - 0.8454 , - 0.2022 , - 0.2829 ], [ - 0.8179 , - 0.1239 , 0.8630 , - 0.2137 , - 0.2275 , - 0.5411 , - 1.3448 , 1.7354 , 0.7751 , 0.6234 ], [ 0.6515 , - 1.0431 , 2.7165 , 0.1873 , - 1.0623 , 0.1286 , 0.3597 , - 0.2739 , 0.3871 , - 1.6699 ], [ - 0.2828 , - 1.4663 , 0.1182 , - 0.0896 , - 0.3640 , - 0.5129 , - 0.4905 , 2.2914 , - 0.2227 , 0.9463 ], [ - 1.2596 , 2.0468 , - 0.4405 , - 0.0411 , - 0.8073 , 0.0490 , - 0.0604 , - 0.1206 , 0.3504 , - 0.1059 ], [ 0.6089 , 0.5885 , 0.7898 , 1.1318 , - 1.9008 , 0.5875 , 0.4227 , - 1.1815 , 0.5652 , - 1.3590 ], [ - 1.4551 , 2.9537 , - 0.2805 , 0.2372 , - 1.4180 , 0.0297 , - 0.1515 , - 0.6111 , 0.6140 , - 0.3354 ], [ - 0.7182 , 1.6778 , 0.0553 , 0.0461 , - 0.5446 , - 0.0338 , - 0.0215 , - 0.0881 , 0.1506 , - 0.2107 ], [ - 0.8027 , - 0.7854 , - 0.1275 , - 0.3177 , - 0.1600 , - 0.1964 , - 0.6084 , 2.1285 , - 0.1815 , 1.1911 ], [ - 2.0656 , - 0.4959 , - 0.1154 , - 0.1363 , 2.2426 , - 0.7441 , - 0.8413 , 0.4675 , 0.3269 , 1.7279 ], [ - 0.3004 , 1.0166 , 1.1175 , - 0.0618 , - 0.0937 , - 0.4221 , 0.1943 , - 1.1020 , 0.3670 , - 0.4683 ], [ - 1.0720 , 0.2252 , 0.0175 , 1.3644 , - 0.7409 , 0.4655 , 0.5439 , 0.0380 , 0.1279 , - 0.2302 ], [ 0.2409 , - 1.2622 , - 0.6336 , 1.8240 , - 0.5951 , 1.3408 , 0.2130 , - 1.3789 , 0.8363 , - 0.2101 ], [ - 1.3849 , 0.3773 , - 0.0585 , 0.6896 , - 0.0998 , 0.2804 , 0.0696 , - 0.2529 , 0.3143 , 0.3409 ], [ - 0.9103 , - 0.1578 , 1.6673 , - 0.4817 , 0.4088 , - 0.5484 , 0.6103 , - 0.2287 , - 0.0665 , 0.0055 ], [ - 1.1692 , - 2.8531 , - 1.2499 , - 0.0257 , 2.8580 , 0.2616 , - 0.7122 , - 0.0551 , 0.8112 , 2.3233 ], [ - 0.2790 , - 1.9494 , 0.6096 , - 0.5653 , 2.2792 , - 1.0687 , 0.1634 , 0.3122 , 0.1053 , 1.0884 ], [ 0.1267 , - 1.2297 , - 0.1315 , 0.2428 , - 0.5436 , 0.4123 , 2.3060 , - 0.9278 , - 0.1528 , - 0.4224 ], [ - 0.0235 , - 0.9137 , - 0.1457 , 1.6858 , - 0.7552 , 0.7293 , 0.2510 , - 0.3955 , - 0.2187 , - 0.1505 ], [ 0.5643 , - 1.2783 , - 1.4149 , 0.0304 , 0.8375 , 1.5018 , 0.0338 , - 0.3875 , - 0.0117 , 0.5751 ], [ 0.2926 , - 0.7486 , - 0.3238 , 1.0384 , 0.0308 , 0.6792 , - 0.0170 , - 0.5797 , 0.2819 , - 0.3510 ], [ 0.1219 , - 0.5862 , 1.5817 , - 0.1297 , 0.4730 , - 0.9171 , 0.7886 , - 0.7022 , - 0.0501 , - 0.2812 ], [ 1.7587 , - 2.4511 , - 0.7369 , 0.4082 , - 0.6426 , 1.1784 , 0.6052 , - 0.7178 , 1.6161 , - 0.2220 ], [ - 0.1267 , - 2.6719 , 0.0505 , - 0.4972 , 2.9027 , - 0.1461 , 0.2807 , - 0.2921 , 0.2231 , 1.1327 ], [ - 0.9892 , 2.4401 , 0.1274 , 0.2838 , - 0.7535 , - 0.1684 , - 0.6493 , - 0.1908 , 0.2290 , - 0.2150 ], [ - 0.2071 , - 2.1351 , - 0.9191 , - 0.9309 , 1.7747 , - 0.3046 , 0.0183 , 1.0136 , - 0.1016 , 2.1288 ], [ - 0.0103 , 0.3280 , - 0.6974 , - 0.2504 , 0.3187 , 0.4390 , - 0.1879 , 0.3954 , 0.2332 , - 0.1971 ], [ - 0.2280 , - 1.6754 , - 0.7438 , 0.5078 , 0.2544 , - 0.1020 , - 0.2503 , 2.0799 , - 0.5033 , 0.5890 ], [ 0.3972 , - 0.9369 , 1.2696 , - 1.6713 , - 0.4159 , - 0.0221 , 0.6489 , - 0.4777 , 1.2497 , 0.3931 ], [ - 0.7566 , - 0.8230 , - 0.0785 , - 0.3083 , 0.7821 , 0.1880 , 0.1037 , - 0.0956 , 0.4219 , 1.0798 ], [ - 1.0328 , - 0.1700 , 1.3806 , 0.5445 , - 0.2624 , - 0.0780 , - 0.3595 , - 0.6253 , 0.4309 , 0.1813 ], [ - 1.0360 , - 0.4704 , 0.1948 , - 0.7066 , 0.6600 , - 0.4633 , - 0.3602 , 1.7494 , 0.1522 , 0.6086 ], [ - 1.2032 , - 0.7903 , - 0.5754 , 0.4722 , 0.6068 , 0.5752 , 0.2151 , - 0.2495 , 0.3420 , 0.9278 ], [ 0.2247 , - 0.1361 , 0.9374 , - 0.1543 , 0.4921 , - 0.6553 , 0.5885 , 0.2617 , - 0.2216 , - 0.3736 ], [ - 0.2867 , - 1.4486 , 0.6658 , - 0.8755 , 2.3195 , - 0.7627 , - 0.2132 , 0.2488 , 0.3484 , 1.0860 ], [ - 1.4031 , - 0.4518 , - 0.3181 , 2.8268 , - 0.5371 , 1.0154 , - 0.9247 , - 0.7385 , 1.1031 , 0.0422 ], [ 2.8604 , - 1.5413 , 0.6241 , - 0.8017 , - 1.4104 , 0.6314 , 0.4614 , - 0.0218 , - 0.3411 , - 0.2609 ], [ 0.2113 , - 1.2348 , - 0.8535 , - 0.1041 , - 0.2703 , - 0.1294 , - 0.7057 , 2.7552 , - 0.4429 , 0.4517 ], [ 4.5191 , - 2.7407 , 1.1091 , 0.3975 , - 0.9456 , 1.2277 , 0.3616 , - 1.6564 , 0.5063 , - 1.4274 ], [ 1.4615 , - 1.0765 , 1.8388 , 1.5006 , - 1.2351 , 0.2781 , 0.2830 , - 0.8491 , 0.2222 , - 1.7779 ], [ - 1.2160 , 0.8502 , 0.2413 , - 0.0798 , - 0.7880 , - 0.4286 , - 0.8060 , 0.7194 , 1.2663 , 0.6412 ], [ - 1.3318 , 2.3388 , - 0.4003 , - 0.1094 , - 1.0285 , 0.1021 , - 0.0388 , - 0.0497 , 0.5137 , - 0.2507 ], [ - 1.7853 , 0.5884 , - 0.6108 , - 0.5557 , 0.8696 , - 0.6226 , - 0.7983 , 1.7169 , - 0.0145 , 0.8231 ], [ - 0.1739 , 0.1562 , - 0.2933 , 2.3195 , - 0.9480 , 1.2019 , - 0.4834 , - 1.0567 , 0.5685 , - 0.6841 ], [ - 0.7920 , - 0.3339 , 0.7452 , - 0.6529 , - 0.3307 , - 0.6092 , - 0.0950 , 1.7311 , - 0.3481 , 0.3801 ], [ - 1.7810 , 1.0676 , - 0.7611 , 0.3658 , - 0.0431 , - 0.1012 , - 0.6048 , 0.3089 , 0.9998 , 0.7164 ], [ - 0.5856 , - 0.5261 , - 0.4859 , - 1.0551 , - 0.1838 , - 0.2144 , - 1.2599 , 3.3891 , 0.4691 , 0.7566 ], [ - 0.4984 , - 1.7770 , - 1.1998 , - 0.1075 , 1.0882 , 0.4539 , - 0.5651 , 1.4381 , - 0.5678 , 1.7479 ], [ 0.2938 , - 1.8536 , 0.4259 , - 0.5429 , 0.0066 , 0.4120 , 2.3793 , - 0.3666 , - 0.2604 , 0.0382 ], [ - 0.4080 , - 0.9851 , 4.0264 , 0.1099 , - 0.1766 , - 1.1557 , 0.6419 , - 0.8147 , 0.7535 , - 1.1452 ], [ - 0.4636 , - 1.7323 , - 0.6433 , - 0.0274 , 0.7227 , - 0.1799 , - 0.9336 , 2.1881 , - 0.2073 , 1.6522 ], [ - 0.9617 , - 0.0348 , - 0.3980 , - 0.4738 , 0.7790 , 0.4671 , - 0.6115 , - 0.7067 , 1.3036 , 0.4923 ], [ - 1.0151 , - 2.5385 , - 0.6072 , 0.2902 , 3.1570 , 0.1062 , - 0.2169 , - 0.4491 , 0.6326 , 1.6829 ], [ - 1.8852 , 0.6066 , - 0.2840 , - 0.4475 , - 0.1147 , - 0.7858 , - 1.1805 , 3.0723 , 0.3960 , 0.9720 ], [ 0.0344 , - 1.4878 , - 0.9675 , 1.9649 , - 0.3146 , 1.2183 , 0.6730 , - 0.3650 , 0.0646 , - 0.0898 ], [ - 0.2118 , - 2.0350 , 0.9917 , - 0.8993 , 1.2334 , - 0.6723 , 2.5847 , - 0.0454 , - 0.4149 , 0.3927 ], [ - 1.7365 , 3.0447 , 0.5115 , 0.0786 , - 0.7544 , - 0.2158 , - 0.4876 , - 0.2891 , 0.5089 , - 0.6719 ], [ 0.3652 , - 0.5457 , - 0.1167 , 2.9056 , - 1.1622 , 0.8192 , - 1.3245 , - 0.6414 , 0.8097 , - 0.4958 ], [ - 0.8755 , - 0.6983 , 0.2208 , - 0.6463 , 0.5276 , 0.1145 , 2.7229 , - 1.0316 , 0.1905 , 0.2090 ], [ - 0.9702 , 0.1265 , - 0.0007 , - 0.5106 , 0.4970 , - 0.0804 , 0.0017 , 0.0607 , 0.6164 , 0.4490 ], [ - 0.8271 , - 0.6822 , - 0.7434 , 2.6457 , - 1.6143 , 1.1486 , - 1.0705 , 0.5611 , 0.6422 , 0.1250 ], [ - 1.9979 , 1.8175 , - 0.1658 , - 0.0343 , - 0.6292 , 0.1774 , 0.3150 , - 0.4633 , 0.9266 , 0.0252 ], [ - 0.9039 , - 0.6030 , - 0.2173 , - 1.1768 , 2.3198 , - 0.5072 , 0.3418 , - 0.1551 , 0.1282 , 1.4250 ], [ - 0.9891 , 0.5212 , - 0.4518 , 0.3267 , - 0.0759 , 0.3826 , - 0.0341 , 0.0382 , 0.2451 , 0.3658 ], [ - 2.1217 , 1.5102 , - 0.7828 , 0.3554 , - 0.4192 , - 0.0772 , 0.0578 , 0.8070 , 0.1701 , 0.5880 ], [ 1.0665 , - 1.3826 , 0.6243 , - 0.8096 , - 0.4227 , 0.5925 , 1.8112 , - 0.9946 , 0.2010 , - 0.7731 ], [ - 1.1263 , - 1.7484 , 0.0041 , - 0.5439 , 1.7242 , - 0.9475 , - 0.3835 , 0.8452 , 0.3077 , 2.2689 ]]) Printing output size This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs . size ()) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS torch . Size ([ 100 , 10 ]) Printing one output This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7. number 0: -0.4181 number 1: -1.0784 ... number 7: 2.9352 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs [ 0 , :]) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639, 2.9352, -0.1552, 0.8852]) Printing prediction output Because our output is of size 100 (our batch size), our prediction size would also of the size 100. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted . size ()) PREDICTION torch . Size ([ 100 ]) Print prediction value We are printing our prediction which as verified above, should be digit 7. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) PREDICTION tensor ( 7 ) Print prediction, label and label size We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7! iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 0' ) print ( labels [ 0 ]) PREDICTION tensor ( 7 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 0 tensor ( 7 ) Print second prediction and ground truth Again, the prediction is correct. Naturally, as our model is quite competent in this simple task. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 1 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 1' ) print ( labels [ 1 ]) PREDICTION tensor ( 2 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 1 tensor ( 2 ) Print accuracy Now we know what each object represents, we can understand how we arrived at our accuracy numbers. One last thing to note is that correct.item() has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this. correct = 0 total = 0 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * ( correct . item () / total ) print ( accuracy ) 82.94 Explanation of Python's .sum() function Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return True or in our case, those predictions that match actual labels (correct predictions). # Explaining .sum() python built-in function # correct += (predicted == labels).sum() import numpy as np a = np . ones (( 10 )) print ( a ) b = np . ones (( 10 )) print ( b ) print ( a == b ) print (( a == b ) . sum ()) # matrix a [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # matrix b [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # boolean array [ True True True True True True True True True True ] # number of elementswhere a matches b 10 Saving Model \u00b6 Saving PyTorch model This is how you save your model. Feel free to just change save_model = True to save your model save_model = False if save_model is True : # Saves only parameters torch . save ( model . state_dict (), 'awesome_model.pkl' ) Building a Logistic Regression Model with PyTorch (GPU) \u00b6 CPU version The usual 7-step process, getting repetitive by now which we like. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # 100 x 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value # 100 x 1 _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.876196026802063 . Accuracy : 64.44 Iteration : 1000. Loss : 1.5153584480285645 . Accuracy : 75.68 Iteration : 1500. Loss : 1.3521136045455933 . Accuracy : 78.98 Iteration : 2000. Loss : 1.2136967182159424 . Accuracy : 80.95 Iteration : 2500. Loss : 1.0934826135635376 . Accuracy : 81.97 Iteration : 3000. Loss : 1.024120569229126 . Accuracy : 82.49 GPU version 2 things must be on GPU - model - tensors Remember step 4 and 7 will be affected and this will be the same for all model building moving forward. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8571407794952393 . Accuracy : 68.99 Iteration : 1000. Loss : 1.5415704250335693 . Accuracy : 75.86 Iteration : 1500. Loss : 1.2755383253097534 . Accuracy : 78.92 Iteration : 2000. Loss : 1.2468739748001099 . Accuracy : 80.72 Iteration : 2500. Loss : 1.0708973407745361 . Accuracy : 81.73 Iteration : 3000. Loss : 1.0359245538711548 . Accuracy : 82.74 Summary \u00b6 We've learnt to... Success Logistic regression basics Problems of linear regression In-depth Logistic Regression Get logits Get softmax Get cross-entropy loss Aim : reduce cross-entropy loss Built a logistic regression model in CPU and GPU Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Important things to be on GPU model tensors with gradients Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Logistic Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-with-pytorch","text":"","title":"Logistic Regression with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#about-logistic-regression","text":"","title":"About Logistic Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-basics","text":"","title":"Logistic Regression Basics"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#classification-algorithm","text":"Example: Spam vs No Spam Input: Bunch of words Output: Probability spam or not","title":"Classification algorithm"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#basic-comparison","text":"Linear regression Output: numeric value given inputs Logistic regression : Output: probability [0, 1] given input belonging to a class","title":"Basic Comparison"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#inputoutput-comparison","text":"Linear regression: Multiplication Input: [1] Output: 2 Input: [2] Output: 4 Trying to model the relationship y = 2x Logistic regression: Spam Input: \"Sign up to get 1 million dollars by tonight\" Output: p = 0.8 Input: \"This is a receipt for your recent purchase with Amazon\" Output: p = 0.3 p: probability it is spam","title":"Input/Output Comparison"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#problems-of-linear-regression","text":"Example Fever Input : temperature Output : fever or no fever Remember Linear regression : minimize error between points and line Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1) If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import numpy as np import matplotlib.pyplot as plt % matplotlib inline x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 100 ,] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Linear Regression Problem 2: Fever points are not predicted with the presence of outliers Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import numpy as np import matplotlib.pyplot as plt x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 300 ] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ()","title":"Problems of Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-in-depth","text":"","title":"Logistic Regression In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#predicting-probability","text":"Linear regression doesn't work Instead of predicting direct values: predict probability","title":"Predicting Probability"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-function-g","text":"\"Two-class logistic regression\" \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 2-class prediction y_0 y_0 and y_1 y_1 Where the labels are y_0 = 0 y_0 = 0 and y_1 = 1 y_1 = 1 Also, it's bolded because it's a vector, not a matrix. g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) = \\frac {1} {1 + e^{-y_1}} g(y_1) g(y_1) = Estimated probability that y = 1 y = 1 g(y_0) = 1 - g(y_1) g(y_0) = 1 - g(y_1) g(y_0) g(y_0) = Estimated probability that y = 0 y = 0 For our illustration above, we have 4 classes, so we have to use softmax function explained below","title":"Logistic Function g()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#softmax-function-g","text":"\"Multi-class logistic regression\" Generalization of logistic function, where you can derive back to the logistic function if you've a 2 class classification problem Here, we will use a 4 class example (K = 4) as shown above to be very clear in how it relates back to that simple examaple. \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} \\boldsymbol{y} = A\\boldsymbol{x} + \\boldsymbol{b} Where \\boldsymbol{y} \\boldsymbol{y} is a vector comprising the 4-class prediction y_0, y_1, y_2, y_3 y_0, y_1, y_2, y_3 Where the 4 labels (K = 4) are y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 y_0 = 0, y_1 = 1, y_2 = 2, y_3 = 3 g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} g(y_i) = \\frac {e^{y_i} } {\\sum^K_i e^{y_i}} where K = 4 because we have 4 classes To put numbers to this equation in relation to the illustration above where we've y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 y_0 = 1.3, y_1 = 1.2, y = 4.5, y = 4.8 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_0) = \\frac {e^{1.3}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.017 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_1) = \\frac {e^{1.2}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.015 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_2) = \\frac {e^{4.5}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.412 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_3) = \\frac {e^{4.8}}{e^{1.3} + e^{1.2} + e^{4.5} + e^{4.8}} = 0.556 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 g(y_0) + g(y_1) + g(y_2) + g(y_3) = 1.0 All softmax outputs have to sum to one as they represent a probability distribution over K classes. Take note how these numbers are not exactly as in the illustration in the softmax box but the concept is important (intentionally made so). y_0 y_0 and y_1 y_1 are approximately similar in values and they return similar probabilities. Similarly, y_2 y_2 and y_3 y_3 are approximately similar in values and they return similar probabilities. Softmax versus Soft(arg)max Do you know many researchers and anyone in deep learning in general use the term softmax when it should be soft(arg)max. This is because soft(arg)max returns the probability distribution over K classes, a vector. However, softmax only returns the max! This means you will be getting a scalar value versus a probability distribution. According to my friend, Alfredo Canziani (postdoc in NYU under Yann Lecun), it was actually a mistake made in the original paper previously but it was too late because the term softmax was adopted. Full credits to him for this tip.","title":"Softmax Function g()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d-for-2-class","text":"Take note that here, S S is our softmax outputs and L L are our labels D(S, L) = -(L log S + (1-L)log(1-S)) D(S, L) = -(L log S + (1-L)log(1-S)) If L = 0 (label) D(S, 0) = - log(1-S) D(S, 0) = - log(1-S) - log(1-S) - log(1-S) : less positive if S \\longrightarrow 0 S \\longrightarrow 0 - log(1-S) - log(1-S) : more positive if S \\longrightarrow 1 S \\longrightarrow 1 (BIGGER LOSS) If L = 1 (label) D(S, 1) = - log S D(S, 1) = - log S -log(S) -log(S) : less positive if S \\longrightarrow 1 S \\longrightarrow 1 -log(S) -log(S) : more positive if S \\longrightarrow 0 S \\longrightarrow 0 (BIGGER LOSS) Numerical example of bigger or small loss You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import math print ( - math . log ( 1 - 0.00001 )) You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1 - 0.99999 )) You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( - math . log ( 0.99999 )) You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( - math . log ( 0.00001 )) 1.0000050000287824e-05 11.51292546497478 1.0000050000287824e-05 11.512925464970229","title":"Cross Entropy Function D() for 2 Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d-for-more-than-2-class","text":"For the case where we have more than 2 class, we need a more generalized function D(S, L) = - \\sum^K_1 L_i log(S_i) D(S, L) = - \\sum^K_1 L_i log(S_i) K K : number of classes L_i L_i : label of i-th class, 1 if that's the class else 0 S_i S_i : output of softmax for i-th class","title":"Cross Entropy Function D() for More Than 2 Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-loss-over-n-samples","text":"Goal: Minimizing Cross Entropy Loss, L Loss = \\frac {1}{N} \\sum_j^N D_j Loss = \\frac {1}{N} \\sum_j^N D_j D_j D_j : j-th sample of cross entropy function D(S, L) D(S, L) N N : number of samples Loss Loss : average cross entropy loss over N samples","title":"Cross Entropy Loss over N samples"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch","text":"","title":"Building a Logistic Regression Model with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1a-loading-mnist-train-dataset","text":"Images from 1 to 9 Inspect length of training dataset You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) len ( train_dataset ) 60000 Inspecting a single image So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ] ( tensor ([[[ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0118 , 0.0706 , 0.0706 , 0.0706 , 0.4941 , 0.5333 , 0.6863 , 0.1020 , 0.6510 , 1.0000 , 0.9686 , 0.4980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1176 , 0.1412 , 0.3686 , 0.6039 , 0.6667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.8824 , 0.6745 , 0.9922 , 0.9490 , 0.7647 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1922 , 0.9333 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9843 , 0.3647 , 0.3216 , 0.3216 , 0.2196 , 0.1529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.7137 , 0.9686 , 0.9451 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3137 , 0.6118 , 0.4196 , 0.9922 , 0.9922 , 0.8039 , 0.0431 , 0.0000 , 0.1686 , 0.6039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0549 , 0.0039 , 0.6039 , 0.9922 , 0.3529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5451 , 0.9922 , 0.7451 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0431 , 0.7451 , 0.9922 , 0.2745 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1373 , 0.9451 , 0.8824 , 0.6275 , 0.4235 , 0.0039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3176 , 0.9412 , 0.9922 , 0.9922 , 0.4667 , 0.0980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1765 , 0.7294 , 0.9922 , 0.9922 , 0.5882 , 0.1059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0627 , 0.3647 , 0.9882 , 0.9922 , 0.7333 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.9765 , 0.9922 , 0.9765 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1804 , 0.5098 , 0.7176 , 0.9922 , 0.9922 , 0.8118 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1529 , 0.5804 , 0.8980 , 0.9922 , 0.9922 , 0.9922 , 0.9804 , 0.7137 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0941 , 0.4471 , 0.8667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7882 , 0.3059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0902 , 0.2588 , 0.8353 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.3176 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.6706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7647 , 0.3137 , 0.0353 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.2157 , 0.6745 , 0.8863 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9569 , 0.5216 , 0.0431 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5333 , 0.9922 , 0.9922 , 0.9922 , 0.8314 , 0.5294 , 0.5176 , 0.0627 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ]]]), tensor ( 5 )) Inspecting a single data point in the training dataset When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label. type ( train_dataset [ 0 ]) tuple Inspecting training dataset first element of tuple This means to access the image, you need to access the first element in the tuple. # Input Matrix train_dataset [ 0 ][ 0 ] . size () # A 28x28 sized image of a digit torch . Size ([ 1 , 28 , 28 ]) Inspecting training dataset second element of tuple The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 )","title":"Step 1a: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#displaying-mnist","text":"Verifying shape of MNIST image As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel. import matplotlib.pyplot as plt % matplotlib inline import numpy as np train_dataset [ 0 ][ 0 ] . numpy () . shape ( 1 , 28 , 28 ) Plot image of MNIST image show_img = train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label As you would expect, the label is 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Plot second image of MNIST image show_img = train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label We should see 0 here as the label. # Label train_dataset [ 1 ][ 1 ] tensor ( 0 )","title":"Displaying MNIST"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1b-loading-mnist-test-dataset","text":"Show our algorithm works beyond the data we have trained on. Out-of-sample Load test dataset Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) len ( test_dataset ) 10000 Test dataset elements Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ]) tuple Test dataset first element in tuple This contains the image matrix, similar to the training set. # Image matrix test_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Plot image sample from test dataset show_img = test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Test dataset second element in tuple # Label test_dataset [ 0 ][ 1 ] tensor ( 7 )","title":"Step 1b: Loading MNIST Test Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-2-make-dataset-iterable","text":"Aim: make the dataset iterable totaldata : 60000 minibatch : 100 Number of examples in 1 iteration iterations : 3000 1 iteration: one mini-batch forward & backward pass epochs 1 epoch: running through the whole dataset once epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 Recap training dataset Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset ) 60000 Defining epochs When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. batch_size = 100 We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters = 3000 One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) num_epochs 5 Create Iterable Object: Training Dataset train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) Check Iterability import collections isinstance ( train_loader , collections . Iterable ) True Create Iterable Object: Testing Dataset # Iterable object test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Check iterability of testing dataset isinstance ( test_loader , collections . Iterable ) True Iterate through dataset This is just a simplified example of what we're doing above where we're creating an iterable object lst to loop through so we can access all the images img_1 and img_2 . Above, the equivalent of lst is train_loader and test_loader . img_1 = np . ones (( 28 , 28 )) img_2 = np . ones (( 28 , 28 )) lst = [ img_1 , img_2 ] # Need to iterate # Think of numbers as the images for i in lst : print ( i . shape ) ( 28 , 28 ) ( 28 , 28 )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-3-building-model","text":"Create model class # Same as linear regression! class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out","title":"Step 3: Building Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-4-instantiate-model-class","text":"Input dimension: Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Check size of dataset This should be 28x28. # Size of images train_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Instantiate model class based on input and out dimensions As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-5-instantiate-loss-class","text":"Logistic Regression : Cross Entropy Loss Linear Regression: MSE Create Cross Entry Loss Class Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters. criterion = nn . CrossEntropyLoss () What happens in nn.CrossEntropyLoss()? It does 2 things at the same time. 1. Computes softmax (logistic/softmax function) 2. Computes cross entropy","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Create optimizer Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the y = AX + b y = AX + b equation where X is our input of size 784. We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. # Type of parameter object print ( model . parameters ()) # Length of parameters print ( len ( list ( model . parameters ()))) # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) < generator object Module . parameters at 0x7ff7c884f830 > 2 torch . Size ([ 10 , 784 ]) torch . Size ([ 10 ]) Quick Matrix Product Review Example 1: matrix product A: (100, 10) A: (100, 10) B: (10, 1) B: (10, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) Example 2: matrix product A: (50, 5) A: (50, 5) B: (5, 2) B: (5, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) Example 3: element-wise addition A: (10, 1) A: (10, 1) B: (10, 1) B: (10, 1) A + B = (10, 1) A + B = (10, 1)","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-7-train-model","text":"7 step process for training models Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8513233661651611 . Accuracy : 70 Iteration : 1000. Loss : 1.5732524394989014 . Accuracy : 77 Iteration : 1500. Loss : 1.3840199708938599 . Accuracy : 79 Iteration : 2000. Loss : 1.1711134910583496 . Accuracy : 81 Iteration : 2500. Loss : 1.1094708442687988 . Accuracy : 82 Iteration : 3000. Loss : 1.002761721611023 . Accuracy : 82","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#break-down-accuracy-calculation","text":"Printing outputs of our model As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model. This would print out the output of the model's predictions on your notebook. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs ) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor ([[ - 0.4181 , - 1.0784 , - 0.4840 , - 0.0985 , - 0.2394 , - 0.1801 , - 1.1639 , 2.9352 , - 0.1552 , 0.8852 ], [ 0.5117 , - 0.1099 , 1.5295 , 0.8863 , - 1.8813 , 0.5967 , 1.3632 , - 1.8977 , 0.4183 , - 1.4990 ], [ - 1.0126 , 2.4112 , 0.2373 , 0.0857 , - 0.7007 , - 0.2015 , - 0.3428 , - 0.2548 , 0.1659 , - 0.4703 ], [ 2.8072 , - 2.2973 , - 0.0984 , - 0.4313 , - 0.9619 , 0.8670 , 1.2201 , 0.3752 , - 0.2873 , - 0.3272 ], [ - 0.0343 , - 2.0043 , 0.5081 , - 0.6452 , 1.8647 , - 0.6924 , 0.1435 , 0.4330 , 0.2958 , 1.0339 ], [ - 1.5392 , 2.9070 , 0.2297 , 0.3139 , - 0.6863 , - 0.2734 , - 0.8377 , - 0.1238 , 0.3285 , - 0.3004 ], [ - 1.2037 , - 1.3739 , - 0.5947 , 0.3530 , 1.4205 , 0.0593 , - 0.7307 , 0.6642 , 0.3937 , 0.8004 ], [ - 1.4439 , - 0.3284 , - 0.7652 , - 0.0952 , 0.9323 , 0.3006 , 0.0238 , - 0.0810 , 0.0612 , 1.3295 ], [ 0.5409 , - 0.5266 , 0.9914 , - 1.2369 , 0.6583 , 0.0992 , 0.8525 , - 1.0562 , 0.2013 , 0.0462 ], [ - 0.6548 , - 0.7253 , - 0.9825 , - 1.1663 , 0.9076 , - 0.0694 , - 0.3708 , 1.8270 , 0.2457 , 1.5921 ], [ 3.2147 , - 1.7689 , 0.8531 , 1.2320 , - 0.8126 , 1.1251 , - 0.2776 , - 1.4244 , 0.5930 , - 1.6183 ], [ 0.7470 , - 0.5545 , 1.0251 , 0.0529 , 0.4384 , - 0.5934 , 0.7666 , - 1.0084 , 0.5313 , - 0.3465 ], [ - 0.7916 , - 1.7064 , - 0.7805 , - 1.1588 , 1.3284 , - 0.1708 , - 0.2092 , 0.9495 , 0.1033 , 2.0208 ], [ 3.0602 , - 2.3578 , - 0.2576 , - 0.2198 , - 0.2372 , 0.9765 , - 0.1514 , - 0.5380 , 0.7970 , 0.1374 ], [ - 1.2613 , 2.8594 , - 0.0874 , 0.1974 , - 1.2018 , - 0.0064 , - 0.0923 , - 0.2142 , 0.2575 , - 0.3218 ], [ 0.4348 , - 0.7216 , 0.0021 , 1.2864 , - 0.5062 , 0.7761 , - 0.3236 , - 0.5667 , 0.5431 , - 0.7781 ], [ - 0.2157 , - 2.0200 , 0.1829 , - 0.6882 , 1.3815 , - 0.7609 , - 0.0902 , 0.8647 , 0.3679 , 1.8843 ], [ 0.0950 , - 1.5009 , - 0.6347 , 0.3662 , - 0.4679 , - 0.0359 , - 0.7671 , 2.7155 , - 0.3991 , 0.5737 ], [ - 0.7005 , - 0.5366 , - 0.0434 , 1.1289 , - 0.5873 , 0.2555 , 0.8187 , - 0.6557 , 0.1241 , - 0.4297 ], [ - 1.0635 , - 1.5991 , - 0.4677 , - 0.1231 , 2.0445 , 0.1128 , - 0.1825 , 0.1075 , 0.0348 , 1.4317 ], [ - 1.0319 , - 0.1595 , - 1.3415 , 0.1095 , 0.5339 , 0.1973 , - 1.3272 , 1.5765 , 0.4784 , 1.4176 ], [ - 0.4928 , - 1.5653 , - 0.0672 , 0.3325 , 0.5359 , 0.5368 , 2.1542 , - 1.4276 , 0.3605 , 0.0587 ], [ - 0.4761 , 0.2958 , 0.6597 , - 0.2658 , 1.1279 , - 1.0676 , 1.2506 , - 0.2059 , - 0.1489 , 0.1051 ], [ - 0.0764 , - 0.9274 , - 0.6838 , 0.3464 , - 0.2656 , 1.4099 , 0.4486 , - 0.9527 , 0.5682 , 0.0156 ], [ - 0.6900 , - 0.9611 , 0.1395 , - 0.0079 , 1.5424 , - 0.3208 , - 0.2682 , 0.3586 , - 0.2771 , 1.0389 ], [ 4.3606 , - 2.8621 , 0.6310 , - 0.9657 , - 0.2486 , 1.2009 , 1.1873 , - 0.8255 , - 0.2103 , - 1.2172 ], [ - 0.1000 , - 1.4268 , - 0.4627 , - 0.1041 , 0.2959 , - 0.1392 , - 0.6855 , 1.8622 , - 0.2580 , 1.1347 ], [ - 0.3625 , - 2.1323 , - 0.2224 , - 0.8754 , 2.4684 , 0.0295 , 0.1161 , - 0.2660 , 0.3037 , 1.4570 ], [ 2.8688 , - 2.4517 , 0.1782 , 1.1149 , - 1.0898 , 1.1062 , - 0.0681 , - 0.5697 , 0.8888 , - 0.6965 ], [ - 1.0429 , 1.4446 , - 0.3349 , 0.1254 , - 0.5017 , 0.2286 , 0.2328 , - 0.3290 , 0.3949 , - 0.2586 ], [ - 0.8476 , - 0.0004 , - 1.1003 , 2.2806 , - 1.2226 , 0.9251 , - 0.3165 , 0.4957 , 0.0690 , 0.0232 ], [ - 0.9108 , 1.1355 , - 0.2715 , 0.2233 , - 0.3681 , 0.1442 , - 0.0001 , - 0.0174 , 0.1454 , 0.2286 ], [ - 1.0663 , - 0.8466 , - 0.7147 , 2.5685 , - 0.2090 , 1.2993 , - 0.3057 , - 0.8314 , 0.7046 , - 0.0176 ], [ 1.7013 , - 1.8051 , 0.7541 , - 1.5248 , 0.8972 , 0.1518 , 1.4876 , - 0.8454 , - 0.2022 , - 0.2829 ], [ - 0.8179 , - 0.1239 , 0.8630 , - 0.2137 , - 0.2275 , - 0.5411 , - 1.3448 , 1.7354 , 0.7751 , 0.6234 ], [ 0.6515 , - 1.0431 , 2.7165 , 0.1873 , - 1.0623 , 0.1286 , 0.3597 , - 0.2739 , 0.3871 , - 1.6699 ], [ - 0.2828 , - 1.4663 , 0.1182 , - 0.0896 , - 0.3640 , - 0.5129 , - 0.4905 , 2.2914 , - 0.2227 , 0.9463 ], [ - 1.2596 , 2.0468 , - 0.4405 , - 0.0411 , - 0.8073 , 0.0490 , - 0.0604 , - 0.1206 , 0.3504 , - 0.1059 ], [ 0.6089 , 0.5885 , 0.7898 , 1.1318 , - 1.9008 , 0.5875 , 0.4227 , - 1.1815 , 0.5652 , - 1.3590 ], [ - 1.4551 , 2.9537 , - 0.2805 , 0.2372 , - 1.4180 , 0.0297 , - 0.1515 , - 0.6111 , 0.6140 , - 0.3354 ], [ - 0.7182 , 1.6778 , 0.0553 , 0.0461 , - 0.5446 , - 0.0338 , - 0.0215 , - 0.0881 , 0.1506 , - 0.2107 ], [ - 0.8027 , - 0.7854 , - 0.1275 , - 0.3177 , - 0.1600 , - 0.1964 , - 0.6084 , 2.1285 , - 0.1815 , 1.1911 ], [ - 2.0656 , - 0.4959 , - 0.1154 , - 0.1363 , 2.2426 , - 0.7441 , - 0.8413 , 0.4675 , 0.3269 , 1.7279 ], [ - 0.3004 , 1.0166 , 1.1175 , - 0.0618 , - 0.0937 , - 0.4221 , 0.1943 , - 1.1020 , 0.3670 , - 0.4683 ], [ - 1.0720 , 0.2252 , 0.0175 , 1.3644 , - 0.7409 , 0.4655 , 0.5439 , 0.0380 , 0.1279 , - 0.2302 ], [ 0.2409 , - 1.2622 , - 0.6336 , 1.8240 , - 0.5951 , 1.3408 , 0.2130 , - 1.3789 , 0.8363 , - 0.2101 ], [ - 1.3849 , 0.3773 , - 0.0585 , 0.6896 , - 0.0998 , 0.2804 , 0.0696 , - 0.2529 , 0.3143 , 0.3409 ], [ - 0.9103 , - 0.1578 , 1.6673 , - 0.4817 , 0.4088 , - 0.5484 , 0.6103 , - 0.2287 , - 0.0665 , 0.0055 ], [ - 1.1692 , - 2.8531 , - 1.2499 , - 0.0257 , 2.8580 , 0.2616 , - 0.7122 , - 0.0551 , 0.8112 , 2.3233 ], [ - 0.2790 , - 1.9494 , 0.6096 , - 0.5653 , 2.2792 , - 1.0687 , 0.1634 , 0.3122 , 0.1053 , 1.0884 ], [ 0.1267 , - 1.2297 , - 0.1315 , 0.2428 , - 0.5436 , 0.4123 , 2.3060 , - 0.9278 , - 0.1528 , - 0.4224 ], [ - 0.0235 , - 0.9137 , - 0.1457 , 1.6858 , - 0.7552 , 0.7293 , 0.2510 , - 0.3955 , - 0.2187 , - 0.1505 ], [ 0.5643 , - 1.2783 , - 1.4149 , 0.0304 , 0.8375 , 1.5018 , 0.0338 , - 0.3875 , - 0.0117 , 0.5751 ], [ 0.2926 , - 0.7486 , - 0.3238 , 1.0384 , 0.0308 , 0.6792 , - 0.0170 , - 0.5797 , 0.2819 , - 0.3510 ], [ 0.1219 , - 0.5862 , 1.5817 , - 0.1297 , 0.4730 , - 0.9171 , 0.7886 , - 0.7022 , - 0.0501 , - 0.2812 ], [ 1.7587 , - 2.4511 , - 0.7369 , 0.4082 , - 0.6426 , 1.1784 , 0.6052 , - 0.7178 , 1.6161 , - 0.2220 ], [ - 0.1267 , - 2.6719 , 0.0505 , - 0.4972 , 2.9027 , - 0.1461 , 0.2807 , - 0.2921 , 0.2231 , 1.1327 ], [ - 0.9892 , 2.4401 , 0.1274 , 0.2838 , - 0.7535 , - 0.1684 , - 0.6493 , - 0.1908 , 0.2290 , - 0.2150 ], [ - 0.2071 , - 2.1351 , - 0.9191 , - 0.9309 , 1.7747 , - 0.3046 , 0.0183 , 1.0136 , - 0.1016 , 2.1288 ], [ - 0.0103 , 0.3280 , - 0.6974 , - 0.2504 , 0.3187 , 0.4390 , - 0.1879 , 0.3954 , 0.2332 , - 0.1971 ], [ - 0.2280 , - 1.6754 , - 0.7438 , 0.5078 , 0.2544 , - 0.1020 , - 0.2503 , 2.0799 , - 0.5033 , 0.5890 ], [ 0.3972 , - 0.9369 , 1.2696 , - 1.6713 , - 0.4159 , - 0.0221 , 0.6489 , - 0.4777 , 1.2497 , 0.3931 ], [ - 0.7566 , - 0.8230 , - 0.0785 , - 0.3083 , 0.7821 , 0.1880 , 0.1037 , - 0.0956 , 0.4219 , 1.0798 ], [ - 1.0328 , - 0.1700 , 1.3806 , 0.5445 , - 0.2624 , - 0.0780 , - 0.3595 , - 0.6253 , 0.4309 , 0.1813 ], [ - 1.0360 , - 0.4704 , 0.1948 , - 0.7066 , 0.6600 , - 0.4633 , - 0.3602 , 1.7494 , 0.1522 , 0.6086 ], [ - 1.2032 , - 0.7903 , - 0.5754 , 0.4722 , 0.6068 , 0.5752 , 0.2151 , - 0.2495 , 0.3420 , 0.9278 ], [ 0.2247 , - 0.1361 , 0.9374 , - 0.1543 , 0.4921 , - 0.6553 , 0.5885 , 0.2617 , - 0.2216 , - 0.3736 ], [ - 0.2867 , - 1.4486 , 0.6658 , - 0.8755 , 2.3195 , - 0.7627 , - 0.2132 , 0.2488 , 0.3484 , 1.0860 ], [ - 1.4031 , - 0.4518 , - 0.3181 , 2.8268 , - 0.5371 , 1.0154 , - 0.9247 , - 0.7385 , 1.1031 , 0.0422 ], [ 2.8604 , - 1.5413 , 0.6241 , - 0.8017 , - 1.4104 , 0.6314 , 0.4614 , - 0.0218 , - 0.3411 , - 0.2609 ], [ 0.2113 , - 1.2348 , - 0.8535 , - 0.1041 , - 0.2703 , - 0.1294 , - 0.7057 , 2.7552 , - 0.4429 , 0.4517 ], [ 4.5191 , - 2.7407 , 1.1091 , 0.3975 , - 0.9456 , 1.2277 , 0.3616 , - 1.6564 , 0.5063 , - 1.4274 ], [ 1.4615 , - 1.0765 , 1.8388 , 1.5006 , - 1.2351 , 0.2781 , 0.2830 , - 0.8491 , 0.2222 , - 1.7779 ], [ - 1.2160 , 0.8502 , 0.2413 , - 0.0798 , - 0.7880 , - 0.4286 , - 0.8060 , 0.7194 , 1.2663 , 0.6412 ], [ - 1.3318 , 2.3388 , - 0.4003 , - 0.1094 , - 1.0285 , 0.1021 , - 0.0388 , - 0.0497 , 0.5137 , - 0.2507 ], [ - 1.7853 , 0.5884 , - 0.6108 , - 0.5557 , 0.8696 , - 0.6226 , - 0.7983 , 1.7169 , - 0.0145 , 0.8231 ], [ - 0.1739 , 0.1562 , - 0.2933 , 2.3195 , - 0.9480 , 1.2019 , - 0.4834 , - 1.0567 , 0.5685 , - 0.6841 ], [ - 0.7920 , - 0.3339 , 0.7452 , - 0.6529 , - 0.3307 , - 0.6092 , - 0.0950 , 1.7311 , - 0.3481 , 0.3801 ], [ - 1.7810 , 1.0676 , - 0.7611 , 0.3658 , - 0.0431 , - 0.1012 , - 0.6048 , 0.3089 , 0.9998 , 0.7164 ], [ - 0.5856 , - 0.5261 , - 0.4859 , - 1.0551 , - 0.1838 , - 0.2144 , - 1.2599 , 3.3891 , 0.4691 , 0.7566 ], [ - 0.4984 , - 1.7770 , - 1.1998 , - 0.1075 , 1.0882 , 0.4539 , - 0.5651 , 1.4381 , - 0.5678 , 1.7479 ], [ 0.2938 , - 1.8536 , 0.4259 , - 0.5429 , 0.0066 , 0.4120 , 2.3793 , - 0.3666 , - 0.2604 , 0.0382 ], [ - 0.4080 , - 0.9851 , 4.0264 , 0.1099 , - 0.1766 , - 1.1557 , 0.6419 , - 0.8147 , 0.7535 , - 1.1452 ], [ - 0.4636 , - 1.7323 , - 0.6433 , - 0.0274 , 0.7227 , - 0.1799 , - 0.9336 , 2.1881 , - 0.2073 , 1.6522 ], [ - 0.9617 , - 0.0348 , - 0.3980 , - 0.4738 , 0.7790 , 0.4671 , - 0.6115 , - 0.7067 , 1.3036 , 0.4923 ], [ - 1.0151 , - 2.5385 , - 0.6072 , 0.2902 , 3.1570 , 0.1062 , - 0.2169 , - 0.4491 , 0.6326 , 1.6829 ], [ - 1.8852 , 0.6066 , - 0.2840 , - 0.4475 , - 0.1147 , - 0.7858 , - 1.1805 , 3.0723 , 0.3960 , 0.9720 ], [ 0.0344 , - 1.4878 , - 0.9675 , 1.9649 , - 0.3146 , 1.2183 , 0.6730 , - 0.3650 , 0.0646 , - 0.0898 ], [ - 0.2118 , - 2.0350 , 0.9917 , - 0.8993 , 1.2334 , - 0.6723 , 2.5847 , - 0.0454 , - 0.4149 , 0.3927 ], [ - 1.7365 , 3.0447 , 0.5115 , 0.0786 , - 0.7544 , - 0.2158 , - 0.4876 , - 0.2891 , 0.5089 , - 0.6719 ], [ 0.3652 , - 0.5457 , - 0.1167 , 2.9056 , - 1.1622 , 0.8192 , - 1.3245 , - 0.6414 , 0.8097 , - 0.4958 ], [ - 0.8755 , - 0.6983 , 0.2208 , - 0.6463 , 0.5276 , 0.1145 , 2.7229 , - 1.0316 , 0.1905 , 0.2090 ], [ - 0.9702 , 0.1265 , - 0.0007 , - 0.5106 , 0.4970 , - 0.0804 , 0.0017 , 0.0607 , 0.6164 , 0.4490 ], [ - 0.8271 , - 0.6822 , - 0.7434 , 2.6457 , - 1.6143 , 1.1486 , - 1.0705 , 0.5611 , 0.6422 , 0.1250 ], [ - 1.9979 , 1.8175 , - 0.1658 , - 0.0343 , - 0.6292 , 0.1774 , 0.3150 , - 0.4633 , 0.9266 , 0.0252 ], [ - 0.9039 , - 0.6030 , - 0.2173 , - 1.1768 , 2.3198 , - 0.5072 , 0.3418 , - 0.1551 , 0.1282 , 1.4250 ], [ - 0.9891 , 0.5212 , - 0.4518 , 0.3267 , - 0.0759 , 0.3826 , - 0.0341 , 0.0382 , 0.2451 , 0.3658 ], [ - 2.1217 , 1.5102 , - 0.7828 , 0.3554 , - 0.4192 , - 0.0772 , 0.0578 , 0.8070 , 0.1701 , 0.5880 ], [ 1.0665 , - 1.3826 , 0.6243 , - 0.8096 , - 0.4227 , 0.5925 , 1.8112 , - 0.9946 , 0.2010 , - 0.7731 ], [ - 1.1263 , - 1.7484 , 0.0041 , - 0.5439 , 1.7242 , - 0.9475 , - 0.3835 , 0.8452 , 0.3077 , 2.2689 ]]) Printing output size This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs . size ()) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS torch . Size ([ 100 , 10 ]) Printing one output This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7. number 0: -0.4181 number 1: -1.0784 ... number 7: 2.9352 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs [ 0 , :]) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639, 2.9352, -0.1552, 0.8852]) Printing prediction output Because our output is of size 100 (our batch size), our prediction size would also of the size 100. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted . size ()) PREDICTION torch . Size ([ 100 ]) Print prediction value We are printing our prediction which as verified above, should be digit 7. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) PREDICTION tensor ( 7 ) Print prediction, label and label size We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7! iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 0' ) print ( labels [ 0 ]) PREDICTION tensor ( 7 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 0 tensor ( 7 ) Print second prediction and ground truth Again, the prediction is correct. Naturally, as our model is quite competent in this simple task. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 1 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 1' ) print ( labels [ 1 ]) PREDICTION tensor ( 2 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 1 tensor ( 2 ) Print accuracy Now we know what each object represents, we can understand how we arrived at our accuracy numbers. One last thing to note is that correct.item() has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this. correct = 0 total = 0 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * ( correct . item () / total ) print ( accuracy ) 82.94 Explanation of Python's .sum() function Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return True or in our case, those predictions that match actual labels (correct predictions). # Explaining .sum() python built-in function # correct += (predicted == labels).sum() import numpy as np a = np . ones (( 10 )) print ( a ) b = np . ones (( 10 )) print ( b ) print ( a == b ) print (( a == b ) . sum ()) # matrix a [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # matrix b [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # boolean array [ True True True True True True True True True True ] # number of elementswhere a matches b 10","title":"Break Down Accuracy Calculation"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#saving-model","text":"Saving PyTorch model This is how you save your model. Feel free to just change save_model = True to save your model save_model = False if save_model is True : # Saves only parameters torch . save ( model . state_dict (), 'awesome_model.pkl' )","title":"Saving Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch-gpu","text":"CPU version The usual 7-step process, getting repetitive by now which we like. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # 100 x 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value # 100 x 1 _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.876196026802063 . Accuracy : 64.44 Iteration : 1000. Loss : 1.5153584480285645 . Accuracy : 75.68 Iteration : 1500. Loss : 1.3521136045455933 . Accuracy : 78.98 Iteration : 2000. Loss : 1.2136967182159424 . Accuracy : 80.95 Iteration : 2500. Loss : 1.0934826135635376 . Accuracy : 81.97 Iteration : 3000. Loss : 1.024120569229126 . Accuracy : 82.49 GPU version 2 things must be on GPU - model - tensors Remember step 4 and 7 will be affected and this will be the same for all model building moving forward. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8571407794952393 . Accuracy : 68.99 Iteration : 1000. Loss : 1.5415704250335693 . Accuracy : 75.86 Iteration : 1500. Loss : 1.2755383253097534 . Accuracy : 78.92 Iteration : 2000. Loss : 1.2468739748001099 . Accuracy : 80.72 Iteration : 2500. Loss : 1.0708973407745361 . Accuracy : 81.73 Iteration : 3000. Loss : 1.0359245538711548 . Accuracy : 82.74","title":"Building a Logistic Regression Model with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#summary","text":"We've learnt to... Success Logistic regression basics Problems of linear regression In-depth Logistic Regression Get logits Get softmax Get cross-entropy loss Aim : reduce cross-entropy loss Built a logistic regression model in CPU and GPU Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Important things to be on GPU model tensors with gradients","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/","text":"Long Short-Term Memory (LSTM) network with PyTorch \u00b6 About LSTMs: Special RNN \u00b6 Capable of learning long-term dependencies LSTM = RNN on super juice RNN Transition to LSTM \u00b6 Building an LSTM with PyTorch \u00b6 Model A: 1 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 The usual loading of our MNIST dataset As usual, we've 60k training images and 10k testing images. Subsequently, we'll have 3 groups: training, validation and testing for a more robust evaluation of algorithms. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) print ( test_dataset . test_data . size ()) `` ` python print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ]) Step 2: Make Dataset Iterable \u00b6 Creating an iterable object for our dataset batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Creating an LSTM model class It is very similar to RNN in terms of the shape of our input of batch_dim x seq_dim x feature_dim . The only change is that we have our cell state on top of our hidden state. PyTorch's LSTM module handles all the other weights for our other gates. class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # 28 time steps # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out Step 4: Instantiate Model Class \u00b6 28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate our LSTM model input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Long Short-Term Memory Neural Network: Cross Entropy Loss Recurrent Neural Network : Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entry Loss Function Because we are doing a classification problem we'll be using a Cross Entropy function. If we were to do a regression problem, then we would typically use a MSE function. criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $ \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Mini-batch Stochastic Gradient Descent learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 1 Layer LSTM Groups of Parameters We will have 6 groups of parameters here comprising weights and biases from: - Input to Hidden Layer Affine Function - Hidden Layer to Output Affine Function - Hidden Layer to Hidden Layer Affine Function Notice how this is exactly the same number of groups of parameters as our RNN? But the sizes of these groups will be larger for an LSTM due to its gates. len ( list ( model . parameters ())) 6 In-depth Parameters Analysis Comparing to RNN's parameters , we've the same number of groups but for LSTM we've 4x the number of parameters! for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Parameters Breakdown \u00b6 This is the breakdown of the parameters associated with the respective affine functions Input \\rightarrow \\rightarrow Gates [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400] \\rightarrow b_1, b_3, b_5, b_7 [400] \\rightarrow b_1, b_3, b_5, b_7 Hidden State \\rightarrow \\rightarrow Gates [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400] \\rightarrow b_2, b_4, b_6, b_8 [400] \\rightarrow b_2, b_4, b_6, b_8 Hidden State \\rightarrow \\rightarrow Output [10, 100] \\rightarrow w_9 [10, 100] \\rightarrow w_9 [10] \\rightarrow b_9 [10] \\rightarrow b_9 Step 7: Train Model \u00b6 Process Convert inputs/labels to variables LSTM Input: (1, 28) RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Training 1 Hidden Layer LSTM # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as a torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.8390830755233765 . Accuracy : 72 Iteration : 1000. Loss : 0.46470555663108826 . Accuracy : 85 Iteration : 1500. Loss : 0.31465113162994385 . Accuracy : 91 Iteration : 2000. Loss : 0.19143860042095184 . Accuracy : 94 Iteration : 2500. Loss : 0.16134005784988403 . Accuracy : 95 Iteration : 3000. Loss : 0.255976140499115 . Accuracy : 95 Model B: 2 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Train 2 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize image images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) LSTMModel ( ( lstm ): LSTM ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3074915409088135 . Accuracy : 11 Iteration : 1000. Loss : 1.8854578733444214 . Accuracy : 35 Iteration : 1500. Loss : 0.5317062139511108 . Accuracy : 80 Iteration : 2000. Loss : 0.15290376543998718 . Accuracy : 92 Iteration : 2500. Loss : 0.19500978291034698 . Accuracy : 93 Iteration : 3000. Loss : 0.10683634132146835 . Accuracy : 95 Parameters Breakdown (Layer 1) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 28] [400, 28] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400] Parameters Breakdown (Layer 2) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 100] [400, 100] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400] Parameters Breakdown (Readout Layer) \u00b6 Hidden State \\rightarrow \\rightarrow Output [10, 100] [10, 100] [10] [10] Model C: 3 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 3 Hidden layer Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) LSTMModel( (lstm): LSTM(28, 100, num_layers=3, batch_first=True) (fc): Linear(in_features=100, out_features=10, bias=True) ) 14 torch.Size([400, 28]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([10, 100]) torch.Size([10]) Iteration: 500. Loss: 2.2927396297454834. Accuracy: 11 Iteration: 1000. Loss: 2.29740309715271. Accuracy: 11 Iteration: 1500. Loss: 2.1950502395629883. Accuracy: 20 Iteration: 2000. Loss: 1.0738657712936401. Accuracy: 59 Iteration: 2500. Loss: 0.5988132357597351. Accuracy: 79 Iteration: 3000. Loss: 0.4107239246368408. Accuracy: 88 Parameters Breakdown (Layer 1) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 28] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400] Parameters Breakdown (Layer 2) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400] Parameters Breakdown (Layer 3) \u00b6 Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400] Parameters Breakdown (Readout Layer) \u00b6 Hidden State \\rightarrow \\rightarrow Output [10, 100] [10] Comparison with RNN \u00b6 Model A RNN Model B RNN Model C RNN ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% Model A LSTM Model B LSTM Model C LSTM 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 96.05% 95.24% 91.22% Deep Learning Notes \u00b6 2 ways to expand a recurrent neural network More hidden units (o, i, f, g) gates More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Recurrent Neural Network with PyTorch (GPU) \u00b6 Model A: 3 Hidden Layers \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM on GPU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # One time step out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.3068575859069824 . Accuracy : 11 Iteration : 1000. Loss : 2.291989803314209 . Accuracy : 14 Iteration : 1500. Loss : 1.909593105316162 . Accuracy : 28 Iteration : 2000. Loss : 0.7345633506774902 . Accuracy : 71 Iteration : 2500. Loss : 0.45030108094215393 . Accuracy : 86 Iteration : 3000. Loss : 0.2627193331718445 . Accuracy : 89 Summary \u00b6 We've learnt to... Success RNN transition to LSTM LSTM Models in PyTorch Model A: 1 Hidden Layer LSTM Model B: 2 Hidden Layer LSTM Model C: 3 Hidden Layer LSTM Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More hidden units More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Long Short Term Memory Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#long-short-term-memory-lstm-network-with-pytorch","text":"","title":"Long Short-Term Memory (LSTM) network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#about-lstms-special-rnn","text":"Capable of learning long-term dependencies LSTM = RNN on super juice","title":"About LSTMs: Special RNN"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#rnn-transition-to-lstm","text":"","title":"RNN Transition to LSTM"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#building-an-lstm-with-pytorch","text":"","title":"Building an LSTM with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-a-1-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer","title":"Model A: 1 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 The usual loading of our MNIST dataset As usual, we've 60k training images and 10k testing images. Subsequently, we'll have 3 groups: training, validation and testing for a more robust evaluation of algorithms. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) print ( test_dataset . test_data . size ()) `` ` python print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ])","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-2-make-dataset-iterable","text":"Creating an iterable object for our dataset batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-3-create-model-class","text":"Creating an LSTM model class It is very similar to RNN in terms of the shape of our input of batch_dim x seq_dim x feature_dim . The only change is that we have our cell state on top of our hidden state. PyTorch's LSTM module handles all the other weights for our other gates. class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # 28 time steps # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-4-instantiate-model-class","text":"28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate our LSTM model input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-5-instantiate-loss-class","text":"Long Short-Term Memory Neural Network: Cross Entropy Loss Recurrent Neural Network : Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entry Loss Function Because we are doing a classification problem we'll be using a Cross Entropy function. If we were to do a regression problem, then we would typically use a MSE function. criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $ \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Mini-batch Stochastic Gradient Descent learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-in-depth","text":"1 Layer LSTM Groups of Parameters We will have 6 groups of parameters here comprising weights and biases from: - Input to Hidden Layer Affine Function - Hidden Layer to Output Affine Function - Hidden Layer to Hidden Layer Affine Function Notice how this is exactly the same number of groups of parameters as our RNN? But the sizes of these groups will be larger for an LSTM due to its gates. len ( list ( model . parameters ())) 6 In-depth Parameters Analysis Comparing to RNN's parameters , we've the same number of groups but for LSTM we've 4x the number of parameters! for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown","text":"This is the breakdown of the parameters associated with the respective affine functions Input \\rightarrow \\rightarrow Gates [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400, 28] \\rightarrow w_1, w_3, w_5, w_7 [400] \\rightarrow b_1, b_3, b_5, b_7 [400] \\rightarrow b_1, b_3, b_5, b_7 Hidden State \\rightarrow \\rightarrow Gates [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400,100] \\rightarrow w_2, w_4, w_6, w_8 [400] \\rightarrow b_2, b_4, b_6, b_8 [400] \\rightarrow b_2, b_4, b_6, b_8 Hidden State \\rightarrow \\rightarrow Output [10, 100] \\rightarrow w_9 [10, 100] \\rightarrow w_9 [10] \\rightarrow b_9 [10] \\rightarrow b_9","title":"Parameters Breakdown"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#step-7-train-model","text":"Process Convert inputs/labels to variables LSTM Input: (1, 28) RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Training 1 Hidden Layer LSTM # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as a torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.8390830755233765 . Accuracy : 72 Iteration : 1000. Loss : 0.46470555663108826 . Accuracy : 85 Iteration : 1500. Loss : 0.31465113162994385 . Accuracy : 91 Iteration : 2000. Loss : 0.19143860042095184 . Accuracy : 94 Iteration : 2500. Loss : 0.16134005784988403 . Accuracy : 95 Iteration : 3000. Loss : 0.255976140499115 . Accuracy : 95","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-b-2-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer","title":"Model B: 2 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Train 2 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as torch tensor with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize image images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) LSTMModel ( ( lstm ): LSTM ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 400 , 28 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 , 100 ]) torch . Size ([ 400 ]) torch . Size ([ 400 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3074915409088135 . Accuracy : 11 Iteration : 1000. Loss : 1.8854578733444214 . Accuracy : 35 Iteration : 1500. Loss : 0.5317062139511108 . Accuracy : 80 Iteration : 2000. Loss : 0.15290376543998718 . Accuracy : 92 Iteration : 2500. Loss : 0.19500978291034698 . Accuracy : 93 Iteration : 3000. Loss : 0.10683634132146835 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-1","text":"Input \\rightarrow \\rightarrow Gates [400, 28] [400, 28] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400]","title":"Parameters Breakdown (Layer 1)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-2","text":"Input \\rightarrow \\rightarrow Gates [400, 100] [400, 100] [400] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400,100] [400] [400]","title":"Parameters Breakdown (Layer 2)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-readout-layer","text":"Hidden State \\rightarrow \\rightarrow Output [10, 100] [10, 100] [10] [10]","title":"Parameters Breakdown (Readout Layer)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-c-3-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 3 Hidden layer","title":"Model C: 3 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach as we are doing truncated backpropagation through time (BPTT) # If we don't, we'll backprop all the way to the start even after going through another batch out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) LSTMModel( (lstm): LSTM(28, 100, num_layers=3, batch_first=True) (fc): Linear(in_features=100, out_features=10, bias=True) ) 14 torch.Size([400, 28]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([400, 100]) torch.Size([400, 100]) torch.Size([400]) torch.Size([400]) torch.Size([10, 100]) torch.Size([10]) Iteration: 500. Loss: 2.2927396297454834. Accuracy: 11 Iteration: 1000. Loss: 2.29740309715271. Accuracy: 11 Iteration: 1500. Loss: 2.1950502395629883. Accuracy: 20 Iteration: 2000. Loss: 1.0738657712936401. Accuracy: 59 Iteration: 2500. Loss: 0.5988132357597351. Accuracy: 79 Iteration: 3000. Loss: 0.4107239246368408. Accuracy: 88","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-1_1","text":"Input \\rightarrow \\rightarrow Gates [400, 28] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400]","title":"Parameters Breakdown (Layer 1)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-2_1","text":"Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400]","title":"Parameters Breakdown (Layer 2)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-layer-3","text":"Input \\rightarrow \\rightarrow Gates [400, 100] [400] Hidden State \\rightarrow \\rightarrow Gates [400,100] [400]","title":"Parameters Breakdown (Layer 3)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#parameters-breakdown-readout-layer_1","text":"Hidden State \\rightarrow \\rightarrow Output [10, 100] [10]","title":"Parameters Breakdown (Readout Layer)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#comparison-with-rnn","text":"Model A RNN Model B RNN Model C RNN ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% Model A LSTM Model B LSTM Model C LSTM 1 Hidden Layer 2 Hidden Layers 3 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 96.05% 95.24% 91.22%","title":"Comparison with RNN"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#deep-learning-notes","text":"2 ways to expand a recurrent neural network More hidden units (o, i, f, g) gates More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"Deep Learning Notes"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#3-building-a-recurrent-neural-network-with-pytorch-gpu","text":"","title":"3. Building a Recurrent Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#model-a-3-hidden-layers","text":"GPU: 2 things must be on GPU - model - tensors","title":"Model A: 3 Hidden Layers"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3 Hidden Layer LSTM on GPU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LSTMModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( LSTMModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your LSTM # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . lstm = nn . LSTM ( input_dim , hidden_dim , layer_dim , batch_first = True ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # Initialize cell state c0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () . to ( device ) # One time step out , ( hn , cn ) = self . lstm ( x , ( h0 . detach (), c0 . detach ())) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 3 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = LSTMModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) labels = labels . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.3068575859069824 . Accuracy : 11 Iteration : 1000. Loss : 2.291989803314209 . Accuracy : 14 Iteration : 1500. Loss : 1.909593105316162 . Accuracy : 28 Iteration : 2000. Loss : 0.7345633506774902 . Accuracy : 71 Iteration : 2500. Loss : 0.45030108094215393 . Accuracy : 86 Iteration : 3000. Loss : 0.2627193331718445 . Accuracy : 89","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#summary","text":"We've learnt to... Success RNN transition to LSTM LSTM Models in PyTorch Model A: 1 Hidden Layer LSTM Model B: 2 Hidden Layer LSTM Model C: 3 Hidden Layer LSTM Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More hidden units More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/","text":"Matrices with PyTorch \u00b6 Matrices \u00b6 Matrices Brief Introduction \u00b6 Basic definition: rectangular array of numbers. Tensors (PyTorch) Ndarrays (NumPy) 2 x 2 Matrix (R x C) 1 1 1 1 2 x 3 Matrix 1 1 1 1 1 1 Creating Matrices \u00b6 Create list # Creating a 2x2 array arr = [[ 1 , 2 ], [ 3 , 4 ]] print ( arr ) [[ 1 , 2 ], [ 3 , 4 ]] Create numpy array via list import numpy as np # Convert to NumPy np . array ( arr ) array ([[ 1 , 2 ], [ 3 , 4 ]]) Convert numpy array to PyTorch tensor import torch # Convert to PyTorch Tensor torch . Tensor ( arr ) 1 2 3 4 [ torch . FloatTensor of size 2 x2 ] Create Matrices with Default Values \u00b6 Create 2x2 numpy array of 1's np . ones (( 2 , 2 )) array ([[ 1. , 1. ], [ 1. , 1. ]]) Create 2x2 torch tensor of 1's torch . ones (( 2 , 2 )) 1 1 1 1 [torch.FloatTensor of size 2x2] Create 2x2 numpy array of random numbers np . random . rand ( 2 , 2 ) array ([[ 0.68270631 , 0.87721678 ], [ 0.07420986 , 0.79669375 ]]) Create 2x2 PyTorch tensor of random numbers torch . rand ( 2 , 2 ) 0.3900 0.8268 0.3888 0.5914 [ torch . FloatTensor of size 2 x2 ] Seeds for Reproducibility \u00b6 Why do we need seeds? We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced. Create seed to enable fixed numbers for random number generation # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Repeat random array generation to check If you do not set the seed, you would not get the same set of numbers like here. # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Create a numpy array without seed Notice how you get different numbers compared to the first 2 tries? # No seed np . random . rand ( 2 , 2 ) array ([[ 0.56804456 , 0.92559664 ], [ 0.07103606 , 0.0871293 ]]) Repeat numpy array generation without seed You get the point now, you get a totally different set of numbers. # No seed np . random . rand ( 2 , 2 ) array ([[ 0.0202184 , 0.83261985 ], [ 0.77815675 , 0.87001215 ]]) Create a PyTorch tensor with a fixed seed # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) Repeat creating a PyTorch fixed seed tensor # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) 0.5488 0.5928 0.7152 0.8443 [ torch . FloatTensor of size 2 x2 ] Creating a PyTorch tensor without seed Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed torch . rand ( 2 , 2 ) 0.6028 0.8579 0.5449 0.8473 [ torch . FloatTensor of size 2 x2 ] Repeat creating a PyTorch tensor without seed Notice how these are different numbers again? # Torch No Seed torch . rand ( 2 , 2 ) 0.4237 0.6236 0.6459 0.3844 [ torch . FloatTensor of size 2 x2 ] Seed for GPU is different for now... Fix a seed for GPU tensors When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. if torch . cuda . is_available (): torch . cuda . manual_seed_all ( 0 ) NumPy and Torch Bridge \u00b6 NumPy to Torch \u00b6 Create a numpy array of 1's # Numpy array np_array = np . ones (( 2 , 2 )) print ( np_array ) [[ 1. 1. ] [ 1. 1. ]] Get the type of class for the numpy array print ( type ( np_array )) < class ' numpy . ndarray '> Convert numpy array to PyTorch tensor # Convert to Torch Tensor torch_tensor = torch . from_numpy ( np_array ) print ( torch_tensor ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Get type of class for PyTorch tensor Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor )) < class ' torch . DoubleTensor '> Create PyTorch tensor from a different numpy datatype You will get an error running this code because PyTorch tensor don't support all datatype. # Data types matter: intentional error np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) torch . from_numpy ( np_array_new ) --------------------------------------------------------------------------- RuntimeError Traceback ( most recent call last ) < ipython - input - 57 - b8b085f9b39d > in < module > () 1 # Data types matter 2 np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) ----> 3 torch . from_numpy ( np_array_new ) RuntimeError : can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8. What conversion support does Numpy to PyTorch tensor bridge gives? double float int64 , int32 , uint8 Create PyTorch long tensor See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int64 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [torch.LongTensor of size 2x2] Create PyTorch int tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . IntTensor of size 2 x2 ] Create PyTorch byte tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . uint8 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . ByteTensor of size 2 x2 ] Create PyTorch Double Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float64 ) torch . from_numpy ( np_array_new ) Alternatively you can do this too via np.double # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . double ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Create PyTorch Float Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Tensor Type Bug Guide These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide! NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor Torch to NumPy \u00b6 Create PyTorch tensor of 1's You would realize this defaults to a float tensor by default if you do this. torch_tensor = torch . ones ( 2 , 2 ) type ( torch_tensor ) torch . FloatTensor Convert tensor to numpy It's as simple as this. torch_to_numpy = torch_tensor . numpy () type ( torch_to_numpy ) # Wowza, we did it. numpy . ndarray Tensors on CPU vs GPU \u00b6 Move tensor to CPU and back This by default creates a tensor on CPU. You do not need to do anything. # CPU tensor_cpu = torch . ones ( 2 , 2 ) If you would like to send a tensor to your GPU, you just need to do a simple .cuda () # CPU to GPU device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) tensor_cpu . to ( device ) And if you want to move that tensor on the GPU back to the CPU, just do the following. # GPU to CPU tensor_cpu . cpu () Tensor Operations \u00b6 Resizing Tensor \u00b6 Creating a 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Getting size of tensor print ( a . size ()) torch . Size ([ 2 , 2 ]) Resize tensor to 4x1 a . view ( 4 ) 1 1 1 1 [ torch . FloatTensor of size 4 ] Get size of resized tensor a . view ( 4 ) . size () torch . Size ([ 4 ]) Element-wise Addition \u00b6 Creating first 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Creating second 2x2 tensor b = torch . ones ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise addition of 2 tensors # Element-wise addition c = a + b print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] Alternative element-wise addition of 2 tensors # Element-wise addition c = torch . add ( a , b ) print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] In-place element-wise addition This would replace the c tensor values with the new addition. # In-place addition print ( 'Old c tensor' ) print ( c ) c . add_ ( a ) print ( '-' * 60 ) print ( 'New c tensor' ) print ( c ) Old c tensor 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] ------------------------------------------------------------ New c tensor 3 3 3 3 [ torch . FloatTensor of size 2 x2 ] Element-wise Subtraction \u00b6 Check values of tensor a and b' Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above. print ( a ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 1 a - b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 2 # Not in-place print ( a . sub ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 3 This will replace a with the final result filled with 2's # Inplace print ( a . sub_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-Wise Multiplication \u00b6 Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 1 a * b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 2 # Not in-place print ( torch . mul ( a , b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 3 # In-place print ( a . mul_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-Wise Division \u00b6 Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 1 b / a 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 2 torch . div ( b , a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 3 # Inplace b . div_ ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Tensor Mean \u00b6 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 mean = 55 /10 = 5.5 mean = 55 /10 = 5.5 Create tensor of size 10 filled from 1 to 10 a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . size () torch . Size ([ 10 ]) Get tensor mean Here we get 5.5 as we've calculated manually above. a . mean ( dim = 0 ) 5.5000 [ torch . FloatTensor of size 1 ] Get tensor mean on second dimension Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate. a . mean ( dim = 1 ) RuntimeError Traceback ( most recent call last ) < ipython - input - 7 - 81 aec0cf1c00 > in < module > () ----> 1 a . mean ( dim = 1 ) RuntimeError : dimension out of range ( expected to be in range of [ - 1 , 0 ], but got 1 ) Create a 2x10 Tensor, of 1-10 digits each a = torch . Tensor ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]]) a . size () torch . Size ([ 2 , 10 ]) Get tensor mean on second dimension Here we won't get an error like previously because we've a tensor of size 2x10 a . mean ( dim = 1 ) 5.5000 5.5000 [ torch . FloatTensor of size 2 x1 ] Tensor Standard Deviation \u00b6 Get standard deviation of tensor a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . std ( dim = 0 ) 3.0277 [ torch . FloatTensor of size 1 ] Summary \u00b6 We've learnt to... Success Create Matrices Create Matrices with Default Initialization Values Zeros Ones Initialize Seeds for Reproducibility on GPU and CPU Convert Matrices: NumPy to Torch and Torch to NumPy Move Tensors: CPU to GPU and GPU to CPU Run Important Tensor Operations Element-wise addition, subtraction, multiplication and division Resize Calculate mean Calculate standard deviation Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices-with-pytorch","text":"","title":"Matrices with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices","text":"","title":"Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction","text":"Basic definition: rectangular array of numbers. Tensors (PyTorch) Ndarrays (NumPy) 2 x 2 Matrix (R x C) 1 1 1 1 2 x 3 Matrix 1 1 1 1 1 1","title":"Matrices Brief Introduction"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices","text":"Create list # Creating a 2x2 array arr = [[ 1 , 2 ], [ 3 , 4 ]] print ( arr ) [[ 1 , 2 ], [ 3 , 4 ]] Create numpy array via list import numpy as np # Convert to NumPy np . array ( arr ) array ([[ 1 , 2 ], [ 3 , 4 ]]) Convert numpy array to PyTorch tensor import torch # Convert to PyTorch Tensor torch . Tensor ( arr ) 1 2 3 4 [ torch . FloatTensor of size 2 x2 ]","title":"Creating Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values","text":"Create 2x2 numpy array of 1's np . ones (( 2 , 2 )) array ([[ 1. , 1. ], [ 1. , 1. ]]) Create 2x2 torch tensor of 1's torch . ones (( 2 , 2 )) 1 1 1 1 [torch.FloatTensor of size 2x2] Create 2x2 numpy array of random numbers np . random . rand ( 2 , 2 ) array ([[ 0.68270631 , 0.87721678 ], [ 0.07420986 , 0.79669375 ]]) Create 2x2 PyTorch tensor of random numbers torch . rand ( 2 , 2 ) 0.3900 0.8268 0.3888 0.5914 [ torch . FloatTensor of size 2 x2 ]","title":"Create Matrices with Default Values"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility","text":"Why do we need seeds? We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced. Create seed to enable fixed numbers for random number generation # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Repeat random array generation to check If you do not set the seed, you would not get the same set of numbers like here. # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Create a numpy array without seed Notice how you get different numbers compared to the first 2 tries? # No seed np . random . rand ( 2 , 2 ) array ([[ 0.56804456 , 0.92559664 ], [ 0.07103606 , 0.0871293 ]]) Repeat numpy array generation without seed You get the point now, you get a totally different set of numbers. # No seed np . random . rand ( 2 , 2 ) array ([[ 0.0202184 , 0.83261985 ], [ 0.77815675 , 0.87001215 ]]) Create a PyTorch tensor with a fixed seed # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) Repeat creating a PyTorch fixed seed tensor # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) 0.5488 0.5928 0.7152 0.8443 [ torch . FloatTensor of size 2 x2 ] Creating a PyTorch tensor without seed Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed torch . rand ( 2 , 2 ) 0.6028 0.8579 0.5449 0.8473 [ torch . FloatTensor of size 2 x2 ] Repeat creating a PyTorch tensor without seed Notice how these are different numbers again? # Torch No Seed torch . rand ( 2 , 2 ) 0.4237 0.6236 0.6459 0.3844 [ torch . FloatTensor of size 2 x2 ] Seed for GPU is different for now... Fix a seed for GPU tensors When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. if torch . cuda . is_available (): torch . cuda . manual_seed_all ( 0 )","title":"Seeds for Reproducibility"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge","text":"","title":"NumPy and Torch Bridge"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch","text":"Create a numpy array of 1's # Numpy array np_array = np . ones (( 2 , 2 )) print ( np_array ) [[ 1. 1. ] [ 1. 1. ]] Get the type of class for the numpy array print ( type ( np_array )) < class ' numpy . ndarray '> Convert numpy array to PyTorch tensor # Convert to Torch Tensor torch_tensor = torch . from_numpy ( np_array ) print ( torch_tensor ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Get type of class for PyTorch tensor Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor )) < class ' torch . DoubleTensor '> Create PyTorch tensor from a different numpy datatype You will get an error running this code because PyTorch tensor don't support all datatype. # Data types matter: intentional error np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) torch . from_numpy ( np_array_new ) --------------------------------------------------------------------------- RuntimeError Traceback ( most recent call last ) < ipython - input - 57 - b8b085f9b39d > in < module > () 1 # Data types matter 2 np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) ----> 3 torch . from_numpy ( np_array_new ) RuntimeError : can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8. What conversion support does Numpy to PyTorch tensor bridge gives? double float int64 , int32 , uint8 Create PyTorch long tensor See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int64 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [torch.LongTensor of size 2x2] Create PyTorch int tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . IntTensor of size 2 x2 ] Create PyTorch byte tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . uint8 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . ByteTensor of size 2 x2 ] Create PyTorch Double Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float64 ) torch . from_numpy ( np_array_new ) Alternatively you can do this too via np.double # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . double ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Create PyTorch Float Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Tensor Type Bug Guide These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide! NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor","title":"NumPy to Torch"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy","text":"Create PyTorch tensor of 1's You would realize this defaults to a float tensor by default if you do this. torch_tensor = torch . ones ( 2 , 2 ) type ( torch_tensor ) torch . FloatTensor Convert tensor to numpy It's as simple as this. torch_to_numpy = torch_tensor . numpy () type ( torch_to_numpy ) # Wowza, we did it. numpy . ndarray","title":"Torch to NumPy"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu","text":"Move tensor to CPU and back This by default creates a tensor on CPU. You do not need to do anything. # CPU tensor_cpu = torch . ones ( 2 , 2 ) If you would like to send a tensor to your GPU, you just need to do a simple .cuda () # CPU to GPU device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) tensor_cpu . to ( device ) And if you want to move that tensor on the GPU back to the CPU, just do the following. # GPU to CPU tensor_cpu . cpu ()","title":"Tensors on CPU vs GPU"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations","text":"","title":"Tensor Operations"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor","text":"Creating a 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Getting size of tensor print ( a . size ()) torch . Size ([ 2 , 2 ]) Resize tensor to 4x1 a . view ( 4 ) 1 1 1 1 [ torch . FloatTensor of size 4 ] Get size of resized tensor a . view ( 4 ) . size () torch . Size ([ 4 ])","title":"Resizing Tensor"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition","text":"Creating first 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Creating second 2x2 tensor b = torch . ones ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise addition of 2 tensors # Element-wise addition c = a + b print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] Alternative element-wise addition of 2 tensors # Element-wise addition c = torch . add ( a , b ) print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] In-place element-wise addition This would replace the c tensor values with the new addition. # In-place addition print ( 'Old c tensor' ) print ( c ) c . add_ ( a ) print ( '-' * 60 ) print ( 'New c tensor' ) print ( c ) Old c tensor 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] ------------------------------------------------------------ New c tensor 3 3 3 3 [ torch . FloatTensor of size 2 x2 ]","title":"Element-wise Addition"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction","text":"Check values of tensor a and b' Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above. print ( a ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 1 a - b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 2 # Not in-place print ( a . sub ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 3 This will replace a with the final result filled with 2's # Inplace print ( a . sub_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-wise Subtraction"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication","text":"Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 1 a * b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 2 # Not in-place print ( torch . mul ( a , b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 3 # In-place print ( a . mul_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-Wise Multiplication"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division","text":"Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 1 b / a 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 2 torch . div ( b , a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 3 # Inplace b . div_ ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-Wise Division"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean","text":"1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 mean = 55 /10 = 5.5 mean = 55 /10 = 5.5 Create tensor of size 10 filled from 1 to 10 a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . size () torch . Size ([ 10 ]) Get tensor mean Here we get 5.5 as we've calculated manually above. a . mean ( dim = 0 ) 5.5000 [ torch . FloatTensor of size 1 ] Get tensor mean on second dimension Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate. a . mean ( dim = 1 ) RuntimeError Traceback ( most recent call last ) < ipython - input - 7 - 81 aec0cf1c00 > in < module > () ----> 1 a . mean ( dim = 1 ) RuntimeError : dimension out of range ( expected to be in range of [ - 1 , 0 ], but got 1 ) Create a 2x10 Tensor, of 1-10 digits each a = torch . Tensor ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]]) a . size () torch . Size ([ 2 , 10 ]) Get tensor mean on second dimension Here we won't get an error like previously because we've a tensor of size 2x10 a . mean ( dim = 1 ) 5.5000 5.5000 [ torch . FloatTensor of size 2 x1 ]","title":"Tensor Mean"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation","text":"Get standard deviation of tensor a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . std ( dim = 0 ) 3.0277 [ torch . FloatTensor of size 1 ]","title":"Tensor Standard Deviation"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#summary","text":"We've learnt to... Success Create Matrices Create Matrices with Default Initialization Values Zeros Ones Initialize Seeds for Reproducibility on GPU and CPU Convert Matrices: NumPy to Torch and Torch to NumPy Move Tensors: CPU to GPU and GPU to CPU Run Important Tensor Operations Element-wise addition, subtraction, multiplication and division Resize Calculate mean Calculate standard deviation","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/","text":"Recurrent Neural Network with PyTorch \u00b6 About Recurrent Neural Network \u00b6 Feedforward Neural Networks Transition to 1 Layer Recurrent Neural Networks (RNN) \u00b6 RNN is essentially an FNN but with a hidden layer (non-linear output) that passes on information to the next FNN Compared to an FNN, we've one additional set of weight and bias that allows information to flow from one FNN to another FNN sequentially that allows time-dependency. The diagram below shows the only difference between an FNN and a RNN. 2 Layer RNN Breakdown \u00b6 Building a Recurrent Neural Network with PyTorch \u00b6 Model A: 1 Hidden Layer (ReLU) \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer ReLU Activation Function Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Looking into the MNIST Dataset import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) We would have 60k training images of size 28 x 28 pixels. print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) Here we would have 10k testing images of the same size, 28 x 28 pixels. print ( test_dataset . test_data . size ()) print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ]) Step 2: Make Dataset Iterable \u00b6 Creating iterable objects to loop through subsequently batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 1 Layer RNN class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, input_dim) # batch_dim = number of samples per batch self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros # (layer_dim, batch_size, hidden_dim) h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 10 # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out Step 4: Instantiate Model Class \u00b6 28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate model class and assign to an object input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Recurrent Neural Network: Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entropy Loss for Classification Task criterion = nn . CrossEntropyLoss () Cross Entropy vs MSE Take note that there are cases where RNN, CNN and FNN use MSE as a loss function. We use cross entropy for classification tasks (predicting 0-9 digits in MNIST for example). And we use MSE for regression tasks (predicting temperatures in every December in San Francisco for example). Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Input to Hidden Layer Affine Function A1, B1 Hidden Layer to Output Affine Function A2, B2 Hidden Layer to Hidden Layer Affine Function A3, B3 Total groups of parameters We should have 6 groups as shown above. len ( list ( model . parameters ())) 6 Input to Hidden Weight Remember we defined our hidden layer to have a size of 100. Because our input is a size of 28 at each time step, this gives rise to a weight matrix of 100 x 28. # Input --> Hidden (A1) list ( model . parameters ())[ 0 ] . size () torch . Size ([ 100 , 28 ]) Input to Hidden Bias # Input --> Hidden BIAS (B1) list ( model . parameters ())[ 2 ] . size () torch . Size ([ 100 ]) Hidden to Hidden # Hidden --> Hidden (A3) list ( model . parameters ())[ 1 ] . size () torch . Size ([ 100 , 100 ]) Hidden to Hidden Bias # Hidden --> Hidden BIAS(B3) list ( model . parameters ())[ 3 ] . size () torch . Size ([ 100 ]) Hidden to Output # Hidden --> Output (A2) list ( model . parameters ())[ 4 ] . size () torch . Size ([ 10 , 100 ]) Hidden to Output Bias # Hidden --> Output BIAS (B2) list ( model . parameters ())[ 5 ] . size () torch . Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs/labels to tensors with gradient accumulation abilities RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Same 7 step process for training models # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.301494836807251 . Accuracy : 12 Iteration : 1000. Loss : 2.2986037731170654 . Accuracy : 14 Iteration : 1500. Loss : 2.278566598892212 . Accuracy : 18 Iteration : 2000. Loss : 2.169614315032959 . Accuracy : 21 Iteration : 2500. Loss : 1.1662731170654297 . Accuracy : 51 Iteration : 3000. Loss : 0.9290509223937988 . Accuracy : 71 Model B: 2 Hidden Layer (ReLU) \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer ReLU Activation Function Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Hidden Layer + ReLU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3019518852233887 . Accuracy : 11 Iteration : 1000. Loss : 2.299217700958252 . Accuracy : 11 Iteration : 1500. Loss : 2.279090166091919 . Accuracy : 14 Iteration : 2000. Loss : 2.126953125 . Accuracy : 25 Iteration : 2500. Loss : 1.356347680091858 . Accuracy : 57 Iteration : 3000. Loss : 0.7377720475196838 . Accuracy : 69 10 sets of parameters First hidden Layer A_1 = [100, 28] A_1 = [100, 28] A_3 = [100, 100] A_3 = [100, 100] B_1 = [100] B_1 = [100] B_3 = [100] B_3 = [100] Second hidden layer A_2 = [100, 100] A_2 = [100, 100] A_5 = [100, 100] A_5 = [100, 100] B_2 = [100] B_2 = [100] B_5 = [100] B_5 = [100] Readout layer A_4 = [10, 100] A_4 = [10, 100] B_4 = [10] B_4 = [10] Model C: 2 Hidden Layer \u00b6 Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer Tanh Activation Function Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model !!! \"2 Hidden + ReLU\" import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 0.5943437218666077 . Accuracy : 77 Iteration : 1000. Loss : 0.22048641741275787 . Accuracy : 91 Iteration : 1500. Loss : 0.18479223549365997 . Accuracy : 94 Iteration : 2000. Loss : 0.2723771929740906 . Accuracy : 91 Iteration : 2500. Loss : 0.18817797303199768 . Accuracy : 92 Iteration : 3000. Loss : 0.1685929149389267 . Accuracy : 92 Summary of Results \u00b6 Model A Model B Model C ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 2 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54% General Deep Learning Notes \u00b6 2 ways to expand a recurrent neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Recurrent Neural Network with PyTorch (GPU) \u00b6 Model C: 2 Hidden Layer (Tanh) \u00b6 GPU: 2 things must be on GPU - model - tensors Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Layer RNN + Tanh import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . to ( device ) # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.5983774662017822 . Accuracy : 81 Iteration : 1000. Loss : 0.2960105836391449 . Accuracy : 86 Iteration : 1500. Loss : 0.19428101181983948 . Accuracy : 93 Iteration : 2000. Loss : 0.11918395012617111 . Accuracy : 95 Iteration : 2500. Loss : 0.11246936023235321 . Accuracy : 95 Iteration : 3000. Loss : 0.15849310159683228 . Accuracy : 95 Summary \u00b6 We've learnt to... Success Feedforward Neural Networks Transition to Recurrent Neural Networks RNN Models in PyTorch Model A: 1 Hidden Layer RNN (ReLU) Model B: 2 Hidden Layer RNN (ReLU) Model C: 2 Hidden Layer RNN (Tanh) Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 7: Train Model Citation \u00b6 If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Recurrent Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#recurrent-neural-network-with-pytorch","text":"","title":"Recurrent Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#about-recurrent-neural-network","text":"","title":"About Recurrent Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#feedforward-neural-networks-transition-to-1-layer-recurrent-neural-networks-rnn","text":"RNN is essentially an FNN but with a hidden layer (non-linear output) that passes on information to the next FNN Compared to an FNN, we've one additional set of weight and bias that allows information to flow from one FNN to another FNN sequentially that allows time-dependency. The diagram below shows the only difference between an FNN and a RNN.","title":"Feedforward Neural Networks Transition to 1 Layer Recurrent Neural Networks (RNN)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#2-layer-rnn-breakdown","text":"","title":"2 Layer RNN Breakdown"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#building-a-recurrent-neural-network-with-pytorch","text":"","title":"Building a Recurrent Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-a-1-hidden-layer-relu","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network input size: 28 x 28 1 Hidden layer ReLU Activation Function","title":"Model A: 1 Hidden Layer (ReLU)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 Looking into the MNIST Dataset import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) We would have 60k training images of size 28 x 28 pixels. print ( train_dataset . train_data . size ()) print ( train_dataset . train_labels . size ()) Here we would have 10k testing images of the same size, 28 x 28 pixels. print ( test_dataset . test_data . size ()) print ( test_dataset . test_labels . size ()) torch . Size ([ 60000 , 28 , 28 ]) torch . Size ([ 60000 ]) torch . Size ([ 10000 , 28 , 28 ]) torch . Size ([ 10000 ])","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-2-make-dataset-iterable","text":"Creating iterable objects to loop through subsequently batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-3-create-model-class","text":"1 Layer RNN class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, input_dim) # batch_dim = number of samples per batch self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros # (layer_dim, batch_size, hidden_dim) h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 10 # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-4-instantiate-model-class","text":"28 time steps Each time step: input dimension = 28 1 hidden layer MNIST 1-9 digits \\rightarrow \\rightarrow output dimension = 10 Instantiate model class and assign to an object input_dim = 28 hidden_dim = 100 layer_dim = 1 output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-5-instantiate-loss-class","text":"Recurrent Neural Network: Cross Entropy Loss Convolutional Neural Network : Cross Entropy Loss Feedforward Neural Network : Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Cross Entropy Loss for Classification Task criterion = nn . CrossEntropyLoss () Cross Entropy vs MSE Take note that there are cases where RNN, CNN and FNN use MSE as a loss function. We use cross entropy for classification tasks (predicting 0-9 digits in MNIST for example). And we use MSE for regression tasks (predicting temperatures in every December in San Francisco for example).","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation abilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : gradients of loss with respect to the model's parameters Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#parameters-in-depth","text":"Input to Hidden Layer Affine Function A1, B1 Hidden Layer to Output Affine Function A2, B2 Hidden Layer to Hidden Layer Affine Function A3, B3 Total groups of parameters We should have 6 groups as shown above. len ( list ( model . parameters ())) 6 Input to Hidden Weight Remember we defined our hidden layer to have a size of 100. Because our input is a size of 28 at each time step, this gives rise to a weight matrix of 100 x 28. # Input --> Hidden (A1) list ( model . parameters ())[ 0 ] . size () torch . Size ([ 100 , 28 ]) Input to Hidden Bias # Input --> Hidden BIAS (B1) list ( model . parameters ())[ 2 ] . size () torch . Size ([ 100 ]) Hidden to Hidden # Hidden --> Hidden (A3) list ( model . parameters ())[ 1 ] . size () torch . Size ([ 100 , 100 ]) Hidden to Hidden Bias # Hidden --> Hidden BIAS(B3) list ( model . parameters ())[ 3 ] . size () torch . Size ([ 100 ]) Hidden to Output # Hidden --> Output (A2) list ( model . parameters ())[ 4 ] . size () torch . Size ([ 10 , 100 ]) Hidden to Output Bias # Hidden --> Output BIAS (B2) list ( model . parameters ())[ 5 ] . size () torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#step-7-train-model","text":"Process Convert inputs/labels to tensors with gradient accumulation abilities RNN Input: (1, 28) CNN Input: (1, 28, 28) FNN Input: (1, 28*28) Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT Same 7 step process for training models # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 2.301494836807251 . Accuracy : 12 Iteration : 1000. Loss : 2.2986037731170654 . Accuracy : 14 Iteration : 1500. Loss : 2.278566598892212 . Accuracy : 18 Iteration : 2000. Loss : 2.169614315032959 . Accuracy : 21 Iteration : 2500. Loss : 1.1662731170654297 . Accuracy : 51 Iteration : 3000. Loss : 0.9290509223937988 . Accuracy : 71","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-b-2-hidden-layer-relu","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer ReLU Activation Function","title":"Model B: 2 Hidden Layer (ReLU)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Hidden Layer + ReLU import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'relu' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): model . train () # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : model . eval () # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 2.3019518852233887 . Accuracy : 11 Iteration : 1000. Loss : 2.299217700958252 . Accuracy : 11 Iteration : 1500. Loss : 2.279090166091919 . Accuracy : 14 Iteration : 2000. Loss : 2.126953125 . Accuracy : 25 Iteration : 2500. Loss : 1.356347680091858 . Accuracy : 57 Iteration : 3000. Loss : 0.7377720475196838 . Accuracy : 69 10 sets of parameters First hidden Layer A_1 = [100, 28] A_1 = [100, 28] A_3 = [100, 100] A_3 = [100, 100] B_1 = [100] B_1 = [100] B_3 = [100] B_3 = [100] Second hidden layer A_2 = [100, 100] A_2 = [100, 100] A_5 = [100, 100] A_5 = [100, 100] B_2 = [100] B_2 = [100] B_5 = [100] B_5 = [100] Readout layer A_4 = [10, 100] A_4 = [10, 100] B_4 = [10] B_4 = [10]","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-c-2-hidden-layer","text":"Unroll 28 time steps Each step input size: 28 x 1 Total per unroll: 28 x 28 Feedforward Neural Network inpt size: 28 x 28 2 Hidden layer Tanh Activation Function","title":"Model C: 2 Hidden Layer"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model !!! \"2 Hidden + ReLU\" import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . requires_grad_ () # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) # JUST PRINTING MODEL & PARAMETERS print ( model ) print ( len ( list ( model . parameters ()))) for i in range ( len ( list ( model . parameters ()))): print ( list ( model . parameters ())[ i ] . size ()) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Resize images images = images . view ( - 1 , seq_dim , input_dim ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) RNNModel ( ( rnn ): RNN ( 28 , 100 , num_layers = 2 , batch_first = True ) ( fc ): Linear ( in_features = 100 , out_features = 10 , bias = True ) ) 10 torch . Size ([ 100 , 28 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 , 100 ]) torch . Size ([ 100 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Iteration : 500. Loss : 0.5943437218666077 . Accuracy : 77 Iteration : 1000. Loss : 0.22048641741275787 . Accuracy : 91 Iteration : 1500. Loss : 0.18479223549365997 . Accuracy : 94 Iteration : 2000. Loss : 0.2723771929740906 . Accuracy : 91 Iteration : 2500. Loss : 0.18817797303199768 . Accuracy : 92 Iteration : 3000. Loss : 0.1685929149389267 . Accuracy : 92","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#summary-of-results","text":"Model A Model B Model C ReLU ReLU Tanh 1 Hidden Layer 2 Hidden Layers 2 Hidden Layers 100 Hidden Units 100 Hidden Units 100 Hidden Units 92.48% 95.09% 95.54%","title":"Summary of Results"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#general-deep-learning-notes","text":"2 ways to expand a recurrent neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Deep Learning Notes"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#3-building-a-recurrent-neural-network-with-pytorch-gpu","text":"","title":"3. Building a Recurrent Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#model-c-2-hidden-layer-tanh","text":"GPU: 2 things must be on GPU - model - tensors","title":"Model C: 2 Hidden Layer (Tanh)"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2 Layer RNN + Tanh import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class RNNModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , layer_dim , output_dim ): super ( RNNModel , self ) . __init__ () # Hidden dimensions self . hidden_dim = hidden_dim # Number of hidden layers self . layer_dim = layer_dim # Building your RNN # batch_first=True causes input/output tensors to be of shape # (batch_dim, seq_dim, feature_dim) self . rnn = nn . RNN ( input_dim , hidden_dim , layer_dim , batch_first = True , nonlinearity = 'tanh' ) # Readout layer self . fc = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Initialize hidden state with zeros ####################### # USE GPU FOR MODEL # ####################### h0 = torch . zeros ( self . layer_dim , x . size ( 0 ), self . hidden_dim ) . to ( device ) # One time step # We need to detach the hidden state to prevent exploding/vanishing gradients # This is part of truncated backpropagation through time (BPTT) out , hn = self . rnn ( x , h0 . detach ()) # Index hidden state of last time step # out.size() --> 100, 28, 100 # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! out = self . fc ( out [:, - 1 , :]) # out.size() --> 100, 10 return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 hidden_dim = 100 layer_dim = 2 # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER output_dim = 10 model = RNNModel ( input_dim , hidden_dim , layer_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' # Number of steps to unroll seq_dim = 28 iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as tensors with gradient accumulation abilities ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # outputs.size() --> 100, 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , seq_dim , input_dim ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions ####################### # USE GPU FOR MODEL # ####################### if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.5983774662017822 . Accuracy : 81 Iteration : 1000. Loss : 0.2960105836391449 . Accuracy : 86 Iteration : 1500. Loss : 0.19428101181983948 . Accuracy : 93 Iteration : 2000. Loss : 0.11918395012617111 . Accuracy : 95 Iteration : 2500. Loss : 0.11246936023235321 . Accuracy : 95 Iteration : 3000. Loss : 0.15849310159683228 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#summary","text":"We've learnt to... Success Feedforward Neural Networks Transition to Recurrent Neural Networks RNN Models in PyTorch Model A: 1 Hidden Layer RNN (ReLU) Model B: 2 Hidden Layer RNN (ReLU) Model C: 2 Hidden Layer RNN (Tanh) Models Variation in Code Modifying only step 4 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation abilities Modifying only Step 3, 4 and 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/#citation","text":"If you have found these useful in your research, presentations, school work, projects or workshops, feel free to cite using this DOI.","title":"Citation"},{"location":"news/ammi_facebook_google_recap_2018_11_21/","text":"AMMI (AIMS) Recap Supported by Facebook and Google \u00b6 To the future of AI in and beyond Africa \u00b6 Helped to teach these students from The African Masters of Machine Intelligence (AMMI) by The African Institute for Mathematical Sciences (AIMS) with Alfredo Canziani. A unique pedagogy is required for deep learning beginning with intuition, culminating in just enough math, and ending with programming. This approached has worked when I delivered my deep learning course to over 3000 students over 120 countries. And I continue to champion this pedagogy. I\u2019m very pleased that I share the same passion with Alfedo when he delivered his interactive lectures. Importantly, I see great potential when I personally interacted with students here. I see future deep learning researchers and applied machine intelligence experts who have the potential to make groundbreaking changes in the fields of healthcare, agriculture and education throughout Africa. With the right education, mentorship and opportunities, they would be pioneers. Simply put, deep learning talent is underexplored in this continent. I am thankful to Alfredo Canziani who has been my best pal and inspirational guy while I was there, Moustapha Cisse for creating these opportunities for young African students to thrive, #Facebook and #Google for supporting this program, and Pedro Antonio Mart\u00ednez Mediano and Marc Deisenroff. Coincidentally, I would also like to thank Yoshua Bengio for holding ICLR in Africa in 2020 for the first time that he announced recently. Finally, really appreciate how #PyTorch has made programming neural networks more approachable and it was made possible with the amazing PyTorch community sparked by Soumith Chintala. Speaking about community, I got to contribute to the deep learning community with the help from supportive groups and individuals from the global deep learning community. Likewise, we should continually help these young Africans like any other. To the future of AI in and beyond Africa. Looking forward to help Moustapha in any way I can to nurture AI talent \ud83d\ude42 I'm being a fanboy here, but Yann Lecun liked my post on Facebook ! Cheers, Ritchie","title":"AMMI (AIMS) supported by Facebook and Google, November 2018"},{"location":"news/ammi_facebook_google_recap_2018_11_21/#ammi-aims-recap-supported-by-facebook-and-google","text":"","title":"AMMI (AIMS) Recap Supported by Facebook and Google"},{"location":"news/ammi_facebook_google_recap_2018_11_21/#to-the-future-of-ai-in-and-beyond-africa","text":"Helped to teach these students from The African Masters of Machine Intelligence (AMMI) by The African Institute for Mathematical Sciences (AIMS) with Alfredo Canziani. A unique pedagogy is required for deep learning beginning with intuition, culminating in just enough math, and ending with programming. This approached has worked when I delivered my deep learning course to over 3000 students over 120 countries. And I continue to champion this pedagogy. I\u2019m very pleased that I share the same passion with Alfedo when he delivered his interactive lectures. Importantly, I see great potential when I personally interacted with students here. I see future deep learning researchers and applied machine intelligence experts who have the potential to make groundbreaking changes in the fields of healthcare, agriculture and education throughout Africa. With the right education, mentorship and opportunities, they would be pioneers. Simply put, deep learning talent is underexplored in this continent. I am thankful to Alfredo Canziani who has been my best pal and inspirational guy while I was there, Moustapha Cisse for creating these opportunities for young African students to thrive, #Facebook and #Google for supporting this program, and Pedro Antonio Mart\u00ednez Mediano and Marc Deisenroff. Coincidentally, I would also like to thank Yoshua Bengio for holding ICLR in Africa in 2020 for the first time that he announced recently. Finally, really appreciate how #PyTorch has made programming neural networks more approachable and it was made possible with the amazing PyTorch community sparked by Soumith Chintala. Speaking about community, I got to contribute to the deep learning community with the help from supportive groups and individuals from the global deep learning community. Likewise, we should continually help these young Africans like any other. To the future of AI in and beyond Africa. Looking forward to help Moustapha in any way I can to nurture AI talent \ud83d\ude42 I'm being a fanboy here, but Yann Lecun liked my post on Facebook ! Cheers, Ritchie","title":"To the future of AI in and beyond Africa"},{"location":"news/deep_learning_wizard_1y_2018_06_01/","text":"Featured on PyTorch Website \u00b6 PyTorch a Year Later \u00b6 We are featured on PyTorch website's post I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday. A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned! A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome. To more great years ahead for PyTorch Cheers, Ritchie Ng","title":"Featured on PyTorch Website 2018"},{"location":"news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website","text":"","title":"Featured on PyTorch Website"},{"location":"news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later","text":"We are featured on PyTorch website's post I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday. A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned! A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome. To more great years ahead for PyTorch Cheers, Ritchie Ng","title":"PyTorch a Year Later"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/","text":"We Are an NVIDIA Inception Partner \u00b6 We did it! \u00b6 After almost a year, we are an NVIDIA Inception Partner now! \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Cheers, Ritchie Ng","title":"NVIDIA Inception Partner Status, Singapore, May 2017"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-are-an-nvidia-inception-partner","text":"","title":"We Are an NVIDIA Inception Partner"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-did-it","text":"After almost a year, we are an NVIDIA Inception Partner now! \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Cheers, Ritchie Ng","title":"We did it!"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/","text":"Recap of Facebook PyTorch Devcon \u00b6 Woo! \u00b6 Proud of how far PyTorch has come. Thanks Soumith Chintala, it is a great journey and it just started. And thanks to Adam Paszke too! Finally got to catch up with Soumith Chintala, Alfredo Canziani and Marek Bardo\u0144ski and found Andrej Karpathy! Back to Singapore tonight \ud83e\udd17 Here are some photos of my trip! Cheers, Ritchie","title":"Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/#recap-of-facebook-pytorch-devcon","text":"","title":"Recap of Facebook PyTorch Devcon"},{"location":"news/facebook_pytorch_devcon_recap_2018_10_02/#woo","text":"Proud of how far PyTorch has come. Thanks Soumith Chintala, it is a great journey and it just started. And thanks to Adam Paszke too! Finally got to catch up with Soumith Chintala, Alfredo Canziani and Marek Bardo\u0144ski and found Andrej Karpathy! Back to Singapore tonight \ud83e\udd17 Here are some photos of my trip! Cheers, Ritchie","title":"Woo!"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/","text":"Facebook PyTorch Developer Conference \u00b6 We are heading down! \u00b6 In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA. I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it! Cheers, Ritchie Ng","title":"Facebook PyTorch Developer Conference, San Francisco, September 2018"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#facebook-pytorch-developer-conference","text":"","title":"Facebook PyTorch Developer Conference"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#we-are-heading-down","text":"In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA. I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it! Cheers, Ritchie Ng","title":"We are heading down!"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/","text":"NExT++ AI in Finance and Healthcare Workshop 2018 \u00b6 Recap \u00b6 This is NExT++\u2019s 3 rd Workshop and this time we are looking at the applications and development of AI related technologies in the healthcare and finance verticals with the theme \u201cAI in Health and Finance\u201d Visual summary of my talk on AI and Unstructured Analytics in Fintech. Doesn't contain everything, but whatever the non-technical designers could come up with in real-time. Really amazing so kudos to them. Really thankful to the whole team in my former lab, NExT++ and a close friend, Choon Meng, for making all the arrangements. Also thanks to Tek Min, Yi Hao and everyone else. Super happy to catch up with Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). Cheers, Ritchie","title":"NExT++ AI in Healthcare and Finance, Nanjing, November 2018"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/#next-ai-in-finance-and-healthcare-workshop-2018","text":"","title":"NExT++ AI in Finance and Healthcare Workshop 2018"},{"location":"news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/#recap","text":"This is NExT++\u2019s 3 rd Workshop and this time we are looking at the applications and development of AI related technologies in the healthcare and finance verticals with the theme \u201cAI in Health and Finance\u201d Visual summary of my talk on AI and Unstructured Analytics in Fintech. Doesn't contain everything, but whatever the non-technical designers could come up with in real-time. Really amazing so kudos to them. Really thankful to the whole team in my former lab, NExT++ and a close friend, Choon Meng, for making all the arrangements. Also thanks to Tek Min, Yi Hao and everyone else. Super happy to catch up with Prof Tat-Seng Chua (KITHCT Chair Professor at the School of Computing), Prof Sun Maosong (Dean of Department of Computer Science and Technology, Tsinghua University), and Prof Dame Wendy Hall (Director of the Web Science Institute, University of Southampton). Cheers, Ritchie","title":"Recap"},{"location":"news/news/","text":"Welcome to our Blog \u00b6 Here, we post news related to Deep Learning Wizard's releases, features and achievements Notable News \u00b6 Foundations of Deep Learning, African Masters of Machine Intelligence (AMMI), Google & Facebook, Kigali, Rwanda, November 2018 Facebook PyTorch Developer Conference, San Francisco, USA, September 2018 Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018 Reached 2200+ students, 2018 Featured on PyTorch Website, January 2018 Reached 1000+ students, 2017 Hosted NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017 NVIDIA Inception Partner, May 2017","title":"Welcome"},{"location":"news/news/#welcome-to-our-blog","text":"Here, we post news related to Deep Learning Wizard's releases, features and achievements","title":"Welcome to our Blog"},{"location":"news/news/#notable-news","text":"Foundations of Deep Learning, African Masters of Machine Intelligence (AMMI), Google & Facebook, Kigali, Rwanda, November 2018 Facebook PyTorch Developer Conference, San Francisco, USA, September 2018 Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018 Reached 2200+ students, 2018 Featured on PyTorch Website, January 2018 Reached 1000+ students, 2017 Hosted NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017 NVIDIA Inception Partner, May 2017","title":"Notable News"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/","text":"NVIDIA Workshop at NUS-MIT-NUHS Datathon \u00b6 Image Recognition Workshop by Ritchie Ng \u00b6 The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning. In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance. Link to the NUS-MIT-NUHS Datathon workshop .","title":"NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#nvidia-workshop-at-nus-mit-nuhs-datathon","text":"","title":"NVIDIA Workshop at NUS-MIT-NUHS Datathon"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng","text":"The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning. In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance. Link to the NUS-MIT-NUHS Datathon workshop .","title":"Image Recognition Workshop by Ritchie Ng"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/","text":"NVIDIA Self-Driving Cars and Healthcare Workshop \u00b6 Hosted by Ritchie Ng \u00b6 A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS. We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA. Details: Wednesday, June 21 st 1:00 PM to 3:30 PM The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613","title":"NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#nvidia-self-driving-cars-and-healthcare-workshop","text":"","title":"NVIDIA Self-Driving Cars and Healthcare Workshop"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng","text":"A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS. We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA. Details: Wednesday, June 21 st 1:00 PM to 3:30 PM The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613","title":"Hosted by Ritchie Ng"}]}