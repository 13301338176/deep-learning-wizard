{
    "docs": [
        {
            "location": "/",
            "text": "About Us\n\u00b6\n\n\nWe deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our \nvideo course\n and join over 2300 deep learning wizards. \n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nPyTorch as our Preferred Deep Learning Library\n\u00b6\n\n\nWe chose \nPyTorch\n because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.\n\n\n# It is this easy! \n\n\nimport\n \ntorch\n\n\n\n# Create a variable of value 1 each.\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\nb\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\n\n# Add the 2 variables to give you 2, it's that simple!\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\n\n\n\nMade for Visual and Book Lovers\n\u00b6\n\n\nWe are visual creatures, that is why we offer detailed \nvideo courses\n on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. \n\n\nFor book lovers, you will be happy to know \nDeep Learning Wizard's wikipedia\n will always be updated first prior to our release of video courses.\n\n\nExperienced Research and Applied Team\n\u00b6\n\n\n\n\nRitchie Ng\n\n\nCurrently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.\n\n\nMy passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.\n\n\nI was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.\n\n\nCheck out my profile link at \nritchieng.com\n\n\n\n\n\n\nJie Fu\n\n\nI am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.\n\n\nI earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.\n\n\nI am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.\n\n\nCheck out my profile link at \nbigaidream.github.io",
            "title": "Home"
        },
        {
            "location": "/#about-us",
            "text": "We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our  video course  and join over 2300 deep learning wizards.   To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.",
            "title": "About Us"
        },
        {
            "location": "/#pytorch-as-our-preferred-deep-learning-library",
            "text": "We chose  PyTorch  because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.  # It is this easy!   import   torch  # Create a variable of value 1 each.  a   =   torch . Tensor ([ 1 ])  b   =   torch . Tensor ([ 1 ])  # Add the 2 variables to give you 2, it's that simple!  c   =   a   +   b",
            "title": "PyTorch as our Preferred Deep Learning Library"
        },
        {
            "location": "/#made-for-visual-and-book-lovers",
            "text": "We are visual creatures, that is why we offer detailed  video courses  on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch.   For book lovers, you will be happy to know  Deep Learning Wizard's wikipedia  will always be updated first prior to our release of video courses.",
            "title": "Made for Visual and Book Lovers"
        },
        {
            "location": "/#experienced-research-and-applied-team",
            "text": "Ritchie Ng  Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.  My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.  I was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.  Check out my profile link at  ritchieng.com    Jie Fu  I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.  I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.  I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.  Check out my profile link at  bigaidream.github.io",
            "title": "Experienced Research and Applied Team"
        },
        {
            "location": "/supporters/",
            "text": "Supporters\n\u00b6\n\n\nMore than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.\n\n\nIndividuals\n\u00b6\n\n\n\n\nAlfredo Canziani\n\n\nAlfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.\n\n\nHe is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun.\n\n\nDo check out his latest \nmini-course on PyTorch\n that was held in Princeton University.\n\n\n\n\n\n\nMarek Bardonski\n\n\nSince graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.\n\n\nNASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.\n\n\nSince his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference.\n\n\n\n\nCorporations\n\u00b6\n\n\n\n\n NVIDIA (NVIDIA Inception Partner)\n\n\n Facebook\n\n\n Amazon\n\n\n\n\nResearch Institutions\n\u00b6\n\n\n\n\n Montreal Institute of Learning Algorithms (MILA), Montreal, Canada\n\n\n Imperial College London, UK\n\n\n Massachusetts Institute of Technology (MIT), USA  \n\n\n National University of Singapore (NUS), Singapore\n\n\n Nanyang Technological University (NTU), Singapore\n\n\n\n\nNVIDIA Inception Partner\n\u00b6\n\n\n\n\n\"NVIDIA Inception Partner\n\n\n\n\nDeep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#supporters",
            "text": "More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#individuals",
            "text": "Alfredo Canziani  Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.  He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun.  Do check out his latest  mini-course on PyTorch  that was held in Princeton University.    Marek Bardonski  Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.  NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.  Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference.",
            "title": "Individuals"
        },
        {
            "location": "/supporters/#corporations",
            "text": "NVIDIA (NVIDIA Inception Partner)   Facebook   Amazon",
            "title": "Corporations"
        },
        {
            "location": "/supporters/#research-institutions",
            "text": "Montreal Institute of Learning Algorithms (MILA), Montreal, Canada   Imperial College London, UK   Massachusetts Institute of Technology (MIT), USA     National University of Singapore (NUS), Singapore   Nanyang Technological University (NTU), Singapore",
            "title": "Research Institutions"
        },
        {
            "location": "/supporters/#nvidia-inception-partner",
            "text": "\"NVIDIA Inception Partner   Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "NVIDIA Inception Partner"
        },
        {
            "location": "/review/",
            "text": "Reviews\n\u00b6\n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nThese are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.\n\n\n\n\nRoberto Trevi\u00f1o Cervantes\n\n\nCongratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.\n\n\n\n\n\n\nMuktabh Mayank\n\n\nThis course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.\n\n\n\n\n\n\nCharles Neiswender\n\n\nI really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.\n\n\n\n\n\n\nIan Lipton\n\n\nThis was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.\n\n\n\n\nAnd check out hundreds of more reviews for our \nvideo course\n!",
            "title": "Reviews"
        },
        {
            "location": "/review/#reviews",
            "text": "To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.  These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.   Roberto Trevi\u00f1o Cervantes  Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.    Muktabh Mayank  This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.    Charles Neiswender  I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.    Ian Lipton  This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.   And check out hundreds of more reviews for our  video course !",
            "title": "Reviews"
        },
        {
            "location": "/deep_learning/intro/",
            "text": "Deep Learning Theory and Programming Tutorials\n\u00b6\n\n\nOur main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our \nvideo course\n.\n\n\n\n\nWork-in-progress\n\n\nThis open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact \nRitchie Ng\n if you would like to contribute via our \nFacebook\n page.\n\n\nAlso take note that these notes are best used as a referral if you are new to deep learning and programming. Go head and take our \nvideo course\n that provides a much easier experience.",
            "title": "Introduction"
        },
        {
            "location": "/deep_learning/intro/#deep-learning-theory-and-programming-tutorials",
            "text": "Our main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our  video course .   Work-in-progress  This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact  Ritchie Ng  if you would like to contribute via our  Facebook  page.  Also take note that these notes are best used as a referral if you are new to deep learning and programming. Go head and take our  video course  that provides a much easier experience.",
            "title": "Deep Learning Theory and Programming Tutorials"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/",
            "text": "PyTorch Fundamentals - Matrices\n\u00b6\n\n\nMatrices\n\u00b6\n\n\nMatrices Brief Introduction\n\u00b6\n\n\n\n\n Basic definition: rectangular array of numbers.\n\n\n Tensors (PyTorch)\n\n\n Ndarrays (NumPy)\n\n\n\n\n2 x 2 Matrix (R x C)\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n2 x 3 Matrix\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nCreating Matrices\n\u00b6\n\n\n\n\nCreate list\n\n\n# Creating a 2x2 array\n\n\narr\n \n=\n \n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\nprint\n(\narr\n)\n\n\n\n\n\n\n\n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\n\n\n\n\n\nCreate numpy array via list\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n\n# Convert to NumPy\n\n\nnp\n.\narray\n(\narr\n)\n\n\n\n\n\n\narray\n([[\n1\n,\n \n2\n],\n\n       \n[\n3\n,\n \n4\n]])\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\nimport\n \ntorch\n\n\n\n\n\n# Convert to PyTorch Tensor\n\n\ntorch\n.\nTensor\n(\narr\n)\n\n\n\n\n\n\n\n1\n  \n2\n\n\n3\n  \n4\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nCreate Matrices with Default Values\n\u00b6\n\n\n\n\nCreate 2x2 numpy array of 1's\n\n\nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\narray\n([[\n \n1.\n,\n  \n1.\n],\n\n       \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\nCreate 2x2 torch tensor of 1's\n\n\ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\n\n\n\n\n\nCreate 2x2 numpy array of random numbers\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\narray\n([[\n \n0.68270631\n,\n  \n0.87721678\n],\n\n       \n[\n \n0.07420986\n,\n  \n0.79669375\n]])\n\n\n\n\n\n\n\nCreate 2x2 PyTorch tensor of random numbers\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n0.3900\n  \n0.8268\n\n\n0.3888\n  \n0.5914\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nSeeds for Reproducibility\n\u00b6\n\n\n\n\nWhy do we need seeds?\n\n\nWe need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.\n\n\n\n\n\n\nCreate seed to enable fixed numbers for random number generation \n\n\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\nRepeat random array generation to check\n\n\nIf you do not set the seed, you would not get the same set of numbers like here.\n\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\nCreate a numpy array without seed\n\n\nNotice how you get different numbers compared to the first 2 tries?\n\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.56804456\n,\n  \n0.92559664\n],\n\n       \n[\n \n0.07103606\n,\n  \n0.0871293\n \n]])\n\n\n\n\n\n\n\nRepeat numpy array generation without seed\n\n\nYou get the point now, you get a totally different set of numbers.\n\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.0202184\n \n,\n  \n0.83261985\n],\n\n       \n[\n \n0.77815675\n,\n  \n0.87001215\n]])\n\n\n\n\n\n\n\nCreate a PyTorch tensor with a fixed seed\n\n\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\nRepeat creating a PyTorch fixed seed tensor\n\n\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n0.5488\n  \n0.5928\n\n\n0.7152\n  \n0.8443\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreating a PyTorch tensor without seed\n\n\nLike with a numpy array of random numbers without seed, you will not get the same results as above.\n\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n0.6028\n  \n0.8579\n\n\n0.5449\n  \n0.8473\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nRepeat creating a PyTorch tensor without seed\n\n\nNotice how these are different numbers again?\n\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n0.4237\n  \n0.6236\n\n\n0.6459\n  \n0.3844\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nSeed for GPU is different for now...\n\n\n\n\nFix a seed for GPU tensors\n\n\nWhen you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. \n\nif\n \ntorch\n.\ncuda\n.\nis_available\n():\n\n    \ntorch\n.\ncuda\n.\nmanual_seed_all\n(\n0\n)\n\n\n\n\n\n\nNumPy and Torch Bridge\n\u00b6\n\n\nNumPy to Torch\n\u00b6\n\n\n\n\nCreate a numpy array of 1's\n\n\n# Numpy array\n\n\nnp_array\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\nprint\n(\nnp_array\n)\n\n\n\n\n\n\n\n[[\n \n1.\n  \n1.\n]\n\n\n[\n \n1.\n  \n1.\n]]\n\n\n\n\n\n\n\nGet the type of class for the numpy array\n\n\nprint\n(\ntype\n(\nnp_array\n))\n\n\n\n\n\n\n\n<\nclass\n \n'\nnumpy\n.\nndarray\n'>\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\n# Convert to Torch Tensor\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nfrom_numpy\n(\nnp_array\n)\n\n\n\n\n\nprint\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nGet type of class for PyTorch tensor\n\n\nNotice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type.\n\nprint\n(\ntype\n(\ntorch_tensor\n))\n\n\n\n\n\n\n<\nclass\n \n'\ntorch\n.\nDoubleTensor\n'>\n\n\n\n\n\n\n\nCreate PyTorch tensor from a different numpy datatype\n\n\nYou will get an error running this code because PyTorch tensor don't support all datatype. \n\n# Data types matter: intentional error\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n---------------------------------------------------------------------------\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n57\n-\nb8b085f9b39d\n>\n \nin\n \n<\nmodule\n>\n()\n\n      \n1\n \n# Data types matter\n\n      \n2\n \nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\n---->\n \n3\n \ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\nRuntimeError\n:\n \ncan\n't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\n\n\n\n\n\n\n\nWhat conversion support does Numpy to PyTorch tensor bridge gives?\n\n\n\n\ndouble\n\n\nfloat\n \n\n\nint64\n, \nint32\n, \nuint8\n \n\n\n\n\n\n\n\n\nCreate PyTorch long tensor\n\n\nSee how a int64 numpy array gives you a PyTorch long tensor?\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n1  1\n1  1\n[torch.LongTensor of size 2x2]\n\n\n\n\n\n\nCreate PyTorch int tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nIntTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch byte tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nuint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nByteTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch Double Tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\nAlternatively you can do this too via \nnp.double\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\ndouble\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch Float Tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nTensor Type Bug Guide\n\n\nThese things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!\n\n\n\n\n\n\n\n\n\n\nNumPy Array Type\n\n\nTorch Tensor Type\n\n\n\n\n\n\n\n\n\n\nint64\n\n\nLongTensor\n\n\n\n\n\n\nint32\n\n\nIntegerTensor\n\n\n\n\n\n\nuint8\n\n\nByteTensor\n\n\n\n\n\n\nfloat64\n\n\nDoubleTensor\n\n\n\n\n\n\nfloat32\n\n\nFloatTensor\n\n\n\n\n\n\ndouble\n\n\nDoubleTensor\n\n\n\n\n\n\n\n\nTorch to NumPy\n\u00b6\n\n\n\n\nCreate PyTorch tensor of 1's\n\n\nYou would realize this defaults to a float tensor by default if you do this.\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\n\ntype\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\ntorch\n.\nFloatTensor\n\n\n\n\n\n\n\nConvert tensor to numpy\n\n\nIt's as simple as this.\n\n\ntorch_to_numpy\n \n=\n \ntorch_tensor\n.\nnumpy\n()\n\n\n\n\n\ntype\n(\ntorch_to_numpy\n)\n\n\n\n\n\n\n\n# Wowza, we did it.\n\n\nnumpy\n.\nndarray\n\n\n\n\n\nTensors on CPU vs GPU\n\u00b6\n\n\n\n\nMove tensor to CPU and back\n\n\nThis by default creates a tensor on CPU. You do not need to do anything.\n\n# CPU\n\n\ntensor_cpu\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\nIf you would like to send a tensor to your GPU, you just need to do a simple \n.cuda\n()\n\n\n# CPU to GPU\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\ntensor_cpu\n.\nto\n(\ndevice\n)\n\n\n\n\n\nAnd if you want to move that tensor on the GPU back to the CPU, just do the following.\n\n\n# GPU to CPU\n\n\ntensor_cpu\n.\ncpu\n()\n\n\n\n\n\n\n\nTensor Operations\n\u00b6\n\n\nResizing Tensor\n\u00b6\n\n\n\n\nCreating a 2x2 tensor\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nGetting size of tensor\n\n\nprint\n(\na\n.\nsize\n())\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n2\n,\n \n2\n])\n\n\n\n\n\n\n\nResize tensor to 4x1\n\n\na\n.\nview\n(\n4\n)\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n4\n]\n\n\n\n\n\n\n\nGet size of resized tensor\n\n\na\n.\nview\n(\n4\n)\n.\nsize\n()\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n4\n])\n\n\n\n\n\nElement-wise Addition\n\u00b6\n\n\n\n\nCreating first 2x2 tensor\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreating second 2x2 tensor\n\n\nb\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise addition of 2 tensors\n\n\n# Element-wise addition\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nAlternative element-wise addition of 2 tensors\n\n\n# Element-wise addition\n\n\nc\n \n=\n \ntorch\n.\nadd\n(\na\n,\n \nb\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nIn-place element-wise addition\n\n\nThis would replace the c tensor values with the new addition. \n\n\n# In-place addition\n\n\nprint\n(\n'Old c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\nc\n.\nadd_\n(\na\n)\n\n\n\nprint\n(\n'-'\n*\n60\n)\n\n\nprint\n(\n'New c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\nOld\n \nc\n \ntensor\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n------------------------------------------------------------\n\n\nNew\n \nc\n \ntensor\n\n\n \n3\n  \n3\n\n \n3\n  \n3\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-wise Subtraction\n\u00b6\n\n\n\n\nCheck values of tensor a and b'\n\n\nTake note that you've created tensor a and b of sizes 2x2 filled with 1's each above. \n\nprint\n(\na\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 1\n\n\na\n \n-\n \nb\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 2\n\n\n# Not in-place\n\n\nprint\n(\na\n.\nsub\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 3\n\n\nThis will replace a with the final result filled with 2's\n\n# Inplace\n\n\nprint\n(\na\n.\nsub_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-Wise Multiplication\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 1\n\n\na\n \n*\n \nb\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 2\n\n\n# Not in-place\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 3\n\n\n# In-place\n\n\nprint\n(\na\n.\nmul_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-Wise Division\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 1\n\n\nb\n \n/\n \na\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 2\n\n\ntorch\n.\ndiv\n(\nb\n,\n \na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 3\n\n\n# Inplace\n\n\nb\n.\ndiv_\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nTensor Mean\n\u00b6\n\n\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n\n\n\n\n mean = 55 /10 = 5.5 \n\n\n mean = 55 /10 = 5.5 \n\n\n\n\n\n\nCreate tensor of size 10 filled from 1 to 10\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nsize\n()\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n10\n])\n\n\n\n\n\n\n\nGet tensor mean\n\n\nHere we get 5.5 as we've calculated manually above.\n\n\na\n.\nmean\n(\ndim\n=\n0\n)\n\n\n\n\n\n\n\n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.\n\n\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n7\n-\n81\naec0cf1c00\n>\n \nin\n \n<\nmodule\n>\n()\n\n\n---->\n \n1\n \na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\nRuntimeError\n:\n \ndimension\n \nout\n \nof\n \nrange\n \n(\nexpected\n \nto\n \nbe\n \nin\n \nrange\n \nof\n \n[\n-\n1\n,\n \n0\n],\n \nbut\n \ngot\n \n1\n)\n\n\n\n\n\n\n\nCreate a 2x10 Tensor, of 1-10 digits each\n\n\na\n \n=\n \ntorch\n.\nTensor\n([[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n],\n \n[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n]])\n\n\n\n\na\n.\nsize\n()\n\n\n\n\n\n\ntorch\n.\nSize\n([\n2\n,\n \n10\n])\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we won't get an error like previously because we've a tensor of size 2x10\n\n\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\n \n5.5000\n\n \n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx1\n]\n\n\n\n\n\nTensor Standard Deviation\n\u00b6\n\n\n\n\nGet standard deviation of tensor\n\n\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nstd\n(\ndim\n=\n0\n)\n\n\n\n\n\n \n3.0277\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Create Matrices\n\n\n Create Matrices with Default Initialization Values\n\n\n Zeros \n\n\n Ones\n\n\n\n\n\n\n Initialize Seeds for Reproducibility on GPU and CPU\n\n\n Convert Matrices: NumPy to Torch and Torch to NumPy\n\n\n Move Tensors: CPU to GPU and GPU to CPU\n\n\n Run Important Tensor Operations\n\n\n Element-wise addition, subtraction, multiplication and division\n\n\n Resize\n\n\n Calculate mean \n\n\n Calculate standard deviation",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#pytorch-fundamentals-matrices",
            "text": "",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices",
            "text": "",
            "title": "Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction",
            "text": "Basic definition: rectangular array of numbers.   Tensors (PyTorch)   Ndarrays (NumPy)   2 x 2 Matrix (R x C)     1  1      1  1     2 x 3 Matrix     1  1  1      1  1  1",
            "title": "Matrices Brief Introduction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices",
            "text": "Create list  # Creating a 2x2 array  arr   =   [[ 1 ,   2 ],   [ 3 ,   4 ]]  print ( arr )    [[ 1 ,   2 ],   [ 3 ,   4 ]]    Create numpy array via list  import   numpy   as   np   # Convert to NumPy  np . array ( arr )    array ([[ 1 ,   2 ], \n        [ 3 ,   4 ]])    Convert numpy array to PyTorch tensor  import   torch   # Convert to PyTorch Tensor  torch . Tensor ( arr )    1    2  3    4  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Creating Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values",
            "text": "Create 2x2 numpy array of 1's  np . ones (( 2 ,   2 ))    array ([[   1. ,    1. ], \n        [   1. ,    1. ]])    Create 2x2 torch tensor of 1's  torch . ones (( 2 ,   2 ))     1  1\n 1  1\n[torch.FloatTensor of size 2x2]   Create 2x2 numpy array of random numbers  np . random . rand ( 2 ,   2 )    array ([[   0.68270631 ,    0.87721678 ], \n        [   0.07420986 ,    0.79669375 ]])    Create 2x2 PyTorch tensor of random numbers  torch . rand ( 2 ,   2 )    0.3900    0.8268  0.3888    0.5914  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Create Matrices with Default Values"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility",
            "text": "Why do we need seeds?  We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.    Create seed to enable fixed numbers for random number generation   # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )    array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])    Repeat random array generation to check  If you do not set the seed, you would not get the same set of numbers like here. # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )    array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])    Create a numpy array without seed  Notice how you get different numbers compared to the first 2 tries? # No seed  np . random . rand ( 2 ,   2 )    array ([[   0.56804456 ,    0.92559664 ], \n        [   0.07103606 ,    0.0871293   ]])    Repeat numpy array generation without seed  You get the point now, you get a totally different set of numbers. # No seed  np . random . rand ( 2 ,   2 )    array ([[   0.0202184   ,    0.83261985 ], \n        [   0.77815675 ,    0.87001215 ]])    Create a PyTorch tensor with a fixed seed  # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )     Repeat creating a PyTorch fixed seed tensor  # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )    0.5488    0.5928  0.7152    0.8443  [ torch . FloatTensor   of   size   2 x2 ]    Creating a PyTorch tensor without seed  Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed  torch . rand ( 2 ,   2 )    0.6028    0.8579  0.5449    0.8473  [ torch . FloatTensor   of   size   2 x2 ]    Repeat creating a PyTorch tensor without seed  Notice how these are different numbers again? # Torch No Seed  torch . rand ( 2 ,   2 )    0.4237    0.6236  0.6459    0.3844  [ torch . FloatTensor   of   size   2 x2 ]   Seed for GPU is different for now...   Fix a seed for GPU tensors  When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above.  if   torch . cuda . is_available (): \n     torch . cuda . manual_seed_all ( 0 )",
            "title": "Seeds for Reproducibility"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge",
            "text": "",
            "title": "NumPy and Torch Bridge"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch",
            "text": "Create a numpy array of 1's  # Numpy array  np_array   =   np . ones (( 2 ,   2 ))   print ( np_array )    [[   1.    1. ]  [   1.    1. ]]    Get the type of class for the numpy array  print ( type ( np_array ))    < class   ' numpy . ndarray '>    Convert numpy array to PyTorch tensor  # Convert to Torch Tensor  torch_tensor   =   torch . from_numpy ( np_array )   print ( torch_tensor )      1    1 \n  1    1  [ torch . DoubleTensor   of   size   2 x2 ]    Get type of class for PyTorch tensor  Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor ))    < class   ' torch . DoubleTensor '>    Create PyTorch tensor from a different numpy datatype  You will get an error running this code because PyTorch tensor don't support all datatype.  # Data types matter: intentional error  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  torch . from_numpy ( np_array_new )    ---------------------------------------------------------------------------  RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 57 - b8b085f9b39d >   in   < module > () \n       1   # Data types matter \n       2   np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  ---->   3   torch . from_numpy ( np_array_new )  RuntimeError :   can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.    What conversion support does Numpy to PyTorch tensor bridge gives?   double  float    int64 ,  int32 ,  uint8       Create PyTorch long tensor  See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int64 )  torch . from_numpy ( np_array_new )    1  1\n1  1\n[torch.LongTensor of size 2x2]   Create PyTorch int tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int32 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . IntTensor   of   size   2 x2 ]    Create PyTorch byte tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . uint8 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . ByteTensor   of   size   2 x2 ]    Create PyTorch Double Tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float64 )  torch . from_numpy ( np_array_new )   Alternatively you can do this too via  np.double  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . double )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . DoubleTensor   of   size   2 x2 ]    Create PyTorch Float Tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float32 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Tensor Type Bug Guide  These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!      NumPy Array Type  Torch Tensor Type      int64  LongTensor    int32  IntegerTensor    uint8  ByteTensor    float64  DoubleTensor    float32  FloatTensor    double  DoubleTensor",
            "title": "NumPy to Torch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy",
            "text": "Create PyTorch tensor of 1's  You would realize this defaults to a float tensor by default if you do this.  torch_tensor   =   torch . ones ( 2 ,   2 )   type ( torch_tensor )    torch . FloatTensor    Convert tensor to numpy  It's as simple as this.  torch_to_numpy   =   torch_tensor . numpy ()   type ( torch_to_numpy )    # Wowza, we did it.  numpy . ndarray",
            "title": "Torch to NumPy"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu",
            "text": "Move tensor to CPU and back  This by default creates a tensor on CPU. You do not need to do anything. # CPU  tensor_cpu   =   torch . ones ( 2 ,   2 )   If you would like to send a tensor to your GPU, you just need to do a simple  .cuda ()  # CPU to GPU  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  tensor_cpu . to ( device )   And if you want to move that tensor on the GPU back to the CPU, just do the following.  # GPU to CPU  tensor_cpu . cpu ()",
            "title": "Tensors on CPU vs GPU"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations",
            "text": "",
            "title": "Tensor Operations"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor",
            "text": "Creating a 2x2 tensor  a   =   torch . ones ( 2 ,   2 )  print ( a )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Getting size of tensor  print ( a . size ())    torch . Size ([ 2 ,   2 ])    Resize tensor to 4x1  a . view ( 4 )    1  1  1  1  [ torch . FloatTensor   of   size   4 ]    Get size of resized tensor  a . view ( 4 ) . size ()    torch . Size ([ 4 ])",
            "title": "Resizing Tensor"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition",
            "text": "Creating first 2x2 tensor  a   =   torch . ones ( 2 ,   2 )  print ( a )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Creating second 2x2 tensor  b   =   torch . ones ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise addition of 2 tensors  # Element-wise addition  c   =   a   +   b  print ( c )      2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]    Alternative element-wise addition of 2 tensors  # Element-wise addition  c   =   torch . add ( a ,   b )  print ( c )      2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]    In-place element-wise addition  This would replace the c tensor values with the new addition.   # In-place addition  print ( 'Old c tensor' )  print ( c )  c . add_ ( a )  print ( '-' * 60 )  print ( 'New c tensor' )  print ( c )    Old   c   tensor \n\n  2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]  ------------------------------------------------------------  New   c   tensor \n\n  3    3 \n  3    3  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Addition"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction",
            "text": "Check values of tensor a and b'  Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above.  print ( a )  print ( b )      1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ] \n\n\n  1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 1  a   -   b    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 2  # Not in-place  print ( a . sub ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 3  This will replace a with the final result filled with 2's # Inplace  print ( a . sub_ ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Subtraction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 1  a   *   b    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 2  # Not in-place  print ( torch . mul ( a ,   b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 3  # In-place  print ( a . mul_ ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Multiplication"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 1  b   /   a    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 2  torch . div ( b ,   a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 3  # Inplace  b . div_ ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Division"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean",
            "text": "1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55  1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55     mean = 55 /10 = 5.5    mean = 55 /10 = 5.5     Create tensor of size 10 filled from 1 to 10  a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . size ()    torch . Size ([ 10 ])    Get tensor mean  Here we get 5.5 as we've calculated manually above.  a . mean ( dim = 0 )    5.5000  [ torch . FloatTensor   of   size   1 ]    Get tensor mean on second dimension  Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.  a . mean ( dim = 1 )    RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 7 - 81 aec0cf1c00 >   in   < module > ()  ---->   1   a . mean ( dim = 1 )  RuntimeError :   dimension   out   of   range   ( expected   to   be   in   range   of   [ - 1 ,   0 ],   but   got   1 )    Create a 2x10 Tensor, of 1-10 digits each  a   =   torch . Tensor ([[ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ],   [ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ]])   a . size ()    torch . Size ([ 2 ,   10 ])    Get tensor mean on second dimension  Here we won't get an error like previously because we've a tensor of size 2x10  a . mean ( dim = 1 )      5.5000 \n  5.5000  [ torch . FloatTensor   of   size   2 x1 ]",
            "title": "Tensor Mean"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation",
            "text": "Get standard deviation of tensor   a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . std ( dim = 0 )     3.0277  [ torch . FloatTensor   of   size   1 ]",
            "title": "Tensor Standard Deviation"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#summary",
            "text": "We've learnt to...   Success    Create Matrices   Create Matrices with Default Initialization Values   Zeros    Ones     Initialize Seeds for Reproducibility on GPU and CPU   Convert Matrices: NumPy to Torch and Torch to NumPy   Move Tensors: CPU to GPU and GPU to CPU   Run Important Tensor Operations   Element-wise addition, subtraction, multiplication and division   Resize   Calculate mean    Calculate standard deviation",
            "title": "Summary"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/",
            "text": "PyTorch Fundamentals - Gradients\n\u00b6\n\n\nTensors with Gradients\n\u00b6\n\n\nCreating Tensors with Gradients\n\u00b6\n\n\n\n\nAllows accumulation of gradients\n\n\n\n\n\n\nMethod 1: Create tensor with gradients\n\n\nIt is very similar to creating a tensor, all you need to do is to add an additional argument.\n\n\nimport\n \ntorch\n\n\n\n\n\na\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n),\n \nrequires_grad\n=\nTrue\n)\n\n\na\n\n\n\n\n\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\nCheck if tensor requires gradients\n\n\nThis should return True otherwise you've not done it right.\n\na\n.\nrequires_grad\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nMethod 2: Create tensor with gradients\n\n\nThis allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.\n\n\n# Normal way of creating gradients\n\n\na\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n# Requires gradient\n\n\na\n.\nrequires_grad_\n()\n\n\n\n# Check if requires gradient\n\n\na\n.\nrequires_grad\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nA tensor without gradients just for comparison\n\n\nIf you do not do either of the methods above, you'll realize you will get False for checking for gradients.\n\n# Not a variable\n\n\nno_gradient\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\nno_gradient\n.\nrequires_grad\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\nTensor with gradients addition operation\n\n\n# Behaves similarly to tensors\n\n\nb\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n),\n \nrequires_grad\n=\nTrue\n)\n\n\nprint\n(\na\n \n+\n \nb\n)\n\n\nprint\n(\ntorch\n.\nadd\n(\na\n,\n \nb\n))\n\n\n\n\n\n\n\ntensor\n([[\n \n2.\n,\n  \n2.\n],\n\n        \n[\n \n2.\n,\n  \n2.\n]])\n\n\n\ntensor\n([[\n \n2.\n,\n  \n2.\n],\n\n        \n[\n \n2.\n,\n  \n2.\n]])\n\n\n\n\n\n\n\nTensor with gradients multiplication operation\n\n\nAs usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation!\n\nprint\n(\na\n \n*\n \nb\n)\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\n\n\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\nManually and Automatically Calculating Gradients\n\u00b6\n\n\nWhat exactly is \nrequires_grad\n?\n\n- Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation\n\n\n\n\ny_i = 5(x_i+1)^2\n\n\ny_i = 5(x_i+1)^2\n\n\n\n\n\n\nCreate tensor of size 2x1 filled with 1's that requires gradient\n\n\nx\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \nrequires_grad\n=\nTrue\n)\n\n\nx\n\n\n\n\n\n\n\ntensor\n([\n \n1.\n,\n  \n1.\n])\n\n\n\n\n\n\n\nSimple linear equation with x tensor created\n\n\n\n\ny_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\n\n\ny_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\n\n\n\n\nWe should get a value of 20 by replicating this simple equation \n\n\ny\n \n=\n \n5\n \n*\n \n(\nx\n \n+\n \n1\n)\n \n**\n \n2\n\n\ny\n\n\n\n\n\n\n\ntensor\n([\n \n20.\n,\n  \n20.\n])\n\n\n\n\n\n\n\nSimple equation with y tensor\n\n\nBackward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable\n\n\nLet's reduce y to a scalar then...\n\n\n\n\no = \\frac{1}{2}\\sum_i y_i\n\n\no = \\frac{1}{2}\\sum_i y_i\n\n\n\n\nAs you can see above, we've a tensor filled with 20's, so average them would return 20\n\n\no\n \n=\n \n(\n1\n/\n2\n)\n \n*\n \ntorch\n.\nsum\n(\ny\n)\n\n\no\n\n\n\n\n\n\n\ntensor\n(\n20.\n)\n\n\n\n\n\n\n\nCalculating first derivative\n\n\n \nRecap \ny\n equation\n: \ny_i = 5(x_i+1)^2\ny_i = 5(x_i+1)^2\n \n\n\n \nRecap \no\n equation\n: \no = \\frac{1}{2}\\sum_i y_i\no = \\frac{1}{2}\\sum_i y_i\n \n\n\n \nSubstitute \ny\n into \no\n equation\n: \no = \\frac{1}{2} \\sum_i 5(x_i+1)^2\no = \\frac{1}{2} \\sum_i 5(x_i+1)^2\n \n\n\n\n\n\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\n\n\n\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\n\n\n\n\n\n\n\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\n\n\n\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\n\n\n\n\nWe should expect to get 10, and it's so simple to do this with PyTorch with the following line...\n\n\nGet first derivative:\n\no\n.\nbackward\n()\n\n\n\n\nPrint out first derivative:\n\n\nx\n.\ngrad\n\n\n\n\n\n\n\ntensor\n([\n \n10.\n,\n  \n10.\n])\n\n\n\n\n\n\n\nIf x requires gradient and you create new objects with it, you get all gradients\n\n\nprint\n(\nx\n.\nrequires_grad\n)\n\n\nprint\n(\ny\n.\nrequires_grad\n)\n\n\nprint\n(\no\n.\nrequires_grad\n)\n\n\n\n\n\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Tensor with Gradients\n\n\n Wraps a tensor for gradient accumulation\n\n\n\n\n\n\n Gradients\n\n\n Define original equation\n\n\n Substitute equation with \nx\n values\n\n\n Reduce to scalar output, \no\n through \nmean\n\n\n Calculate gradients with \no.backward()\n\n\n Then access gradients of the \nx\n tensor with \nrequires_grad\n through \nx.grad",
            "title": "PyTorch Fundamentals - Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#pytorch-fundamentals-gradients",
            "text": "",
            "title": "PyTorch Fundamentals - Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients",
            "text": "",
            "title": "Tensors with Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients",
            "text": "Allows accumulation of gradients    Method 1: Create tensor with gradients  It is very similar to creating a tensor, all you need to do is to add an additional argument.  import   torch   a   =   torch . ones (( 2 ,   2 ),   requires_grad = True )  a    tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])    Check if tensor requires gradients  This should return True otherwise you've not done it right. a . requires_grad    True    Method 2: Create tensor with gradients  This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.  # Normal way of creating gradients  a   =   torch . ones (( 2 ,   2 ))  # Requires gradient  a . requires_grad_ ()  # Check if requires gradient  a . requires_grad    True    A tensor without gradients just for comparison  If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable  no_gradient   =   torch . ones ( 2 ,   2 )   no_gradient . requires_grad    False    Tensor with gradients addition operation  # Behaves similarly to tensors  b   =   torch . ones (( 2 ,   2 ),   requires_grad = True )  print ( a   +   b )  print ( torch . add ( a ,   b ))    tensor ([[   2. ,    2. ], \n         [   2. ,    2. ]])  tensor ([[   2. ,    2. ], \n         [   2. ,    2. ]])    Tensor with gradients multiplication operation  As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a   *   b )  print ( torch . mul ( a ,   b ))    tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])  tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])",
            "title": "Creating Tensors with Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients",
            "text": "What exactly is  requires_grad ? \n- Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation   y_i = 5(x_i+1)^2  y_i = 5(x_i+1)^2    Create tensor of size 2x1 filled with 1's that requires gradient  x   =   torch . ones ( 2 ,   requires_grad = True )  x    tensor ([   1. ,    1. ])    Simple linear equation with x tensor created   y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20  y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20   We should get a value of 20 by replicating this simple equation   y   =   5   *   ( x   +   1 )   **   2  y    tensor ([   20. ,    20. ])    Simple equation with y tensor  Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable  Let's reduce y to a scalar then...   o = \\frac{1}{2}\\sum_i y_i  o = \\frac{1}{2}\\sum_i y_i   As you can see above, we've a tensor filled with 20's, so average them would return 20  o   =   ( 1 / 2 )   *   torch . sum ( y )  o    tensor ( 20. )    Calculating first derivative    Recap  y  equation :  y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2      Recap  o  equation :  o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i      Substitute  y  into  o  equation :  o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2     \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]  \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]    \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10  \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10   We should expect to get 10, and it's so simple to do this with PyTorch with the following line...  Get first derivative: o . backward ()   Print out first derivative:  x . grad    tensor ([   10. ,    10. ])    If x requires gradient and you create new objects with it, you get all gradients  print ( x . requires_grad )  print ( y . requires_grad )  print ( o . requires_grad )    True  True  True",
            "title": "Manually and Automatically Calculating Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#summary",
            "text": "We've learnt to...   Success    Tensor with Gradients   Wraps a tensor for gradient accumulation     Gradients   Define original equation   Substitute equation with  x  values   Reduce to scalar output,  o  through  mean   Calculate gradients with  o.backward()   Then access gradients of the  x  tensor with  requires_grad  through  x.grad",
            "title": "Summary"
        },
        {
            "location": "/news/news/",
            "text": "Welcome to our Blog\n\u00b6\n\n\nHere, we post news related to Deep Learning Wizard's releases, features and achievements \n\n\nNotable News\n\u00b6\n\n\n\n\n Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, July 2018\n\n\n Reached 2200+ students, 2018\n\n\n Featured on PyTorch Website, January 2018\n\n\n Reached 1000+ students, 2017\n\n\n Hosted NVIDIA Self-Driving Cars and Healthcare Talk, June 2017\n\n\n And more...",
            "title": "Welcome"
        },
        {
            "location": "/news/news/#welcome-to-our-blog",
            "text": "Here, we post news related to Deep Learning Wizard's releases, features and achievements",
            "title": "Welcome to our Blog"
        },
        {
            "location": "/news/news/#notable-news",
            "text": "Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, July 2018   Reached 2200+ students, 2018   Featured on PyTorch Website, January 2018   Reached 1000+ students, 2017   Hosted NVIDIA Self-Driving Cars and Healthcare Talk, June 2017   And more...",
            "title": "Notable News"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/",
            "text": "NVIDIA Workshop at NUS-MIT-NUHS Datathon\n\u00b6\n\n\nImage Recognition Workshop by Ritchie Ng\n\u00b6\n\n\nThe NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.\n\n\nIn \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.\n\n\nLink to the NUS-MIT-NUHS Datathon \nworkshop\n.",
            "title": "NUS-MIT-NUHS NVIDIA Image Recognition Workshop July 2018"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/#nvidia-workshop-at-nus-mit-nuhs-datathon",
            "text": "",
            "title": "NVIDIA Workshop at NUS-MIT-NUHS Datathon"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng",
            "text": "The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.  In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.  Link to the NUS-MIT-NUHS Datathon  workshop .",
            "title": "Image Recognition Workshop by Ritchie Ng"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/",
            "text": "Featured on PyTorch Website\n\u00b6\n\n\nPyTorch a Year Later\n\u00b6\n\n\nWe are featured on \nPyTorch website's post\n \n\n\nI used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.\n\n\nA year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!\n\n\nA big shoutout for \nAlfredo Canziani\n who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.\n\n\nTo more great years ahead for PyTorch \n\n\nCheers,\n\nRitchie Ng",
            "title": "Featured on PyTorch Website 2018"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website",
            "text": "",
            "title": "Featured on PyTorch Website"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later",
            "text": "We are featured on  PyTorch website's post    I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.  A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!  A big shoutout for  Alfredo Canziani  who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.  To more great years ahead for PyTorch   Cheers, Ritchie Ng",
            "title": "PyTorch a Year Later"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/",
            "text": "NVIDIA Self-Driving Cars and Healthcare Workshop\n\u00b6\n\n\nHosted by Ritchie Ng\n\u00b6\n\n\nA talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.\n\n\nWe will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.\n\n\nDetails:\nWednesday, June 21\nst\n\n1:00 PM to 3:30 PM\n\n\nThe Hangar by NUS Enterprise\n21 Heng Mui Keng Terrace, Singapore 119613",
            "title": "NVIDIA Self Driving Cars & Healthcare Talk June 2017"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/#nvidia-self-driving-cars-and-healthcare-workshop",
            "text": "",
            "title": "NVIDIA Self-Driving Cars and Healthcare Workshop"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng",
            "text": "A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.  We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.  Details:\nWednesday, June 21 st \n1:00 PM to 3:30 PM  The Hangar by NUS Enterprise\n21 Heng Mui Keng Terrace, Singapore 119613",
            "title": "Hosted by Ritchie Ng"
        }
    ]
}