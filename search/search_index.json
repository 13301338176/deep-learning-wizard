{
    "docs": [
        {
            "location": "/",
            "text": "About Us\n\u00b6\n\n\nWe deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our \nvideo course\n and join over 2300 deep learning wizards. \n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nPyTorch as our Preferred Deep Learning Library\n\u00b6\n\n\nWe chose \nPyTorch\n because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.\n\n\n# It is this easy! \n\n\nimport\n \ntorch\n\n\n\n# Create a variable of value 1 each.\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\nb\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\n\n# Add the 2 variables to give you 2, it's that simple!\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\n\n\n\nMade for Visual and Book Lovers\n\u00b6\n\n\nWe are visual creatures, that is why we offer detailed \nvideo courses\n on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. \n\n\nFor book lovers, you will be happy to know \nDeep Learning Wizard's wikipedia\n will always be updated first prior to our release of video courses.\n\n\nExperienced Research and Applied Team\n\u00b6\n\n\n\n\nRitchie Ng\n\n\nCurrently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.\n\n\nMy passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.\n\n\nI was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.\n\n\nCheck out my profile link at \nritchieng.com\n\n\n\n\n\n\nJie Fu\n\n\nI am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.\n\n\nI earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.\n\n\nI am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.\n\n\nCheck out my profile link at \nbigaidream.github.io",
            "title": "Home"
        },
        {
            "location": "/#about-us",
            "text": "We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our  video course  and join over 2300 deep learning wizards.   To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.",
            "title": "About Us"
        },
        {
            "location": "/#pytorch-as-our-preferred-deep-learning-library",
            "text": "We chose  PyTorch  because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.  # It is this easy!   import   torch  # Create a variable of value 1 each.  a   =   torch . Tensor ([ 1 ])  b   =   torch . Tensor ([ 1 ])  # Add the 2 variables to give you 2, it's that simple!  c   =   a   +   b",
            "title": "PyTorch as our Preferred Deep Learning Library"
        },
        {
            "location": "/#made-for-visual-and-book-lovers",
            "text": "We are visual creatures, that is why we offer detailed  video courses  on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch.   For book lovers, you will be happy to know  Deep Learning Wizard's wikipedia  will always be updated first prior to our release of video courses.",
            "title": "Made for Visual and Book Lovers"
        },
        {
            "location": "/#experienced-research-and-applied-team",
            "text": "Ritchie Ng  Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.  My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.  I was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.  Check out my profile link at  ritchieng.com    Jie Fu  I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.  I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.  I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.  Check out my profile link at  bigaidream.github.io",
            "title": "Experienced Research and Applied Team"
        },
        {
            "location": "/supporters/",
            "text": "Supporters\n\u00b6\n\n\nMore than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.\n\n\nIndividuals\n\u00b6\n\n\n\n\nAlfredo Canziani\n\n\nAlfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.\n\n\nHe is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun.\n\n\nDo check out his latest \nmini-course on PyTorch\n that was held in Princeton University.\n\n\n\n\n\n\nMarek Bardonski\n\n\nSince graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.\n\n\nNASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.\n\n\nSince his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at \nSigmoidal\n.\n\n\n\n\nCorporations\n\u00b6\n\n\n\n\n NVIDIA (NVIDIA Inception Partner)\n\n\n Facebook\n\n\n Amazon\n\n\n\n\nResearch Institutions\n\u00b6\n\n\n\n\n Montreal Institute of Learning Algorithms (MILA), Montreal, Canada\n\n\n Imperial College London, UK\n\n\n Massachusetts Institute of Technology (MIT), USA  \n\n\n National University of Singapore (NUS), Singapore\n\n\n Nanyang Technological University (NTU), Singapore\n\n\n\n\nNVIDIA Inception Partner\n\u00b6\n\n\n\n\n\"NVIDIA Inception Partner\n\n\n\n\nDeep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#supporters",
            "text": "More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#individuals",
            "text": "Alfredo Canziani  Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.  He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun.  Do check out his latest  mini-course on PyTorch  that was held in Princeton University.    Marek Bardonski  Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.  NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.  Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at  Sigmoidal .",
            "title": "Individuals"
        },
        {
            "location": "/supporters/#corporations",
            "text": "NVIDIA (NVIDIA Inception Partner)   Facebook   Amazon",
            "title": "Corporations"
        },
        {
            "location": "/supporters/#research-institutions",
            "text": "Montreal Institute of Learning Algorithms (MILA), Montreal, Canada   Imperial College London, UK   Massachusetts Institute of Technology (MIT), USA     National University of Singapore (NUS), Singapore   Nanyang Technological University (NTU), Singapore",
            "title": "Research Institutions"
        },
        {
            "location": "/supporters/#nvidia-inception-partner",
            "text": "\"NVIDIA Inception Partner   Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "NVIDIA Inception Partner"
        },
        {
            "location": "/review/",
            "text": "Reviews\n\u00b6\n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nThese are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.\n\n\n\n\nRoberto Trevi\u00f1o Cervantes\n\n\nCongratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.\n\n\n\n\n\n\nMuktabh Mayank\n\n\nThis course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.\n\n\n\n\n\n\nCharles Neiswender\n\n\nI really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.\n\n\n\n\n\n\nIan Lipton\n\n\nThis was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.\n\n\n\n\nAnd check out hundreds of more reviews for our \nvideo course\n!",
            "title": "Reviews"
        },
        {
            "location": "/review/#reviews",
            "text": "To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.  These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.   Roberto Trevi\u00f1o Cervantes  Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.    Muktabh Mayank  This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.    Charles Neiswender  I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.    Ian Lipton  This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.   And check out hundreds of more reviews for our  video course !",
            "title": "Reviews"
        },
        {
            "location": "/deep_learning/intro/",
            "text": "Deep Learning Theory and Programming Tutorials\n\u00b6\n\n\nOur main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our \nvideo course\n.\n\n\n\n\nWork-in-progress\n\n\nThis open-source portion is still a work in progress, it is very sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact \nRitchie Ng\n if you would like to contribute via our \nFacebook\n page.\n\n\nAlso take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our \nvideo course\n that provides a much easier experience.\n\n\nAll of our code allows you to run in a notebook for this deep learning section. Please use a \njupyter notebook\n and run the examples from the start of the page to the end.",
            "title": "Introduction"
        },
        {
            "location": "/deep_learning/intro/#deep-learning-theory-and-programming-tutorials",
            "text": "Our main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our  video course .   Work-in-progress  This open-source portion is still a work in progress, it is very sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact  Ritchie Ng  if you would like to contribute via our  Facebook  page.  Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our  video course  that provides a much easier experience.  All of our code allows you to run in a notebook for this deep learning section. Please use a  jupyter notebook  and run the examples from the start of the page to the end.",
            "title": "Deep Learning Theory and Programming Tutorials"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/",
            "text": "PyTorch Fundamentals - Matrices\n\u00b6\n\n\nMatrices\n\u00b6\n\n\nMatrices Brief Introduction\n\u00b6\n\n\n\n\n Basic definition: rectangular array of numbers.\n\n\n Tensors (PyTorch)\n\n\n Ndarrays (NumPy)\n\n\n\n\n2 x 2 Matrix (R x C)\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n2 x 3 Matrix\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nCreating Matrices\n\u00b6\n\n\n\n\nCreate list\n\n\n# Creating a 2x2 array\n\n\narr\n \n=\n \n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\nprint\n(\narr\n)\n\n\n\n\n\n\n\n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\n\n\n\n\n\nCreate numpy array via list\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n\n# Convert to NumPy\n\n\nnp\n.\narray\n(\narr\n)\n\n\n\n\n\n\narray\n([[\n1\n,\n \n2\n],\n\n       \n[\n3\n,\n \n4\n]])\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\nimport\n \ntorch\n\n\n\n\n\n# Convert to PyTorch Tensor\n\n\ntorch\n.\nTensor\n(\narr\n)\n\n\n\n\n\n\n\n1\n  \n2\n\n\n3\n  \n4\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nCreate Matrices with Default Values\n\u00b6\n\n\n\n\nCreate 2x2 numpy array of 1's\n\n\nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\narray\n([[\n \n1.\n,\n  \n1.\n],\n\n       \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\nCreate 2x2 torch tensor of 1's\n\n\ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\n\n\n\n\n\nCreate 2x2 numpy array of random numbers\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\narray\n([[\n \n0.68270631\n,\n  \n0.87721678\n],\n\n       \n[\n \n0.07420986\n,\n  \n0.79669375\n]])\n\n\n\n\n\n\n\nCreate 2x2 PyTorch tensor of random numbers\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n0.3900\n  \n0.8268\n\n\n0.3888\n  \n0.5914\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nSeeds for Reproducibility\n\u00b6\n\n\n\n\nWhy do we need seeds?\n\n\nWe need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.\n\n\n\n\n\n\nCreate seed to enable fixed numbers for random number generation \n\n\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\nRepeat random array generation to check\n\n\nIf you do not set the seed, you would not get the same set of numbers like here.\n\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\nCreate a numpy array without seed\n\n\nNotice how you get different numbers compared to the first 2 tries?\n\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.56804456\n,\n  \n0.92559664\n],\n\n       \n[\n \n0.07103606\n,\n  \n0.0871293\n \n]])\n\n\n\n\n\n\n\nRepeat numpy array generation without seed\n\n\nYou get the point now, you get a totally different set of numbers.\n\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.0202184\n \n,\n  \n0.83261985\n],\n\n       \n[\n \n0.77815675\n,\n  \n0.87001215\n]])\n\n\n\n\n\n\n\nCreate a PyTorch tensor with a fixed seed\n\n\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\nRepeat creating a PyTorch fixed seed tensor\n\n\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n0.5488\n  \n0.5928\n\n\n0.7152\n  \n0.8443\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreating a PyTorch tensor without seed\n\n\nLike with a numpy array of random numbers without seed, you will not get the same results as above.\n\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n0.6028\n  \n0.8579\n\n\n0.5449\n  \n0.8473\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nRepeat creating a PyTorch tensor without seed\n\n\nNotice how these are different numbers again?\n\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n0.4237\n  \n0.6236\n\n\n0.6459\n  \n0.3844\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nSeed for GPU is different for now...\n\n\n\n\nFix a seed for GPU tensors\n\n\nWhen you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. \n\nif\n \ntorch\n.\ncuda\n.\nis_available\n():\n\n    \ntorch\n.\ncuda\n.\nmanual_seed_all\n(\n0\n)\n\n\n\n\n\n\nNumPy and Torch Bridge\n\u00b6\n\n\nNumPy to Torch\n\u00b6\n\n\n\n\nCreate a numpy array of 1's\n\n\n# Numpy array\n\n\nnp_array\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\nprint\n(\nnp_array\n)\n\n\n\n\n\n\n\n[[\n \n1.\n  \n1.\n]\n\n\n[\n \n1.\n  \n1.\n]]\n\n\n\n\n\n\n\nGet the type of class for the numpy array\n\n\nprint\n(\ntype\n(\nnp_array\n))\n\n\n\n\n\n\n\n<\nclass\n \n'\nnumpy\n.\nndarray\n'>\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\n# Convert to Torch Tensor\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nfrom_numpy\n(\nnp_array\n)\n\n\n\n\n\nprint\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nGet type of class for PyTorch tensor\n\n\nNotice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type.\n\nprint\n(\ntype\n(\ntorch_tensor\n))\n\n\n\n\n\n\n<\nclass\n \n'\ntorch\n.\nDoubleTensor\n'>\n\n\n\n\n\n\n\nCreate PyTorch tensor from a different numpy datatype\n\n\nYou will get an error running this code because PyTorch tensor don't support all datatype. \n\n# Data types matter: intentional error\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n---------------------------------------------------------------------------\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n57\n-\nb8b085f9b39d\n>\n \nin\n \n<\nmodule\n>\n()\n\n      \n1\n \n# Data types matter\n\n      \n2\n \nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\n---->\n \n3\n \ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\nRuntimeError\n:\n \ncan\n't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\n\n\n\n\n\n\n\nWhat conversion support does Numpy to PyTorch tensor bridge gives?\n\n\n\n\ndouble\n\n\nfloat\n \n\n\nint64\n, \nint32\n, \nuint8\n \n\n\n\n\n\n\n\n\nCreate PyTorch long tensor\n\n\nSee how a int64 numpy array gives you a PyTorch long tensor?\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n1  1\n1  1\n[torch.LongTensor of size 2x2]\n\n\n\n\n\n\nCreate PyTorch int tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nIntTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch byte tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nuint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nByteTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch Double Tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\nAlternatively you can do this too via \nnp.double\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\ndouble\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch Float Tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nTensor Type Bug Guide\n\n\nThese things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!\n\n\n\n\n\n\n\n\n\n\nNumPy Array Type\n\n\nTorch Tensor Type\n\n\n\n\n\n\n\n\n\n\nint64\n\n\nLongTensor\n\n\n\n\n\n\nint32\n\n\nIntegerTensor\n\n\n\n\n\n\nuint8\n\n\nByteTensor\n\n\n\n\n\n\nfloat64\n\n\nDoubleTensor\n\n\n\n\n\n\nfloat32\n\n\nFloatTensor\n\n\n\n\n\n\ndouble\n\n\nDoubleTensor\n\n\n\n\n\n\n\n\nTorch to NumPy\n\u00b6\n\n\n\n\nCreate PyTorch tensor of 1's\n\n\nYou would realize this defaults to a float tensor by default if you do this.\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\n\ntype\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\ntorch\n.\nFloatTensor\n\n\n\n\n\n\n\nConvert tensor to numpy\n\n\nIt's as simple as this.\n\n\ntorch_to_numpy\n \n=\n \ntorch_tensor\n.\nnumpy\n()\n\n\n\n\n\ntype\n(\ntorch_to_numpy\n)\n\n\n\n\n\n\n\n# Wowza, we did it.\n\n\nnumpy\n.\nndarray\n\n\n\n\n\nTensors on CPU vs GPU\n\u00b6\n\n\n\n\nMove tensor to CPU and back\n\n\nThis by default creates a tensor on CPU. You do not need to do anything.\n\n# CPU\n\n\ntensor_cpu\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\nIf you would like to send a tensor to your GPU, you just need to do a simple \n.cuda\n()\n\n\n# CPU to GPU\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\ntensor_cpu\n.\nto\n(\ndevice\n)\n\n\n\n\n\nAnd if you want to move that tensor on the GPU back to the CPU, just do the following.\n\n\n# GPU to CPU\n\n\ntensor_cpu\n.\ncpu\n()\n\n\n\n\n\n\n\nTensor Operations\n\u00b6\n\n\nResizing Tensor\n\u00b6\n\n\n\n\nCreating a 2x2 tensor\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nGetting size of tensor\n\n\nprint\n(\na\n.\nsize\n())\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n2\n,\n \n2\n])\n\n\n\n\n\n\n\nResize tensor to 4x1\n\n\na\n.\nview\n(\n4\n)\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n4\n]\n\n\n\n\n\n\n\nGet size of resized tensor\n\n\na\n.\nview\n(\n4\n)\n.\nsize\n()\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n4\n])\n\n\n\n\n\nElement-wise Addition\n\u00b6\n\n\n\n\nCreating first 2x2 tensor\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreating second 2x2 tensor\n\n\nb\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise addition of 2 tensors\n\n\n# Element-wise addition\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nAlternative element-wise addition of 2 tensors\n\n\n# Element-wise addition\n\n\nc\n \n=\n \ntorch\n.\nadd\n(\na\n,\n \nb\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nIn-place element-wise addition\n\n\nThis would replace the c tensor values with the new addition. \n\n\n# In-place addition\n\n\nprint\n(\n'Old c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\nc\n.\nadd_\n(\na\n)\n\n\n\nprint\n(\n'-'\n*\n60\n)\n\n\nprint\n(\n'New c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\nOld\n \nc\n \ntensor\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n------------------------------------------------------------\n\n\nNew\n \nc\n \ntensor\n\n\n \n3\n  \n3\n\n \n3\n  \n3\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-wise Subtraction\n\u00b6\n\n\n\n\nCheck values of tensor a and b'\n\n\nTake note that you've created tensor a and b of sizes 2x2 filled with 1's each above. \n\nprint\n(\na\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 1\n\n\na\n \n-\n \nb\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 2\n\n\n# Not in-place\n\n\nprint\n(\na\n.\nsub\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 3\n\n\nThis will replace a with the final result filled with 2's\n\n# Inplace\n\n\nprint\n(\na\n.\nsub_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-Wise Multiplication\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 1\n\n\na\n \n*\n \nb\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 2\n\n\n# Not in-place\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 3\n\n\n# In-place\n\n\nprint\n(\na\n.\nmul_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-Wise Division\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 1\n\n\nb\n \n/\n \na\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 2\n\n\ntorch\n.\ndiv\n(\nb\n,\n \na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 3\n\n\n# Inplace\n\n\nb\n.\ndiv_\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nTensor Mean\n\u00b6\n\n\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n\n\n\n\n mean = 55 /10 = 5.5 \n\n\n mean = 55 /10 = 5.5 \n\n\n\n\n\n\nCreate tensor of size 10 filled from 1 to 10\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nsize\n()\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n10\n])\n\n\n\n\n\n\n\nGet tensor mean\n\n\nHere we get 5.5 as we've calculated manually above.\n\n\na\n.\nmean\n(\ndim\n=\n0\n)\n\n\n\n\n\n\n\n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.\n\n\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n7\n-\n81\naec0cf1c00\n>\n \nin\n \n<\nmodule\n>\n()\n\n\n---->\n \n1\n \na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\nRuntimeError\n:\n \ndimension\n \nout\n \nof\n \nrange\n \n(\nexpected\n \nto\n \nbe\n \nin\n \nrange\n \nof\n \n[\n-\n1\n,\n \n0\n],\n \nbut\n \ngot\n \n1\n)\n\n\n\n\n\n\n\nCreate a 2x10 Tensor, of 1-10 digits each\n\n\na\n \n=\n \ntorch\n.\nTensor\n([[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n],\n \n[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n]])\n\n\n\n\na\n.\nsize\n()\n\n\n\n\n\n\ntorch\n.\nSize\n([\n2\n,\n \n10\n])\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we won't get an error like previously because we've a tensor of size 2x10\n\n\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\n \n5.5000\n\n \n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx1\n]\n\n\n\n\n\nTensor Standard Deviation\n\u00b6\n\n\n\n\nGet standard deviation of tensor\n\n\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nstd\n(\ndim\n=\n0\n)\n\n\n\n\n\n \n3.0277\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Create Matrices\n\n\n Create Matrices with Default Initialization Values\n\n\n Zeros \n\n\n Ones\n\n\n\n\n\n\n Initialize Seeds for Reproducibility on GPU and CPU\n\n\n Convert Matrices: NumPy to Torch and Torch to NumPy\n\n\n Move Tensors: CPU to GPU and GPU to CPU\n\n\n Run Important Tensor Operations\n\n\n Element-wise addition, subtraction, multiplication and division\n\n\n Resize\n\n\n Calculate mean \n\n\n Calculate standard deviation",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#pytorch-fundamentals-matrices",
            "text": "",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices",
            "text": "",
            "title": "Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction",
            "text": "Basic definition: rectangular array of numbers.   Tensors (PyTorch)   Ndarrays (NumPy)   2 x 2 Matrix (R x C)     1  1      1  1     2 x 3 Matrix     1  1  1      1  1  1",
            "title": "Matrices Brief Introduction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices",
            "text": "Create list  # Creating a 2x2 array  arr   =   [[ 1 ,   2 ],   [ 3 ,   4 ]]  print ( arr )    [[ 1 ,   2 ],   [ 3 ,   4 ]]    Create numpy array via list  import   numpy   as   np   # Convert to NumPy  np . array ( arr )    array ([[ 1 ,   2 ], \n        [ 3 ,   4 ]])    Convert numpy array to PyTorch tensor  import   torch   # Convert to PyTorch Tensor  torch . Tensor ( arr )    1    2  3    4  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Creating Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values",
            "text": "Create 2x2 numpy array of 1's  np . ones (( 2 ,   2 ))    array ([[   1. ,    1. ], \n        [   1. ,    1. ]])    Create 2x2 torch tensor of 1's  torch . ones (( 2 ,   2 ))     1  1\n 1  1\n[torch.FloatTensor of size 2x2]   Create 2x2 numpy array of random numbers  np . random . rand ( 2 ,   2 )    array ([[   0.68270631 ,    0.87721678 ], \n        [   0.07420986 ,    0.79669375 ]])    Create 2x2 PyTorch tensor of random numbers  torch . rand ( 2 ,   2 )    0.3900    0.8268  0.3888    0.5914  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Create Matrices with Default Values"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility",
            "text": "Why do we need seeds?  We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.    Create seed to enable fixed numbers for random number generation   # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )    array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])    Repeat random array generation to check  If you do not set the seed, you would not get the same set of numbers like here. # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )    array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])    Create a numpy array without seed  Notice how you get different numbers compared to the first 2 tries? # No seed  np . random . rand ( 2 ,   2 )    array ([[   0.56804456 ,    0.92559664 ], \n        [   0.07103606 ,    0.0871293   ]])    Repeat numpy array generation without seed  You get the point now, you get a totally different set of numbers. # No seed  np . random . rand ( 2 ,   2 )    array ([[   0.0202184   ,    0.83261985 ], \n        [   0.77815675 ,    0.87001215 ]])    Create a PyTorch tensor with a fixed seed  # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )     Repeat creating a PyTorch fixed seed tensor  # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )    0.5488    0.5928  0.7152    0.8443  [ torch . FloatTensor   of   size   2 x2 ]    Creating a PyTorch tensor without seed  Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed  torch . rand ( 2 ,   2 )    0.6028    0.8579  0.5449    0.8473  [ torch . FloatTensor   of   size   2 x2 ]    Repeat creating a PyTorch tensor without seed  Notice how these are different numbers again? # Torch No Seed  torch . rand ( 2 ,   2 )    0.4237    0.6236  0.6459    0.3844  [ torch . FloatTensor   of   size   2 x2 ]   Seed for GPU is different for now...   Fix a seed for GPU tensors  When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above.  if   torch . cuda . is_available (): \n     torch . cuda . manual_seed_all ( 0 )",
            "title": "Seeds for Reproducibility"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge",
            "text": "",
            "title": "NumPy and Torch Bridge"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch",
            "text": "Create a numpy array of 1's  # Numpy array  np_array   =   np . ones (( 2 ,   2 ))   print ( np_array )    [[   1.    1. ]  [   1.    1. ]]    Get the type of class for the numpy array  print ( type ( np_array ))    < class   ' numpy . ndarray '>    Convert numpy array to PyTorch tensor  # Convert to Torch Tensor  torch_tensor   =   torch . from_numpy ( np_array )   print ( torch_tensor )      1    1 \n  1    1  [ torch . DoubleTensor   of   size   2 x2 ]    Get type of class for PyTorch tensor  Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor ))    < class   ' torch . DoubleTensor '>    Create PyTorch tensor from a different numpy datatype  You will get an error running this code because PyTorch tensor don't support all datatype.  # Data types matter: intentional error  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  torch . from_numpy ( np_array_new )    ---------------------------------------------------------------------------  RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 57 - b8b085f9b39d >   in   < module > () \n       1   # Data types matter \n       2   np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  ---->   3   torch . from_numpy ( np_array_new )  RuntimeError :   can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.    What conversion support does Numpy to PyTorch tensor bridge gives?   double  float    int64 ,  int32 ,  uint8       Create PyTorch long tensor  See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int64 )  torch . from_numpy ( np_array_new )    1  1\n1  1\n[torch.LongTensor of size 2x2]   Create PyTorch int tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int32 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . IntTensor   of   size   2 x2 ]    Create PyTorch byte tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . uint8 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . ByteTensor   of   size   2 x2 ]    Create PyTorch Double Tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float64 )  torch . from_numpy ( np_array_new )   Alternatively you can do this too via  np.double  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . double )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . DoubleTensor   of   size   2 x2 ]    Create PyTorch Float Tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float32 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Tensor Type Bug Guide  These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!      NumPy Array Type  Torch Tensor Type      int64  LongTensor    int32  IntegerTensor    uint8  ByteTensor    float64  DoubleTensor    float32  FloatTensor    double  DoubleTensor",
            "title": "NumPy to Torch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy",
            "text": "Create PyTorch tensor of 1's  You would realize this defaults to a float tensor by default if you do this.  torch_tensor   =   torch . ones ( 2 ,   2 )   type ( torch_tensor )    torch . FloatTensor    Convert tensor to numpy  It's as simple as this.  torch_to_numpy   =   torch_tensor . numpy ()   type ( torch_to_numpy )    # Wowza, we did it.  numpy . ndarray",
            "title": "Torch to NumPy"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu",
            "text": "Move tensor to CPU and back  This by default creates a tensor on CPU. You do not need to do anything. # CPU  tensor_cpu   =   torch . ones ( 2 ,   2 )   If you would like to send a tensor to your GPU, you just need to do a simple  .cuda ()  # CPU to GPU  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  tensor_cpu . to ( device )   And if you want to move that tensor on the GPU back to the CPU, just do the following.  # GPU to CPU  tensor_cpu . cpu ()",
            "title": "Tensors on CPU vs GPU"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations",
            "text": "",
            "title": "Tensor Operations"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor",
            "text": "Creating a 2x2 tensor  a   =   torch . ones ( 2 ,   2 )  print ( a )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Getting size of tensor  print ( a . size ())    torch . Size ([ 2 ,   2 ])    Resize tensor to 4x1  a . view ( 4 )    1  1  1  1  [ torch . FloatTensor   of   size   4 ]    Get size of resized tensor  a . view ( 4 ) . size ()    torch . Size ([ 4 ])",
            "title": "Resizing Tensor"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition",
            "text": "Creating first 2x2 tensor  a   =   torch . ones ( 2 ,   2 )  print ( a )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Creating second 2x2 tensor  b   =   torch . ones ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise addition of 2 tensors  # Element-wise addition  c   =   a   +   b  print ( c )      2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]    Alternative element-wise addition of 2 tensors  # Element-wise addition  c   =   torch . add ( a ,   b )  print ( c )      2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]    In-place element-wise addition  This would replace the c tensor values with the new addition.   # In-place addition  print ( 'Old c tensor' )  print ( c )  c . add_ ( a )  print ( '-' * 60 )  print ( 'New c tensor' )  print ( c )    Old   c   tensor \n\n  2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]  ------------------------------------------------------------  New   c   tensor \n\n  3    3 \n  3    3  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Addition"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction",
            "text": "Check values of tensor a and b'  Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above.  print ( a )  print ( b )      1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ] \n\n\n  1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 1  a   -   b    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 2  # Not in-place  print ( a . sub ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 3  This will replace a with the final result filled with 2's # Inplace  print ( a . sub_ ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Subtraction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 1  a   *   b    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 2  # Not in-place  print ( torch . mul ( a ,   b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 3  # In-place  print ( a . mul_ ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Multiplication"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 1  b   /   a    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 2  torch . div ( b ,   a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 3  # Inplace  b . div_ ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Division"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean",
            "text": "1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55  1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55     mean = 55 /10 = 5.5    mean = 55 /10 = 5.5     Create tensor of size 10 filled from 1 to 10  a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . size ()    torch . Size ([ 10 ])    Get tensor mean  Here we get 5.5 as we've calculated manually above.  a . mean ( dim = 0 )    5.5000  [ torch . FloatTensor   of   size   1 ]    Get tensor mean on second dimension  Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.  a . mean ( dim = 1 )    RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 7 - 81 aec0cf1c00 >   in   < module > ()  ---->   1   a . mean ( dim = 1 )  RuntimeError :   dimension   out   of   range   ( expected   to   be   in   range   of   [ - 1 ,   0 ],   but   got   1 )    Create a 2x10 Tensor, of 1-10 digits each  a   =   torch . Tensor ([[ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ],   [ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ]])   a . size ()    torch . Size ([ 2 ,   10 ])    Get tensor mean on second dimension  Here we won't get an error like previously because we've a tensor of size 2x10  a . mean ( dim = 1 )      5.5000 \n  5.5000  [ torch . FloatTensor   of   size   2 x1 ]",
            "title": "Tensor Mean"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation",
            "text": "Get standard deviation of tensor   a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . std ( dim = 0 )     3.0277  [ torch . FloatTensor   of   size   1 ]",
            "title": "Tensor Standard Deviation"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#summary",
            "text": "We've learnt to...   Success    Create Matrices   Create Matrices with Default Initialization Values   Zeros    Ones     Initialize Seeds for Reproducibility on GPU and CPU   Convert Matrices: NumPy to Torch and Torch to NumPy   Move Tensors: CPU to GPU and GPU to CPU   Run Important Tensor Operations   Element-wise addition, subtraction, multiplication and division   Resize   Calculate mean    Calculate standard deviation",
            "title": "Summary"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/",
            "text": "PyTorch Fundamentals - Gradients\n\u00b6\n\n\nTensors with Gradients\n\u00b6\n\n\nCreating Tensors with Gradients\n\u00b6\n\n\n\n\nAllows accumulation of gradients\n\n\n\n\n\n\nMethod 1: Create tensor with gradients\n\n\nIt is very similar to creating a tensor, all you need to do is to add an additional argument.\n\n\nimport\n \ntorch\n\n\n\n\n\na\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n),\n \nrequires_grad\n=\nTrue\n)\n\n\na\n\n\n\n\n\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\nCheck if tensor requires gradients\n\n\nThis should return True otherwise you've not done it right.\n\na\n.\nrequires_grad\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nMethod 2: Create tensor with gradients\n\n\nThis allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.\n\n\n# Normal way of creating gradients\n\n\na\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n# Requires gradient\n\n\na\n.\nrequires_grad_\n()\n\n\n\n# Check if requires gradient\n\n\na\n.\nrequires_grad\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nA tensor without gradients just for comparison\n\n\nIf you do not do either of the methods above, you'll realize you will get False for checking for gradients.\n\n# Not a variable\n\n\nno_gradient\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\nno_gradient\n.\nrequires_grad\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\nTensor with gradients addition operation\n\n\n# Behaves similarly to tensors\n\n\nb\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n),\n \nrequires_grad\n=\nTrue\n)\n\n\nprint\n(\na\n \n+\n \nb\n)\n\n\nprint\n(\ntorch\n.\nadd\n(\na\n,\n \nb\n))\n\n\n\n\n\n\n\ntensor\n([[\n \n2.\n,\n  \n2.\n],\n\n        \n[\n \n2.\n,\n  \n2.\n]])\n\n\n\ntensor\n([[\n \n2.\n,\n  \n2.\n],\n\n        \n[\n \n2.\n,\n  \n2.\n]])\n\n\n\n\n\n\n\nTensor with gradients multiplication operation\n\n\nAs usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation!\n\nprint\n(\na\n \n*\n \nb\n)\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\n\n\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\nManually and Automatically Calculating Gradients\n\u00b6\n\n\nWhat exactly is \nrequires_grad\n?\n\n- Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation\n\n\n\n\ny_i = 5(x_i+1)^2\n\n\ny_i = 5(x_i+1)^2\n\n\n\n\n\n\nCreate tensor of size 2x1 filled with 1's that requires gradient\n\n\nx\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \nrequires_grad\n=\nTrue\n)\n\n\nx\n\n\n\n\n\n\n\ntensor\n([\n \n1.\n,\n  \n1.\n])\n\n\n\n\n\n\n\nSimple linear equation with x tensor created\n\n\n\n\ny_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\n\n\ny_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\n\n\n\n\nWe should get a value of 20 by replicating this simple equation \n\n\ny\n \n=\n \n5\n \n*\n \n(\nx\n \n+\n \n1\n)\n \n**\n \n2\n\n\ny\n\n\n\n\n\n\n\ntensor\n([\n \n20.\n,\n  \n20.\n])\n\n\n\n\n\n\n\nSimple equation with y tensor\n\n\nBackward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable\n\n\nLet's reduce y to a scalar then...\n\n\n\n\no = \\frac{1}{2}\\sum_i y_i\n\n\no = \\frac{1}{2}\\sum_i y_i\n\n\n\n\nAs you can see above, we've a tensor filled with 20's, so average them would return 20\n\n\no\n \n=\n \n(\n1\n/\n2\n)\n \n*\n \ntorch\n.\nsum\n(\ny\n)\n\n\no\n\n\n\n\n\n\n\ntensor\n(\n20.\n)\n\n\n\n\n\n\n\nCalculating first derivative\n\n\n \nRecap \ny\n equation\n: \ny_i = 5(x_i+1)^2\ny_i = 5(x_i+1)^2\n \n\n\n \nRecap \no\n equation\n: \no = \\frac{1}{2}\\sum_i y_i\no = \\frac{1}{2}\\sum_i y_i\n \n\n\n \nSubstitute \ny\n into \no\n equation\n: \no = \\frac{1}{2} \\sum_i 5(x_i+1)^2\no = \\frac{1}{2} \\sum_i 5(x_i+1)^2\n \n\n\n\n\n\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\n\n\n\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\n\n\n\n\n\n\n\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\n\n\n\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\n\n\n\n\nWe should expect to get 10, and it's so simple to do this with PyTorch with the following line...\n\n\nGet first derivative:\n\no\n.\nbackward\n()\n\n\n\n\nPrint out first derivative:\n\n\nx\n.\ngrad\n\n\n\n\n\n\n\ntensor\n([\n \n10.\n,\n  \n10.\n])\n\n\n\n\n\n\n\nIf x requires gradient and you create new objects with it, you get all gradients\n\n\nprint\n(\nx\n.\nrequires_grad\n)\n\n\nprint\n(\ny\n.\nrequires_grad\n)\n\n\nprint\n(\no\n.\nrequires_grad\n)\n\n\n\n\n\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Tensor with Gradients\n\n\n Wraps a tensor for gradient accumulation\n\n\n\n\n\n\n Gradients\n\n\n Define original equation\n\n\n Substitute equation with \nx\n values\n\n\n Reduce to scalar output, \no\n through \nmean\n\n\n Calculate gradients with \no.backward()\n\n\n Then access gradients of the \nx\n tensor with \nrequires_grad\n through \nx.grad",
            "title": "PyTorch Fundamentals - Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#pytorch-fundamentals-gradients",
            "text": "",
            "title": "PyTorch Fundamentals - Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients",
            "text": "",
            "title": "Tensors with Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients",
            "text": "Allows accumulation of gradients    Method 1: Create tensor with gradients  It is very similar to creating a tensor, all you need to do is to add an additional argument.  import   torch   a   =   torch . ones (( 2 ,   2 ),   requires_grad = True )  a    tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])    Check if tensor requires gradients  This should return True otherwise you've not done it right. a . requires_grad    True    Method 2: Create tensor with gradients  This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.  # Normal way of creating gradients  a   =   torch . ones (( 2 ,   2 ))  # Requires gradient  a . requires_grad_ ()  # Check if requires gradient  a . requires_grad    True    A tensor without gradients just for comparison  If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable  no_gradient   =   torch . ones ( 2 ,   2 )   no_gradient . requires_grad    False    Tensor with gradients addition operation  # Behaves similarly to tensors  b   =   torch . ones (( 2 ,   2 ),   requires_grad = True )  print ( a   +   b )  print ( torch . add ( a ,   b ))    tensor ([[   2. ,    2. ], \n         [   2. ,    2. ]])  tensor ([[   2. ,    2. ], \n         [   2. ,    2. ]])    Tensor with gradients multiplication operation  As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a   *   b )  print ( torch . mul ( a ,   b ))    tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])  tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])",
            "title": "Creating Tensors with Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients",
            "text": "What exactly is  requires_grad ? \n- Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation   y_i = 5(x_i+1)^2  y_i = 5(x_i+1)^2    Create tensor of size 2x1 filled with 1's that requires gradient  x   =   torch . ones ( 2 ,   requires_grad = True )  x    tensor ([   1. ,    1. ])    Simple linear equation with x tensor created   y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20  y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20   We should get a value of 20 by replicating this simple equation   y   =   5   *   ( x   +   1 )   **   2  y    tensor ([   20. ,    20. ])    Simple equation with y tensor  Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable  Let's reduce y to a scalar then...   o = \\frac{1}{2}\\sum_i y_i  o = \\frac{1}{2}\\sum_i y_i   As you can see above, we've a tensor filled with 20's, so average them would return 20  o   =   ( 1 / 2 )   *   torch . sum ( y )  o    tensor ( 20. )    Calculating first derivative    Recap  y  equation :  y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2      Recap  o  equation :  o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i      Substitute  y  into  o  equation :  o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2     \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]  \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]    \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10  \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10   We should expect to get 10, and it's so simple to do this with PyTorch with the following line...  Get first derivative: o . backward ()   Print out first derivative:  x . grad    tensor ([   10. ,    10. ])    If x requires gradient and you create new objects with it, you get all gradients  print ( x . requires_grad )  print ( y . requires_grad )  print ( o . requires_grad )    True  True  True",
            "title": "Manually and Automatically Calculating Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#summary",
            "text": "We've learnt to...   Success    Tensor with Gradients   Wraps a tensor for gradient accumulation     Gradients   Define original equation   Substitute equation with  x  values   Reduce to scalar output,  o  through  mean   Calculate gradients with  o.backward()   Then access gradients of the  x  tensor with  requires_grad  through  x.grad",
            "title": "Summary"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/",
            "text": "Linear Regression with PyTorch\n\u00b6\n\n\nAbout Linear Regression\n\u00b6\n\n\nSimple Linear Regression Basics\n\u00b6\n\n\n\n\nAllows us to understand \nrelationship\n between two \ncontinuous variables\n\n\nExample\n\n\nx: independent variable\n\n\nweight\n\n\n\n\n\n\ny: dependent variable\n\n\nheight\n\n\n\n\n\n\n\n\n\n\ny = \\alpha x + \\beta\ny = \\alpha x + \\beta\n\n\n\n\nExample of simple linear regression\n\u00b6\n\n\n\n\nCreate plot for simple linear regression\n\n\nTake note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists.\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n%\nmatplotlib\n \ninline\n\n\n\n# Creates 50 random x and y numbers\n\n\nnp\n.\nrandom\n.\nseed\n(\n1\n)\n\n\nn\n \n=\n \n50\n\n\nx\n \n=\n \nnp\n.\nrandom\n.\nrandn\n(\nn\n)\n\n\ny\n \n=\n \nx\n \n*\n \nnp\n.\nrandom\n.\nrandn\n(\nn\n)\n\n\n\n# Makes the dots colorful\n\n\ncolors\n \n=\n \nnp\n.\nrandom\n.\nrand\n(\nn\n)\n\n\n\n# Plots best-fit line via polyfit\n\n\nplt\n.\nplot\n(\nnp\n.\nunique\n(\nx\n),\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\nx\n,\n \ny\n,\n \n1\n))(\nnp\n.\nunique\n(\nx\n)))\n\n\n\n# Plots the random x and y data points we created\n\n\n# Interestingly, alpha makes it more aesthetically pleasing\n\n\nplt\n.\nscatter\n(\nx\n,\n \ny\n,\n \nc\n=\ncolors\n,\n \nalpha\n=\n0.5\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\n\n\n\nAim of Linear Regression\n\u00b6\n\n\n\n\nMinimize the distance between the points and the line (\ny = \\alpha x + \\beta\ny = \\alpha x + \\beta\n)\n\n\nAdjusting\n\n\nCoefficient: \n\\alpha\n\\alpha\n\n\nBias/intercept: \n\\beta\n\\beta\n\n\n\n\n\n\n\n\nBuilding a Linear Regression Model with PyTorch\n\u00b6\n\n\nExample\n\u00b6\n\n\n\n\nCoefficient: \n\\alpha = 2\n\\alpha = 2\n\n\nBias/intercept: \n\\beta = 1\n\\beta = 1\n\n\nEquation: \ny = 2x + 1\ny = 2x + 1\n\n\n\n\nBuilding a Toy Dataset\n\u00b6\n\n\n\n\nCreate a list of values from 0 to 11\n\n\nx_values\n \n=\n \n[\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n11\n)]\n\n\n\n\n\nx_values\n\n\n\n\n\n\n\n[\n0\n,\n \n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n]\n\n\n\n\n\n\n\nConvert list of numbers to numpy array\n\n\n# Convert to numpy\n\n\nx_train\n \n=\n \nnp\n.\narray\n(\nx_values\n,\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\nx_train\n.\nshape\n\n\n\n\n\n\n\n(\n11\n,)\n\n\n\n\n\n\n\nConvert to 2-dimensional array\n\n\nIf you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road.\n\n# IMPORTANT: 2D required\n\n\nx_train\n \n=\n \nx_train\n.\nreshape\n(\n-\n1\n,\n \n1\n)\n\n\nx_train\n.\nshape\n\n\n\n\n\n\n(\n11\n,\n \n1\n)\n\n\n\n\n\n\n\nCreate list of y values\n\n\nWe want y values for every x value we have above. \n\n\ny = 2x + 1\ny = 2x + 1\n\n\ny_values\n \n=\n \n[\n2\n*\ni\n \n+\n \n1\n \nfor\n \ni\n \nin\n \nx_values\n]\n\n\n\n\n\ny_values\n\n\n\n\n\n\n\n[\n1\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n9\n,\n \n11\n,\n \n13\n,\n \n15\n,\n \n17\n,\n \n19\n,\n \n21\n]\n\n\n\n\n\n\n\nAlternative to create list of y values\n\n\nIf you're weak in list iterators, this might be an easier alternative.\n\n# In case you're weak in list iterators...\n\n\ny_values\n \n=\n \n[]\n\n\nfor\n \ni\n \nin\n \nx_values\n:\n\n    \nresult\n \n=\n \n2\n*\ni\n \n+\n \n1\n\n    \ny_values\n.\nappend\n(\nresult\n)\n \n\n\n\ny_values\n\n\n\n\n\n\n\n[\n1\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n9\n,\n \n11\n,\n \n13\n,\n \n15\n,\n \n17\n,\n \n19\n,\n \n21\n]\n\n\n\n\n\n\n\nConvert to numpy array\n\n\nYou will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good.\n\n\ny_train\n \n=\n \nnp\n.\narray\n(\ny_values\n,\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\ny_train\n.\nshape\n\n\n\n\n\n\n\n(\n11\n,)\n\n\n\n\n\n\n\nReshape y numpy array to 2-dimension\n\n\n# IMPORTANT: 2D required\n\n\ny_train\n \n=\n \ny_train\n.\nreshape\n(\n-\n1\n,\n \n1\n)\n\n\ny_train\n.\nshape\n\n\n\n\n\n\n\n(\n11\n,\n \n1\n)\n\n\n\n\n\nBuilding Model\n\u00b6\n\n\n\n\nCritical Imports\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\n\n\n\n\n\n\n\nCreate Model\n\n\n\n\nLinear model\n\n\nTrue Equation: \ny = 2x + 1\ny = 2x + 1\n\n\n\n\n\n\nForward\n\n\nExample\n\n\nInput \nx = 1\nx = 1\n\n\nOutput \n\\hat y = ?\n\\hat y = ?\n\n\n\n\n\n\n\n\n\n\n\n\n# Create class\n\n\nclass\n \nLinearRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLinearRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n  \n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n\n\n\n\n\n\nInstantiate Model Class\n\n\n\n\ninput: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\ndesired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n\n\n\n\ninput_dim\n \n=\n \n1\n\n\noutput_dim\n \n=\n \n1\n\n\n\nmodel\n \n=\n \nLinearRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n\n\n\n\n\n\nInstantiate Loss Class\n\n\n\n\nMSE Loss: Mean Squared Error\n\n\nMSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)\nMSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)\n\n\n\\hat y\n\\hat y\n: prediction\n\n\ny\ny\n: true value\n\n\n\n\n\n\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\n\n\n\n\n\n\n\nInstantiate Optimizer Class\n\n\n\n\nSimplified equation\n\n\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\n\n\n\\theta\n\\theta\n: parameters (our variables)\n\n\n\\eta\n\\eta\n: learning rate (how fast we want to learn)\n\n\n\\nabla_\\theta\n\\nabla_\\theta\n: parameters' gradients\n\n\n\n\n\n\n\n\n\n\nEven simplier equation\n\n\nparameters = parameters - learning_rate * parameters_gradients\n\n\nparameters: \n\\alpha\n\\alpha\n and \n\\beta\n\\beta\n in \ny = \\alpha x + \\beta\ny = \\alpha x + \\beta\n\n\ndesired parameters: \n\\alpha = 2\n\\alpha = 2\n and \n\\beta = 1\n\\beta = 1\n in $ y = 2x + 1$ \n\n\n\n\n\n\n\n\n\n\n\n\nlearning_rate\n \n=\n \n0.01\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n\n\n\n\n\n\nTrain Model\n\n\n\n\n\n\n1 epoch: going through the whole x_train data once\n\n\n\n\n100 epochs: \n\n\n100x mapping \nx_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\n\n\n\n\n\n\nProcess \n\n\n\n\nConvert inputs/labels to tensors with gradients\n\n\nClear gradient buffets\n\n\nGet output given inputs \n\n\nGet loss\n\n\nGet gradients w.r.t. parameters\n\n\nUpdate parameters using gradients\n\n\nparameters = parameters - learning_rate * parameters_gradients\n\n\n\n\n\n\nREPEAT\n\n\n\n\n\n\n\n\nepochs\n \n=\n \n100\n\n\n\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n    \nepoch\n \n+=\n \n1\n\n    \n# Convert numpy array to torch Variable\n\n    \ninputs\n \n=\n \ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n()\n\n    \nlabels\n \n=\n \ntorch\n.\nfrom_numpy\n(\ny_train\n)\n\n\n    \n# Clear gradients w.r.t. parameters\n\n    \noptimizer\n.\nzero_grad\n()\n \n\n    \n# Forward to get output\n\n    \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n\n    \n# Calculate Loss\n\n    \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n    \n# Getting gradients w.r.t. parameters\n\n    \nloss\n.\nbackward\n()\n\n\n    \n# Updating parameters\n\n    \noptimizer\n.\nstep\n()\n\n\n    \nprint\n(\n'epoch {}, loss {}'\n.\nformat\n(\nepoch\n,\n \nloss\n.\nitem\n()))\n\n\n\n\n\n\n\nepoch\n \n1\n,\n \nloss\n \n140.58143615722656\n\n\nepoch\n \n2\n,\n \nloss\n \n11.467253684997559\n\n\nepoch\n \n3\n,\n \nloss\n \n0.9358152747154236\n\n\nepoch\n \n4\n,\n \nloss\n \n0.07679400593042374\n\n\nepoch\n \n5\n,\n \nloss\n \n0.0067212567664682865\n\n\nepoch\n \n6\n,\n \nloss\n \n0.0010006226366385818\n\n\nepoch\n \n7\n,\n \nloss\n \n0.0005289533291943371\n\n\nepoch\n \n8\n,\n \nloss\n \n0.0004854927829001099\n\n\nepoch\n \n9\n,\n \nloss\n \n0.00047700389404781163\n\n\nepoch\n \n10\n,\n \nloss\n \n0.0004714332753792405\n\n\nepoch\n \n11\n,\n \nloss\n \n0.00046614606981165707\n\n\nepoch\n \n12\n,\n \nloss\n \n0.0004609318566508591\n\n\nepoch\n \n13\n,\n \nloss\n \n0.0004557870561257005\n\n\nepoch\n \n14\n,\n \nloss\n \n0.00045069155748933554\n\n\nepoch\n \n15\n,\n \nloss\n \n0.00044567222357727587\n\n\nepoch\n \n16\n,\n \nloss\n \n0.00044068993884138763\n\n\nepoch\n \n17\n,\n \nloss\n \n0.00043576463940553367\n\n\nepoch\n \n18\n,\n \nloss\n \n0.00043090470717288554\n\n\nepoch\n \n19\n,\n \nloss\n \n0.00042609183583408594\n\n\nepoch\n \n20\n,\n \nloss\n \n0.0004213254142086953\n\n\nepoch\n \n21\n,\n \nloss\n \n0.0004166301223449409\n\n\nepoch\n \n22\n,\n \nloss\n \n0.0004119801160413772\n\n\nepoch\n \n23\n,\n \nloss\n \n0.00040738462121225893\n\n\nepoch\n \n24\n,\n \nloss\n \n0.0004028224211651832\n\n\nepoch\n \n25\n,\n \nloss\n \n0.0003983367350883782\n\n\nepoch\n \n26\n,\n \nloss\n \n0.0003938761365134269\n\n\nepoch\n \n27\n,\n \nloss\n \n0.000389480876037851\n\n\nepoch\n \n28\n,\n \nloss\n \n0.00038514015614055097\n\n\nepoch\n \n29\n,\n \nloss\n \n0.000380824290914461\n\n\nepoch\n \n30\n,\n \nloss\n \n0.00037657516077160835\n\n\nepoch\n \n31\n,\n \nloss\n \n0.000372376263840124\n\n\nepoch\n \n32\n,\n \nloss\n \n0.0003682126116473228\n\n\nepoch\n \n33\n,\n \nloss\n \n0.0003640959912445396\n\n\nepoch\n \n34\n,\n \nloss\n \n0.00036003670538775623\n\n\nepoch\n \n35\n,\n \nloss\n \n0.00035601368290372193\n\n\nepoch\n \n36\n,\n \nloss\n \n0.00035203873994760215\n\n\nepoch\n \n37\n,\n \nloss\n \n0.00034810820943675935\n\n\nepoch\n \n38\n,\n \nloss\n \n0.000344215368386358\n\n\nepoch\n \n39\n,\n \nloss\n \n0.0003403784066904336\n\n\nepoch\n \n40\n,\n \nloss\n \n0.00033658024040050805\n\n\nepoch\n \n41\n,\n \nloss\n \n0.0003328165039420128\n\n\nepoch\n \n42\n,\n \nloss\n \n0.0003291067841928452\n\n\nepoch\n \n43\n,\n \nloss\n \n0.0003254293987993151\n\n\nepoch\n \n44\n,\n \nloss\n \n0.0003217888588551432\n\n\nepoch\n \n45\n,\n \nloss\n \n0.0003182037326041609\n\n\nepoch\n \n46\n,\n \nloss\n \n0.0003146533854305744\n\n\nepoch\n \n47\n,\n \nloss\n \n0.00031113551813177764\n\n\nepoch\n \n48\n,\n \nloss\n \n0.0003076607536058873\n\n\nepoch\n \n49\n,\n \nloss\n \n0.00030422292184084654\n\n\nepoch\n \n50\n,\n \nloss\n \n0.00030083119054324925\n\n\nepoch\n \n51\n,\n \nloss\n \n0.00029746422660537064\n\n\nepoch\n \n52\n,\n \nloss\n \n0.0002941471466328949\n\n\nepoch\n \n53\n,\n \nloss\n \n0.00029085995629429817\n\n\nepoch\n \n54\n,\n \nloss\n \n0.0002876132493838668\n\n\nepoch\n \n55\n,\n \nloss\n \n0.00028440452297218144\n\n\nepoch\n \n56\n,\n \nloss\n \n0.00028122696676291525\n\n\nepoch\n \n57\n,\n \nloss\n \n0.00027808290906250477\n\n\nepoch\n \n58\n,\n \nloss\n \n0.00027497278642840683\n\n\nepoch\n \n59\n,\n \nloss\n \n0.00027190230321139097\n\n\nepoch\n \n60\n,\n \nloss\n \n0.00026887087733484805\n\n\nepoch\n \n61\n,\n \nloss\n \n0.0002658693410921842\n\n\nepoch\n \n62\n,\n \nloss\n \n0.0002629039518069476\n\n\nepoch\n \n63\n,\n \nloss\n \n0.00025996880140155554\n\n\nepoch\n \n64\n,\n \nloss\n \n0.0002570618235040456\n\n\nepoch\n \n65\n,\n \nloss\n \n0.00025419273879379034\n\n\nepoch\n \n66\n,\n \nloss\n \n0.00025135406758636236\n\n\nepoch\n \n67\n,\n \nloss\n \n0.0002485490695107728\n\n\nepoch\n \n68\n,\n \nloss\n \n0.0002457649679854512\n\n\nepoch\n \n69\n,\n \nloss\n \n0.0002430236927466467\n\n\nepoch\n \n70\n,\n \nloss\n \n0.00024031475186347961\n\n\nepoch\n \n71\n,\n \nloss\n \n0.00023762597993481904\n\n\nepoch\n \n72\n,\n \nloss\n \n0.00023497406800743192\n\n\nepoch\n \n73\n,\n \nloss\n \n0.0002323519001947716\n\n\nepoch\n \n74\n,\n \nloss\n \n0.00022976362379267812\n\n\nepoch\n \n75\n,\n \nloss\n \n0.0002271933335578069\n\n\nepoch\n \n76\n,\n \nloss\n \n0.00022465786605607718\n\n\nepoch\n \n77\n,\n \nloss\n \n0.00022214400814846158\n\n\nepoch\n \n78\n,\n \nloss\n \n0.00021966728672850877\n\n\nepoch\n \n79\n,\n \nloss\n \n0.0002172116219298914\n\n\nepoch\n \n80\n,\n \nloss\n \n0.00021478648704942316\n\n\nepoch\n \n81\n,\n \nloss\n \n0.00021239375928416848\n\n\nepoch\n \n82\n,\n \nloss\n \n0.0002100227284245193\n\n\nepoch\n \n83\n,\n \nloss\n \n0.00020767028036061674\n\n\nepoch\n \n84\n,\n \nloss\n \n0.00020534756185952574\n\n\nepoch\n \n85\n,\n \nloss\n \n0.00020305956422816962\n\n\nepoch\n \n86\n,\n \nloss\n \n0.0002007894654525444\n\n\nepoch\n \n87\n,\n \nloss\n \n0.00019854879064951092\n\n\nepoch\n \n88\n,\n \nloss\n \n0.00019633043848443776\n\n\nepoch\n \n89\n,\n \nloss\n \n0.00019413618429098278\n\n\nepoch\n \n90\n,\n \nloss\n \n0.00019197272195015103\n\n\nepoch\n \n91\n,\n \nloss\n \n0.0001898303598864004\n\n\nepoch\n \n92\n,\n \nloss\n \n0.00018771187751553953\n\n\nepoch\n \n93\n,\n \nloss\n \n0.00018561164324637502\n\n\nepoch\n \n94\n,\n \nloss\n \n0.00018354636267758906\n\n\nepoch\n \n95\n,\n \nloss\n \n0.00018149390234611928\n\n\nepoch\n \n96\n,\n \nloss\n \n0.0001794644631445408\n\n\nepoch\n \n97\n,\n \nloss\n \n0.00017746571393217891\n\n\nepoch\n \n98\n,\n \nloss\n \n0.00017548113828524947\n\n\nepoch\n \n99\n,\n \nloss\n \n0.00017352371651213616\n\n\nepoch\n \n100\n,\n \nloss\n \n0.00017157981346827\n\n\n\n\n\n\n\nLooking at predicted values\n\n\n# Purely inference\n\n\npredicted\n \n=\n \nmodel\n(\ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n())\n.\ndata\n.\nnumpy\n()\n\n\npredicted\n\n\n\n\n\n\n\narray\n([[\n \n0.9756333\n],\n\n       \n[\n \n2.9791424\n],\n\n       \n[\n \n4.982651\n \n],\n\n       \n[\n \n6.9861603\n],\n\n       \n[\n \n8.98967\n  \n],\n\n       \n[\n10.993179\n \n],\n\n       \n[\n12.996688\n \n],\n\n       \n[\n15.000196\n \n],\n\n       \n[\n17.003706\n \n],\n\n       \n[\n19.007215\n \n],\n\n       \n[\n21.010725\n \n]],\n \ndtype\n=\nfloat32\n)\n\n\n\n\n\n\n\nLooking at training values\n\n\nThese are the true values, you can see how it's able to predict similar values.\n\n\n# y = 2x + 1 \n\n\ny_train\n\n\n\n\n\n\n\narray\n([[\n \n1.\n],\n\n       \n[\n \n3.\n],\n\n       \n[\n \n5.\n],\n\n       \n[\n \n7.\n],\n\n       \n[\n \n9.\n],\n\n       \n[\n11.\n],\n\n       \n[\n13.\n],\n\n       \n[\n15.\n],\n\n       \n[\n17.\n],\n\n       \n[\n19.\n],\n\n       \n[\n21.\n]],\n \ndtype\n=\nfloat32\n)\n\n\n\n\n\n\n\nPlot of predicted and actual values\n\n\n# Clear figure\n\n\nplt\n.\nclf\n()\n\n\n\n# Get predictions\n\n\npredicted\n \n=\n \nmodel\n(\ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n())\n.\ndata\n.\nnumpy\n()\n\n\n\n# Plot true data\n\n\nplt\n.\nplot\n(\nx_train\n,\n \ny_train\n,\n \n'go'\n,\n \nlabel\n=\n'True data'\n,\n \nalpha\n=\n0.5\n)\n\n\n\n# Plot predictions\n\n\nplt\n.\nplot\n(\nx_train\n,\n \npredicted\n,\n \n'--'\n,\n \nlabel\n=\n'Predictions'\n,\n \nalpha\n=\n0.5\n)\n\n\n\n# Legend and plot\n\n\nplt\n.\nlegend\n(\nloc\n=\n'best'\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\n\n\n\n\n\nSave Model\n\n\nsave_model\n \n=\n \nFalse\n\n\nif\n \nsave_model\n \nis\n \nTrue\n:\n\n    \n# Saves only parameters\n\n    \n# alpha & beta\n\n    \ntorch\n.\nsave\n(\nmodel\n.\nstate_dict\n(),\n \n'awesome_model.pkl'\n)\n\n\n\n\n\n\n\n\n\nLoad Model\n\n\nload_model\n \n=\n \nFalse\n\n\nif\n \nload_model\n \nis\n \nTrue\n:\n\n    \nmodel\n.\nload_state_dict\n(\ntorch\n.\nload\n(\n'awesome_model.pkl'\n))\n\n\n\n\n\n\n\nBuilding a Linear Regression Model with PyTorch (GPU)\n\u00b6\n\n\n\n\nCPU Summary\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\n\n'''\n\n\nSTEP 1: CREATE MODEL CLASS\n\n\n'''\n\n\nclass\n \nLinearRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLinearRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n  \n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n'''\n\n\nSTEP 2: INSTANTIATE MODEL CLASS\n\n\n'''\n\n\ninput_dim\n \n=\n \n1\n\n\noutput_dim\n \n=\n \n1\n\n\n\nmodel\n \n=\n \nLinearRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n'''\n\n\nSTEP 3: INSTANTIATE LOSS CLASS\n\n\n'''\n\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\n\n'''\n\n\nSTEP 4: INSTANTIATE OPTIMIZER CLASS\n\n\n'''\n\n\n\nlearning_rate\n \n=\n \n0.01\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n'''\n\n\nSTEP 5: TRAIN THE MODEL\n\n\n'''\n\n\nepochs\n \n=\n \n100\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n    \nepoch\n \n+=\n \n1\n\n    \n# Convert numpy array to torch Variable\n\n    \ninputs\n \n=\n \ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n()\n\n    \nlabels\n \n=\n \ntorch\n.\nfrom_numpy\n(\ny_train\n)\n\n\n    \n# Clear gradients w.r.t. parameters\n\n    \noptimizer\n.\nzero_grad\n()\n \n\n    \n# Forward to get output\n\n    \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n\n    \n# Calculate Loss\n\n    \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n    \n# Getting gradients w.r.t. parameters\n\n    \nloss\n.\nbackward\n()\n\n\n    \n# Updating parameters\n\n    \noptimizer\n.\nstep\n()\n\n\n\n\n\n\n\n\n\nGPU Summary\n\n\n\n\nJust remember always 2 things must be on GPU\n\n\nmodel\n\n\ntensors with gradients\n\n\n\n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n'''\n\n\nSTEP 1: CREATE MODEL CLASS\n\n\n'''\n\n\nclass\n \nLinearRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLinearRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n  \n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n'''\n\n\nSTEP 2: INSTANTIATE MODEL CLASS\n\n\n'''\n\n\ninput_dim\n \n=\n \n1\n\n\noutput_dim\n \n=\n \n1\n\n\n\nmodel\n \n=\n \nLinearRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n\n#######################\n\n\n#  USE GPU FOR MODEL  #\n\n\n#######################\n\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\nmodel\n.\nto\n(\ndevice\n)\n\n\n\n'''\n\n\nSTEP 3: INSTANTIATE LOSS CLASS\n\n\n'''\n\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\n\n'''\n\n\nSTEP 4: INSTANTIATE OPTIMIZER CLASS\n\n\n'''\n\n\n\nlearning_rate\n \n=\n \n0.01\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n'''\n\n\nSTEP 5: TRAIN THE MODEL\n\n\n'''\n\n\nepochs\n \n=\n \n100\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n    \nepoch\n \n+=\n \n1\n\n    \n# Convert numpy array to torch Variable\n\n\n    \n#######################\n\n    \n#  USE GPU FOR MODEL  #\n\n    \n#######################\n\n    \ninputs\n \n=\n \ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nto\n(\ndevice\n)\n\n    \nlabels\n \n=\n \ntorch\n.\nfrom_numpy\n(\ny_train\n)\n.\nto\n(\ndevice\n)\n\n\n    \n# Clear gradients w.r.t. parameters\n\n    \noptimizer\n.\nzero_grad\n()\n \n\n    \n# Forward to get output\n\n    \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n\n    \n# Calculate Loss\n\n    \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n    \n# Getting gradients w.r.t. parameters\n\n    \nloss\n.\nbackward\n()\n\n\n    \n# Updating parameters\n\n    \noptimizer\n.\nstep\n()\n\n\n    \n# Logging\n\n    \nprint\n(\n'epoch {}, loss {}'\n.\nformat\n(\nepoch\n,\n \nloss\n.\nitem\n()))\n\n\n\n\n\n\n\nepoch\n \n1\n,\n \nloss\n \n336.0314025878906\n\n\nepoch\n \n2\n,\n \nloss\n \n27.67657470703125\n\n\nepoch\n \n3\n,\n \nloss\n \n2.5220539569854736\n\n\nepoch\n \n4\n,\n \nloss\n \n0.46732547879219055\n\n\nepoch\n \n5\n,\n \nloss\n \n0.2968060076236725\n\n\nepoch\n \n6\n,\n \nloss\n \n0.2800087630748749\n\n\nepoch\n \n7\n,\n \nloss\n \n0.27578213810920715\n\n\nepoch\n \n8\n,\n \nloss\n \n0.2726128399372101\n\n\nepoch\n \n9\n,\n \nloss\n \n0.269561231136322\n\n\nepoch\n \n10\n,\n \nloss\n \n0.2665504515171051\n\n\nepoch\n \n11\n,\n \nloss\n \n0.2635740041732788\n\n\nepoch\n \n12\n,\n \nloss\n \n0.26063060760498047\n\n\nepoch\n \n13\n,\n \nloss\n \n0.2577202618122101\n\n\nepoch\n \n14\n,\n \nloss\n \n0.2548423111438751\n\n\nepoch\n \n15\n,\n \nloss\n \n0.25199657678604126\n\n\nepoch\n \n16\n,\n \nloss\n \n0.24918246269226074\n\n\nepoch\n \n17\n,\n \nloss\n \n0.24639996886253357\n\n\nepoch\n \n18\n,\n \nloss\n \n0.24364829063415527\n\n\nepoch\n \n19\n,\n \nloss\n \n0.24092751741409302\n\n\nepoch\n \n20\n,\n \nloss\n \n0.2382371574640274\n\n\nepoch\n \n21\n,\n \nloss\n \n0.23557686805725098\n\n\nepoch\n \n22\n,\n \nloss\n \n0.2329462170600891\n\n\nepoch\n \n23\n,\n \nloss\n \n0.2303449958562851\n\n\nepoch\n \n24\n,\n \nloss\n \n0.22777271270751953\n\n\nepoch\n \n25\n,\n \nloss\n \n0.2252292037010193\n\n\nepoch\n \n26\n,\n \nloss\n \n0.22271405160427094\n\n\nepoch\n \n27\n,\n \nloss\n \n0.22022713720798492\n\n\nepoch\n \n28\n,\n \nloss\n \n0.21776780486106873\n\n\nepoch\n \n29\n,\n \nloss\n \n0.21533599495887756\n\n\nepoch\n \n30\n,\n \nloss\n \n0.21293145418167114\n\n\nepoch\n \n31\n,\n \nloss\n \n0.21055366098880768\n\n\nepoch\n \n32\n,\n \nloss\n \n0.20820240676403046\n\n\nepoch\n \n33\n,\n \nloss\n \n0.2058774083852768\n\n\nepoch\n \n34\n,\n \nloss\n \n0.20357847213745117\n\n\nepoch\n \n35\n,\n \nloss\n \n0.20130516588687897\n\n\nepoch\n \n36\n,\n \nloss\n \n0.1990572065114975\n\n\nepoch\n \n37\n,\n \nloss\n \n0.19683438539505005\n\n\nepoch\n \n38\n,\n \nloss\n \n0.19463638961315155\n\n\nepoch\n \n39\n,\n \nloss\n \n0.19246290624141693\n\n\nepoch\n \n40\n,\n \nloss\n \n0.1903136670589447\n\n\nepoch\n \n41\n,\n \nloss\n \n0.1881885528564453\n\n\nepoch\n \n42\n,\n \nloss\n \n0.18608702719211578\n\n\nepoch\n \n43\n,\n \nloss\n \n0.18400898575782776\n\n\nepoch\n \n44\n,\n \nloss\n \n0.18195408582687378\n\n\nepoch\n \n45\n,\n \nloss\n \n0.17992223799228668\n\n\nepoch\n \n46\n,\n \nloss\n \n0.17791320383548737\n\n\nepoch\n \n47\n,\n \nloss\n \n0.17592646181583405\n\n\nepoch\n \n48\n,\n \nloss\n \n0.17396186292171478\n\n\nepoch\n \n49\n,\n \nloss\n \n0.17201924324035645\n\n\nepoch\n \n50\n,\n \nloss\n \n0.17009828984737396\n\n\nepoch\n \n51\n,\n \nloss\n \n0.16819894313812256\n\n\nepoch\n \n52\n,\n \nloss\n \n0.16632060706615448\n\n\nepoch\n \n53\n,\n \nloss\n \n0.16446338593959808\n\n\nepoch\n \n54\n,\n \nloss\n \n0.16262666881084442\n\n\nepoch\n \n55\n,\n \nloss\n \n0.16081078350543976\n\n\nepoch\n \n56\n,\n \nloss\n \n0.15901507437229156\n\n\nepoch\n \n57\n,\n \nloss\n \n0.15723931789398193\n\n\nepoch\n \n58\n,\n \nloss\n \n0.15548335015773773\n\n\nepoch\n \n59\n,\n \nloss\n \n0.15374726057052612\n\n\nepoch\n \n60\n,\n \nloss\n \n0.1520303338766098\n\n\nepoch\n \n61\n,\n \nloss\n \n0.15033268928527832\n\n\nepoch\n \n62\n,\n \nloss\n \n0.14865389466285706\n\n\nepoch\n \n63\n,\n \nloss\n \n0.14699392020702362\n\n\nepoch\n \n64\n,\n \nloss\n \n0.14535246789455414\n\n\nepoch\n \n65\n,\n \nloss\n \n0.14372935891151428\n\n\nepoch\n \n66\n,\n \nloss\n \n0.14212435483932495\n\n\nepoch\n \n67\n,\n \nloss\n \n0.14053721725940704\n\n\nepoch\n \n68\n,\n \nloss\n \n0.13896773755550385\n\n\nepoch\n \n69\n,\n \nloss\n \n0.1374160647392273\n\n\nepoch\n \n70\n,\n \nloss\n \n0.1358814686536789\n\n\nepoch\n \n71\n,\n \nloss\n \n0.13436420261859894\n\n\nepoch\n \n72\n,\n \nloss\n \n0.13286370038986206\n\n\nepoch\n \n73\n,\n \nloss\n \n0.1313801407814026\n\n\nepoch\n \n74\n,\n \nloss\n \n0.12991292774677277\n\n\nepoch\n \n75\n,\n \nloss\n \n0.12846232950687408\n\n\nepoch\n \n76\n,\n \nloss\n \n0.1270277351140976\n\n\nepoch\n \n77\n,\n \nloss\n \n0.12560924887657166\n\n\nepoch\n \n78\n,\n \nloss\n \n0.12420656532049179\n\n\nepoch\n \n79\n,\n \nloss\n \n0.12281957268714905\n\n\nepoch\n \n80\n,\n \nloss\n \n0.1214480847120285\n\n\nepoch\n \n81\n,\n \nloss\n \n0.12009195983409882\n\n\nepoch\n \n82\n,\n \nloss\n \n0.1187509223818779\n\n\nepoch\n \n83\n,\n \nloss\n \n0.11742479354143143\n\n\nepoch\n \n84\n,\n \nloss\n \n0.11611353605985641\n\n\nepoch\n \n85\n,\n \nloss\n \n0.11481687426567078\n\n\nepoch\n \n86\n,\n \nloss\n \n0.11353478580713272\n\n\nepoch\n \n87\n,\n \nloss\n \n0.11226697266101837\n\n\nepoch\n \n88\n,\n \nloss\n \n0.11101329326629639\n\n\nepoch\n \n89\n,\n \nloss\n \n0.10977360606193542\n\n\nepoch\n \n90\n,\n \nloss\n \n0.10854770988225937\n\n\nepoch\n \n91\n,\n \nloss\n \n0.10733554512262344\n\n\nepoch\n \n92\n,\n \nloss\n \n0.10613703727722168\n\n\nepoch\n \n93\n,\n \nloss\n \n0.10495180636644363\n\n\nepoch\n \n94\n,\n \nloss\n \n0.10377981513738632\n\n\nepoch\n \n95\n,\n \nloss\n \n0.10262089222669601\n\n\nepoch\n \n96\n,\n \nloss\n \n0.10147502273321152\n\n\nepoch\n \n97\n,\n \nloss\n \n0.1003417894244194\n\n\nepoch\n \n98\n,\n \nloss\n \n0.09922132641077042\n\n\nepoch\n \n99\n,\n \nloss\n \n0.0981132984161377\n\n\nepoch\n \n100\n,\n \nloss\n \n0.09701769798994064\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Simple \nlinear regression basics\n\n\n \ny = Ax + B\ny = Ax + B\n\n\n \ny = 2x + 1\ny = 2x + 1\n\n\n\n\n\n\n \nExample\n of simple linear regression\n\n\n \nAim\n of linear regression\n\n\n Minimizing distance between the points and the line\n\n\n Calculate \"distance\" through \nMSE\n\n\n Calculate \ngradients\n\n\n Update parameters with \nparameters = parameters - learning_rate * gradients\n\n\n Slowly update parameters \nA\nA\n and \nB\nB\n model the linear relationship between \ny\ny\n and \nx\nx\n of the form \ny = 2x + 1\ny = 2x + 1\n\n\n\n\n\n\n\n\n\n\n Built a linear regression \nmodel\n in \nCPU and GPU\n\n\n Step 1: Create Model Class\n\n\n Step 2: Instantiate Model Class\n\n\n Step 3: Instantiate Loss Class\n\n\n Step 4: Instantiate Optimizer Class\n\n\n Step 5: Train Model\n\n\n\n\n\n\n Important things to be on \nGPU\n\n\n \nmodel\n\n\n \ntensors with gradients\n\n\n\n\n\n\n How to bring to \nGPU\n?\n\n\nmodel_name.to(device)\n\n\nvariable_name.to(device)",
            "title": "PyTorch Fundamentals - Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#linear-regression-with-pytorch",
            "text": "",
            "title": "Linear Regression with PyTorch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#about-linear-regression",
            "text": "",
            "title": "About Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#simple-linear-regression-basics",
            "text": "Allows us to understand  relationship  between two  continuous variables  Example  x: independent variable  weight    y: dependent variable  height      y = \\alpha x + \\beta y = \\alpha x + \\beta",
            "title": "Simple Linear Regression Basics"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#example-of-simple-linear-regression",
            "text": "Create plot for simple linear regression  Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists.  import   numpy   as   np  import   matplotlib.pyplot   as   plt  % matplotlib   inline  # Creates 50 random x and y numbers  np . random . seed ( 1 )  n   =   50  x   =   np . random . randn ( n )  y   =   x   *   np . random . randn ( n )  # Makes the dots colorful  colors   =   np . random . rand ( n )  # Plots best-fit line via polyfit  plt . plot ( np . unique ( x ),   np . poly1d ( np . polyfit ( x ,   y ,   1 ))( np . unique ( x )))  # Plots the random x and y data points we created  # Interestingly, alpha makes it more aesthetically pleasing  plt . scatter ( x ,   y ,   c = colors ,   alpha = 0.5 )  plt . show ()",
            "title": "Example of simple linear regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#aim-of-linear-regression",
            "text": "Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta )  Adjusting  Coefficient:  \\alpha \\alpha  Bias/intercept:  \\beta \\beta",
            "title": "Aim of Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch",
            "text": "",
            "title": "Building a Linear Regression Model with PyTorch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#example",
            "text": "Coefficient:  \\alpha = 2 \\alpha = 2  Bias/intercept:  \\beta = 1 \\beta = 1  Equation:  y = 2x + 1 y = 2x + 1",
            "title": "Example"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-toy-dataset",
            "text": "Create a list of values from 0 to 11  x_values   =   [ i   for   i   in   range ( 11 )]   x_values    [ 0 ,   1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ]    Convert list of numbers to numpy array  # Convert to numpy  x_train   =   np . array ( x_values ,   dtype = np . float32 )  x_train . shape    ( 11 ,)    Convert to 2-dimensional array  If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required  x_train   =   x_train . reshape ( - 1 ,   1 )  x_train . shape    ( 11 ,   1 )    Create list of y values  We want y values for every x value we have above.   y = 2x + 1 y = 2x + 1  y_values   =   [ 2 * i   +   1   for   i   in   x_values ]   y_values    [ 1 ,   3 ,   5 ,   7 ,   9 ,   11 ,   13 ,   15 ,   17 ,   19 ,   21 ]    Alternative to create list of y values  If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators...  y_values   =   []  for   i   in   x_values : \n     result   =   2 * i   +   1 \n     y_values . append ( result )    y_values    [ 1 ,   3 ,   5 ,   7 ,   9 ,   11 ,   13 ,   15 ,   17 ,   19 ,   21 ]    Convert to numpy array  You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good.  y_train   =   np . array ( y_values ,   dtype = np . float32 )  y_train . shape    ( 11 ,)    Reshape y numpy array to 2-dimension  # IMPORTANT: 2D required  y_train   =   y_train . reshape ( - 1 ,   1 )  y_train . shape    ( 11 ,   1 )",
            "title": "Building a Toy Dataset"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-model",
            "text": "Critical Imports  import   torch  import   torch.nn   as   nn     Create Model   Linear model  True Equation:  y = 2x + 1 y = 2x + 1    Forward  Example  Input  x = 1 x = 1  Output  \\hat y = ? \\hat y = ?       # Create class  class   LinearRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LinearRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim )   \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out     Instantiate Model Class   input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]   input_dim   =   1  output_dim   =   1  model   =   LinearRegressionModel ( input_dim ,   output_dim )     Instantiate Loss Class   MSE Loss: Mean Squared Error  MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)  \\hat y \\hat y : prediction  y y : true value     criterion   =   nn . MSELoss ()     Instantiate Optimizer Class   Simplified equation  \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta  \\theta \\theta : parameters (our variables)  \\eta \\eta : learning rate (how fast we want to learn)  \\nabla_\\theta \\nabla_\\theta : parameters' gradients      Even simplier equation  parameters = parameters - learning_rate * parameters_gradients  parameters:  \\alpha \\alpha  and  \\beta \\beta  in  y = \\alpha x + \\beta y = \\alpha x + \\beta  desired parameters:  \\alpha = 2 \\alpha = 2  and  \\beta = 1 \\beta = 1  in $ y = 2x + 1$        learning_rate   =   0.01  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )     Train Model    1 epoch: going through the whole x_train data once   100 epochs:   100x mapping  x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]       Process    Convert inputs/labels to tensors with gradients  Clear gradient buffets  Get output given inputs   Get loss  Get gradients w.r.t. parameters  Update parameters using gradients  parameters = parameters - learning_rate * parameters_gradients    REPEAT     epochs   =   100   for   epoch   in   range ( epochs ): \n     epoch   +=   1 \n     # Convert numpy array to torch Variable \n     inputs   =   torch . from_numpy ( x_train ) . requires_grad_ () \n     labels   =   torch . from_numpy ( y_train ) \n\n     # Clear gradients w.r.t. parameters \n     optimizer . zero_grad ()  \n\n     # Forward to get output \n     outputs   =   model ( inputs ) \n\n     # Calculate Loss \n     loss   =   criterion ( outputs ,   labels ) \n\n     # Getting gradients w.r.t. parameters \n     loss . backward () \n\n     # Updating parameters \n     optimizer . step () \n\n     print ( 'epoch {}, loss {}' . format ( epoch ,   loss . item ()))    epoch   1 ,   loss   140.58143615722656  epoch   2 ,   loss   11.467253684997559  epoch   3 ,   loss   0.9358152747154236  epoch   4 ,   loss   0.07679400593042374  epoch   5 ,   loss   0.0067212567664682865  epoch   6 ,   loss   0.0010006226366385818  epoch   7 ,   loss   0.0005289533291943371  epoch   8 ,   loss   0.0004854927829001099  epoch   9 ,   loss   0.00047700389404781163  epoch   10 ,   loss   0.0004714332753792405  epoch   11 ,   loss   0.00046614606981165707  epoch   12 ,   loss   0.0004609318566508591  epoch   13 ,   loss   0.0004557870561257005  epoch   14 ,   loss   0.00045069155748933554  epoch   15 ,   loss   0.00044567222357727587  epoch   16 ,   loss   0.00044068993884138763  epoch   17 ,   loss   0.00043576463940553367  epoch   18 ,   loss   0.00043090470717288554  epoch   19 ,   loss   0.00042609183583408594  epoch   20 ,   loss   0.0004213254142086953  epoch   21 ,   loss   0.0004166301223449409  epoch   22 ,   loss   0.0004119801160413772  epoch   23 ,   loss   0.00040738462121225893  epoch   24 ,   loss   0.0004028224211651832  epoch   25 ,   loss   0.0003983367350883782  epoch   26 ,   loss   0.0003938761365134269  epoch   27 ,   loss   0.000389480876037851  epoch   28 ,   loss   0.00038514015614055097  epoch   29 ,   loss   0.000380824290914461  epoch   30 ,   loss   0.00037657516077160835  epoch   31 ,   loss   0.000372376263840124  epoch   32 ,   loss   0.0003682126116473228  epoch   33 ,   loss   0.0003640959912445396  epoch   34 ,   loss   0.00036003670538775623  epoch   35 ,   loss   0.00035601368290372193  epoch   36 ,   loss   0.00035203873994760215  epoch   37 ,   loss   0.00034810820943675935  epoch   38 ,   loss   0.000344215368386358  epoch   39 ,   loss   0.0003403784066904336  epoch   40 ,   loss   0.00033658024040050805  epoch   41 ,   loss   0.0003328165039420128  epoch   42 ,   loss   0.0003291067841928452  epoch   43 ,   loss   0.0003254293987993151  epoch   44 ,   loss   0.0003217888588551432  epoch   45 ,   loss   0.0003182037326041609  epoch   46 ,   loss   0.0003146533854305744  epoch   47 ,   loss   0.00031113551813177764  epoch   48 ,   loss   0.0003076607536058873  epoch   49 ,   loss   0.00030422292184084654  epoch   50 ,   loss   0.00030083119054324925  epoch   51 ,   loss   0.00029746422660537064  epoch   52 ,   loss   0.0002941471466328949  epoch   53 ,   loss   0.00029085995629429817  epoch   54 ,   loss   0.0002876132493838668  epoch   55 ,   loss   0.00028440452297218144  epoch   56 ,   loss   0.00028122696676291525  epoch   57 ,   loss   0.00027808290906250477  epoch   58 ,   loss   0.00027497278642840683  epoch   59 ,   loss   0.00027190230321139097  epoch   60 ,   loss   0.00026887087733484805  epoch   61 ,   loss   0.0002658693410921842  epoch   62 ,   loss   0.0002629039518069476  epoch   63 ,   loss   0.00025996880140155554  epoch   64 ,   loss   0.0002570618235040456  epoch   65 ,   loss   0.00025419273879379034  epoch   66 ,   loss   0.00025135406758636236  epoch   67 ,   loss   0.0002485490695107728  epoch   68 ,   loss   0.0002457649679854512  epoch   69 ,   loss   0.0002430236927466467  epoch   70 ,   loss   0.00024031475186347961  epoch   71 ,   loss   0.00023762597993481904  epoch   72 ,   loss   0.00023497406800743192  epoch   73 ,   loss   0.0002323519001947716  epoch   74 ,   loss   0.00022976362379267812  epoch   75 ,   loss   0.0002271933335578069  epoch   76 ,   loss   0.00022465786605607718  epoch   77 ,   loss   0.00022214400814846158  epoch   78 ,   loss   0.00021966728672850877  epoch   79 ,   loss   0.0002172116219298914  epoch   80 ,   loss   0.00021478648704942316  epoch   81 ,   loss   0.00021239375928416848  epoch   82 ,   loss   0.0002100227284245193  epoch   83 ,   loss   0.00020767028036061674  epoch   84 ,   loss   0.00020534756185952574  epoch   85 ,   loss   0.00020305956422816962  epoch   86 ,   loss   0.0002007894654525444  epoch   87 ,   loss   0.00019854879064951092  epoch   88 ,   loss   0.00019633043848443776  epoch   89 ,   loss   0.00019413618429098278  epoch   90 ,   loss   0.00019197272195015103  epoch   91 ,   loss   0.0001898303598864004  epoch   92 ,   loss   0.00018771187751553953  epoch   93 ,   loss   0.00018561164324637502  epoch   94 ,   loss   0.00018354636267758906  epoch   95 ,   loss   0.00018149390234611928  epoch   96 ,   loss   0.0001794644631445408  epoch   97 ,   loss   0.00017746571393217891  epoch   98 ,   loss   0.00017548113828524947  epoch   99 ,   loss   0.00017352371651213616  epoch   100 ,   loss   0.00017157981346827    Looking at predicted values  # Purely inference  predicted   =   model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy ()  predicted    array ([[   0.9756333 ], \n        [   2.9791424 ], \n        [   4.982651   ], \n        [   6.9861603 ], \n        [   8.98967    ], \n        [ 10.993179   ], \n        [ 12.996688   ], \n        [ 15.000196   ], \n        [ 17.003706   ], \n        [ 19.007215   ], \n        [ 21.010725   ]],   dtype = float32 )    Looking at training values  These are the true values, you can see how it's able to predict similar values.  # y = 2x + 1   y_train    array ([[   1. ], \n        [   3. ], \n        [   5. ], \n        [   7. ], \n        [   9. ], \n        [ 11. ], \n        [ 13. ], \n        [ 15. ], \n        [ 17. ], \n        [ 19. ], \n        [ 21. ]],   dtype = float32 )    Plot of predicted and actual values  # Clear figure  plt . clf ()  # Get predictions  predicted   =   model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy ()  # Plot true data  plt . plot ( x_train ,   y_train ,   'go' ,   label = 'True data' ,   alpha = 0.5 )  # Plot predictions  plt . plot ( x_train ,   predicted ,   '--' ,   label = 'Predictions' ,   alpha = 0.5 )  # Legend and plot  plt . legend ( loc = 'best' )  plt . show ()      Save Model  save_model   =   False  if   save_model   is   True : \n     # Saves only parameters \n     # alpha & beta \n     torch . save ( model . state_dict (),   'awesome_model.pkl' )     Load Model  load_model   =   False  if   load_model   is   True : \n     model . load_state_dict ( torch . load ( 'awesome_model.pkl' ))",
            "title": "Building Model"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch-gpu",
            "text": "CPU Summary  import   torch  import   torch.nn   as   nn  '''  STEP 1: CREATE MODEL CLASS  '''  class   LinearRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LinearRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim )   \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out  '''  STEP 2: INSTANTIATE MODEL CLASS  '''  input_dim   =   1  output_dim   =   1  model   =   LinearRegressionModel ( input_dim ,   output_dim )  '''  STEP 3: INSTANTIATE LOSS CLASS  '''  criterion   =   nn . MSELoss ()  '''  STEP 4: INSTANTIATE OPTIMIZER CLASS  '''  learning_rate   =   0.01  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )  '''  STEP 5: TRAIN THE MODEL  '''  epochs   =   100  for   epoch   in   range ( epochs ): \n     epoch   +=   1 \n     # Convert numpy array to torch Variable \n     inputs   =   torch . from_numpy ( x_train ) . requires_grad_ () \n     labels   =   torch . from_numpy ( y_train ) \n\n     # Clear gradients w.r.t. parameters \n     optimizer . zero_grad ()  \n\n     # Forward to get output \n     outputs   =   model ( inputs ) \n\n     # Calculate Loss \n     loss   =   criterion ( outputs ,   labels ) \n\n     # Getting gradients w.r.t. parameters \n     loss . backward () \n\n     # Updating parameters \n     optimizer . step ()     GPU Summary   Just remember always 2 things must be on GPU  model  tensors with gradients     import   torch  import   torch.nn   as   nn  import   numpy   as   np  '''  STEP 1: CREATE MODEL CLASS  '''  class   LinearRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LinearRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim )   \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out  '''  STEP 2: INSTANTIATE MODEL CLASS  '''  input_dim   =   1  output_dim   =   1  model   =   LinearRegressionModel ( input_dim ,   output_dim )  #######################  #  USE GPU FOR MODEL  #  #######################  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  model . to ( device )  '''  STEP 3: INSTANTIATE LOSS CLASS  '''  criterion   =   nn . MSELoss ()  '''  STEP 4: INSTANTIATE OPTIMIZER CLASS  '''  learning_rate   =   0.01  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )  '''  STEP 5: TRAIN THE MODEL  '''  epochs   =   100  for   epoch   in   range ( epochs ): \n     epoch   +=   1 \n     # Convert numpy array to torch Variable \n\n     ####################### \n     #  USE GPU FOR MODEL  # \n     ####################### \n     inputs   =   torch . from_numpy ( x_train ) . to ( device ) \n     labels   =   torch . from_numpy ( y_train ) . to ( device ) \n\n     # Clear gradients w.r.t. parameters \n     optimizer . zero_grad ()  \n\n     # Forward to get output \n     outputs   =   model ( inputs ) \n\n     # Calculate Loss \n     loss   =   criterion ( outputs ,   labels ) \n\n     # Getting gradients w.r.t. parameters \n     loss . backward () \n\n     # Updating parameters \n     optimizer . step () \n\n     # Logging \n     print ( 'epoch {}, loss {}' . format ( epoch ,   loss . item ()))    epoch   1 ,   loss   336.0314025878906  epoch   2 ,   loss   27.67657470703125  epoch   3 ,   loss   2.5220539569854736  epoch   4 ,   loss   0.46732547879219055  epoch   5 ,   loss   0.2968060076236725  epoch   6 ,   loss   0.2800087630748749  epoch   7 ,   loss   0.27578213810920715  epoch   8 ,   loss   0.2726128399372101  epoch   9 ,   loss   0.269561231136322  epoch   10 ,   loss   0.2665504515171051  epoch   11 ,   loss   0.2635740041732788  epoch   12 ,   loss   0.26063060760498047  epoch   13 ,   loss   0.2577202618122101  epoch   14 ,   loss   0.2548423111438751  epoch   15 ,   loss   0.25199657678604126  epoch   16 ,   loss   0.24918246269226074  epoch   17 ,   loss   0.24639996886253357  epoch   18 ,   loss   0.24364829063415527  epoch   19 ,   loss   0.24092751741409302  epoch   20 ,   loss   0.2382371574640274  epoch   21 ,   loss   0.23557686805725098  epoch   22 ,   loss   0.2329462170600891  epoch   23 ,   loss   0.2303449958562851  epoch   24 ,   loss   0.22777271270751953  epoch   25 ,   loss   0.2252292037010193  epoch   26 ,   loss   0.22271405160427094  epoch   27 ,   loss   0.22022713720798492  epoch   28 ,   loss   0.21776780486106873  epoch   29 ,   loss   0.21533599495887756  epoch   30 ,   loss   0.21293145418167114  epoch   31 ,   loss   0.21055366098880768  epoch   32 ,   loss   0.20820240676403046  epoch   33 ,   loss   0.2058774083852768  epoch   34 ,   loss   0.20357847213745117  epoch   35 ,   loss   0.20130516588687897  epoch   36 ,   loss   0.1990572065114975  epoch   37 ,   loss   0.19683438539505005  epoch   38 ,   loss   0.19463638961315155  epoch   39 ,   loss   0.19246290624141693  epoch   40 ,   loss   0.1903136670589447  epoch   41 ,   loss   0.1881885528564453  epoch   42 ,   loss   0.18608702719211578  epoch   43 ,   loss   0.18400898575782776  epoch   44 ,   loss   0.18195408582687378  epoch   45 ,   loss   0.17992223799228668  epoch   46 ,   loss   0.17791320383548737  epoch   47 ,   loss   0.17592646181583405  epoch   48 ,   loss   0.17396186292171478  epoch   49 ,   loss   0.17201924324035645  epoch   50 ,   loss   0.17009828984737396  epoch   51 ,   loss   0.16819894313812256  epoch   52 ,   loss   0.16632060706615448  epoch   53 ,   loss   0.16446338593959808  epoch   54 ,   loss   0.16262666881084442  epoch   55 ,   loss   0.16081078350543976  epoch   56 ,   loss   0.15901507437229156  epoch   57 ,   loss   0.15723931789398193  epoch   58 ,   loss   0.15548335015773773  epoch   59 ,   loss   0.15374726057052612  epoch   60 ,   loss   0.1520303338766098  epoch   61 ,   loss   0.15033268928527832  epoch   62 ,   loss   0.14865389466285706  epoch   63 ,   loss   0.14699392020702362  epoch   64 ,   loss   0.14535246789455414  epoch   65 ,   loss   0.14372935891151428  epoch   66 ,   loss   0.14212435483932495  epoch   67 ,   loss   0.14053721725940704  epoch   68 ,   loss   0.13896773755550385  epoch   69 ,   loss   0.1374160647392273  epoch   70 ,   loss   0.1358814686536789  epoch   71 ,   loss   0.13436420261859894  epoch   72 ,   loss   0.13286370038986206  epoch   73 ,   loss   0.1313801407814026  epoch   74 ,   loss   0.12991292774677277  epoch   75 ,   loss   0.12846232950687408  epoch   76 ,   loss   0.1270277351140976  epoch   77 ,   loss   0.12560924887657166  epoch   78 ,   loss   0.12420656532049179  epoch   79 ,   loss   0.12281957268714905  epoch   80 ,   loss   0.1214480847120285  epoch   81 ,   loss   0.12009195983409882  epoch   82 ,   loss   0.1187509223818779  epoch   83 ,   loss   0.11742479354143143  epoch   84 ,   loss   0.11611353605985641  epoch   85 ,   loss   0.11481687426567078  epoch   86 ,   loss   0.11353478580713272  epoch   87 ,   loss   0.11226697266101837  epoch   88 ,   loss   0.11101329326629639  epoch   89 ,   loss   0.10977360606193542  epoch   90 ,   loss   0.10854770988225937  epoch   91 ,   loss   0.10733554512262344  epoch   92 ,   loss   0.10613703727722168  epoch   93 ,   loss   0.10495180636644363  epoch   94 ,   loss   0.10377981513738632  epoch   95 ,   loss   0.10262089222669601  epoch   96 ,   loss   0.10147502273321152  epoch   97 ,   loss   0.1003417894244194  epoch   98 ,   loss   0.09922132641077042  epoch   99 ,   loss   0.0981132984161377  epoch   100 ,   loss   0.09701769798994064",
            "title": "Building a Linear Regression Model with PyTorch (GPU)"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#summary",
            "text": "We've learnt to...   Success    Simple  linear regression basics    y = Ax + B y = Ax + B    y = 2x + 1 y = 2x + 1      Example  of simple linear regression    Aim  of linear regression   Minimizing distance between the points and the line   Calculate \"distance\" through  MSE   Calculate  gradients   Update parameters with  parameters = parameters - learning_rate * gradients   Slowly update parameters  A A  and  B B  model the linear relationship between  y y  and  x x  of the form  y = 2x + 1 y = 2x + 1       Built a linear regression  model  in  CPU and GPU   Step 1: Create Model Class   Step 2: Instantiate Model Class   Step 3: Instantiate Loss Class   Step 4: Instantiate Optimizer Class   Step 5: Train Model     Important things to be on  GPU    model    tensors with gradients     How to bring to  GPU ?  model_name.to(device)  variable_name.to(device)",
            "title": "Summary"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/",
            "text": "Logistic Regression with PyTorch\n\u00b6\n\n\nAbout Logistic Regression\n\u00b6\n\n\nLogistic Regression Basics\n\u00b6\n\n\nClassification algorithm\n\u00b6\n\n\n\n\nExample: Spam vs No Spam\n\n\nInput: Bunch of words\n\n\nOutput: Probability spam or not\n\n\n\n\n\n\n\n\nBasic Comparison\n\u00b6\n\n\n\n\nLinear regression\n\n\nOutput: numeric value given inputs\n\n\n\n\n\n\nLogistic regression\n:\n\n\nOutput: probability [0, 1] given input belonging to a class\n\n\n\n\n\n\n\n\nInput/Output Comparison\n\u00b6\n\n\n\n\nLinear regression: Multiplication\n\n\nInput: [1]\n\n\nOutput: 2\n\n\n\n\n\n\nInput: [2]\n\n\nOutput: 4\n\n\n\n\n\n\nTrying to model the relationship \ny = 2x\n\n\n\n\n\n\nLogistic regression: Spam\n\n\nInput: \"Sign up to get 1 million dollars by tonight\"\n\n\nOutput: p = 0.8\n\n\n\n\n\n\nInput: \"This is a receipt for your recent purchase with Amazon\"\n\n\nOutput: p = 0.3\n\n\n\n\n\n\np: probability it is spam\n\n\n\n\n\n\n\n\nProblems of Linear Regression\n\u00b6\n\n\n\n\nExample\n\n\nFever\n\n\nInput\n: temperature\n\n\nOutput\n: fever or no fever\n\n\n\n\n\n\nRemember\n\n\nLinear regression\n: minimize error between points and line\n\n\n\n\n\n\n\n\n\n\nLinear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)\n\n\nIf you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context.\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n%\nmatplotlib\n \ninline\n\n\n\nx\n \n=\n \n[\n1\n,\n \n5\n,\n \n10\n,\n \n10\n,\n \n25\n,\n \n50\n,\n \n70\n,\n \n75\n,\n \n100\n,]\n\n\ny\n \n=\n \n[\n0\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n1\n,\n \n1\n,\n \n1\n,\n \n1\n]\n\n\n\ncolors\n \n=\n \nnp\n.\nrandom\n.\nrand\n(\nlen\n(\nx\n))\n\n\nplt\n.\nplot\n(\nnp\n.\nunique\n(\nx\n),\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\nx\n,\n \ny\n,\n \n1\n))(\nnp\n.\nunique\n(\nx\n)))\n\n\nplt\n.\nylabel\n(\n\"Fever\"\n)\n\n\nplt\n.\nxlabel\n(\n\"Temperature\"\n)\n\n\n\nplt\n.\nscatter\n(\nx\n,\n \ny\n,\n \nc\n=\ncolors\n,\n \nalpha\n=\n0.5\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\n\n\n\n\nLinear Regression Problem 2: Fever points are not predicted with the presence of outliers\n\n\nPreviously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all.\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n\nx\n \n=\n \n[\n1\n,\n \n5\n,\n \n10\n,\n \n10\n,\n \n25\n,\n \n50\n,\n \n70\n,\n \n75\n,\n \n300\n]\n\n\ny\n \n=\n \n[\n0\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n1\n,\n \n1\n,\n \n1\n,\n \n1\n]\n\n\n\ncolors\n \n=\n \nnp\n.\nrandom\n.\nrand\n(\nlen\n(\nx\n))\n\n\nplt\n.\nplot\n(\nnp\n.\nunique\n(\nx\n),\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\nx\n,\n \ny\n,\n \n1\n))(\nnp\n.\nunique\n(\nx\n)))\n\n\nplt\n.\nylabel\n(\n\"Fever\"\n)\n\n\nplt\n.\nxlabel\n(\n\"Temperature\"\n)\n\n\n\nplt\n.\nscatter\n(\nx\n,\n \ny\n,\n \nc\n=\ncolors\n,\n \nalpha\n=\n0.5\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\n\n\nLogistic Regression In-Depth\n\u00b6\n\n\nPredicting Probability\n\u00b6\n\n\n\n\nLinear regression doesn't work\n\n\nInstead of predicting direct values: \npredict probability\n\n\n\n\n\n\nLogistic Function g()\n\u00b6\n\n\n\n\nTwo-class logistic regression\n\n\ny = A x + b\ny = A x + b\n\n\ng(y) = A x + b\ng(y) = A x + b\n\n\ng(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}}\ng(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}}\n\n\ng(y)\ng(y)\n = Estimated probability that \ny = 1\ny = 1\n given \nx\nx\n\n\n\n\nSoftmax Function g()\n\u00b6\n\n\n\n\nMulti-class logistic regression\n\n\nGeneralization of logistic function\n\n\n\n\nCross Entropy Function D()\n\u00b6\n\n\n\n\nD(S, L) = L log S - (1-L)log(1-S)\nD(S, L) = L log S - (1-L)log(1-S)\n\n\nIf L = 0 (label)\n\n\nD(S, 0) = - log(1-S)\nD(S, 0) = - log(1-S)\n\n\n- log(1-S)\n- log(1-S)\n: less positive if \nS \\longrightarrow 0\nS \\longrightarrow 0\n\n\n- log(1-S)\n- log(1-S)\n: more positive if \nS \\longrightarrow 1\nS \\longrightarrow 1\n (BIGGER LOSS)\n\n\n\n\n\n\n\n\n\n\nIf L = 1 (label)\n\n\nD(S, 1) = log S\nD(S, 1) = log S\n\n\nlogS\nlogS\n: less negative if \nS \\longrightarrow 1\nS \\longrightarrow 1\n\n\nlogS\nlogS\n: more negative if \nS \\longrightarrow 0\nS \\longrightarrow 0\n (BIGGER LOSS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical example of bigger or small loss\n\n\nYou get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction).\n\nimport\n \nmath\n\n\nprint\n(\n-\nmath\n.\nlog\n(\n1\n \n-\n \n0.00001\n))\n\n\n\n\nYou get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction).\n\nprint\n(\n-\nmath\n.\nlog\n(\n1\n \n-\n \n0.99999\n))\n \n\n\n\nYou get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction).\n\nprint\n(\nmath\n.\nlog\n(\n0.99999\n))\n\n\n\n\nYou get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction).\n\nprint\n(\nmath\n.\nlog\n(\n0.00001\n))\n\n\n\n\n\n\n1.0000050000287824e-05\n\n\n11.51292546497478\n\n\n-\n1.0000050000287824e-05\n\n\n-\n11.512925464970229\n\n\n\n\n\nCross Entropy Loss L\n\u00b6\n\n\n\n\nGoal: Minimizing Cross Entropy Loss\n\n\nL = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i)\nL = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i)\n\n\n\n\nBuilding a Logistic Regression Model with PyTorch\n\u00b6\n\n\n\n\nSteps\n\u00b6\n\n\n\n\nStep 1: Load Dataset\n\n\nStep 2: Make Dataset Iterable\n\n\nStep 3: Create Model Class\n\n\nStep 4: Instantiate Model Class\n\n\nStep 5: Instantiate Loss Class\n\n\nStep 6: Instantiate Optimizer Class\n\n\nStep 7: Train Model\n\n\n\n\nStep 1a: Loading MNIST Train Dataset\n\u00b6\n\n\nImages from 1 to 9\n\n\n\n\nInspect length of training dataset\n\n\nYou can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images.\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorchvision.transforms\n \nas\n \ntransforms\n\n\nimport\n \ntorchvision.datasets\n \nas\n \ndsets\n\n\n\n\ntrain_dataset\n \n=\n \ndsets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \n                            \ntrain\n=\nTrue\n,\n \n                            \ntransform\n=\ntransforms\n.\nToTensor\n(),\n\n                            \ndownload\n=\nTrue\n)\n\n\n\n\n\nlen\n(\ntrain_dataset\n)\n\n\n\n\n\n\n\n60000\n\n\n\n\n\n\n\nInspecting a single image\n\n\nSo this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers.\n\ntrain_dataset\n[\n0\n]\n\n\n\n\n\n\n(\ntensor\n([[[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0118\n,\n  \n0.0706\n,\n\n            \n0.0706\n,\n  \n0.0706\n,\n  \n0.4941\n,\n  \n0.5333\n,\n  \n0.6863\n,\n  \n0.1020\n,\n  \n0.6510\n,\n\n            \n1.0000\n,\n  \n0.9686\n,\n  \n0.4980\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.1176\n,\n  \n0.1412\n,\n  \n0.3686\n,\n  \n0.6039\n,\n  \n0.6667\n,\n  \n0.9922\n,\n\n            \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.8824\n,\n  \n0.6745\n,\n  \n0.9922\n,\n\n            \n0.9490\n,\n  \n0.7647\n,\n  \n0.2510\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.1922\n,\n  \n0.9333\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n\n            \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9843\n,\n  \n0.3647\n,\n  \n0.3216\n,\n  \n0.3216\n,\n\n            \n0.2196\n,\n  \n0.1529\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0706\n,\n  \n0.8588\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n\n            \n0.7765\n,\n  \n0.7137\n,\n  \n0.9686\n,\n  \n0.9451\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.3137\n,\n  \n0.6118\n,\n  \n0.4196\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.8039\n,\n\n            \n0.0431\n,\n  \n0.0000\n,\n  \n0.1686\n,\n  \n0.6039\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0549\n,\n  \n0.0039\n,\n  \n0.6039\n,\n  \n0.9922\n,\n  \n0.3529\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.5451\n,\n  \n0.9922\n,\n  \n0.7451\n,\n\n            \n0.0078\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0431\n,\n  \n0.7451\n,\n  \n0.9922\n,\n\n            \n0.2745\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.1373\n,\n  \n0.9451\n,\n\n            \n0.8824\n,\n  \n0.6275\n,\n  \n0.4235\n,\n  \n0.0039\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.3176\n,\n\n            \n0.9412\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.4667\n,\n  \n0.0980\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.1765\n,\n  \n0.7294\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.5882\n,\n  \n0.1059\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0627\n,\n  \n0.3647\n,\n  \n0.9882\n,\n  \n0.9922\n,\n  \n0.7333\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.9765\n,\n  \n0.9922\n,\n  \n0.9765\n,\n  \n0.2510\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.1804\n,\n  \n0.5098\n,\n  \n0.7176\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.8118\n,\n  \n0.0078\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.1529\n,\n  \n0.5804\n,\n\n            \n0.8980\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9804\n,\n  \n0.7137\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0941\n,\n  \n0.4471\n,\n  \n0.8667\n,\n  \n0.9922\n,\n\n            \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.7882\n,\n  \n0.3059\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0902\n,\n  \n0.2588\n,\n  \n0.8353\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n\n            \n0.9922\n,\n  \n0.7765\n,\n  \n0.3176\n,\n  \n0.0078\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0706\n,\n\n            \n0.6706\n,\n  \n0.8588\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.7647\n,\n\n            \n0.3137\n,\n  \n0.0353\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.2157\n,\n  \n0.6745\n,\n  \n0.8863\n,\n\n            \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9922\n,\n  \n0.9569\n,\n  \n0.5216\n,\n  \n0.0431\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.5333\n,\n  \n0.9922\n,\n  \n0.9922\n,\n\n            \n0.9922\n,\n  \n0.8314\n,\n  \n0.5294\n,\n  \n0.5176\n,\n  \n0.0627\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n],\n\n          \n[\n \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n\n            \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n,\n  \n0.0000\n]]]),\n\n \ntensor\n(\n5\n))\n\n\n\n\n\n\n\nInspecting a single data point in the training dataset\n\n\nWhen you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label.\n\n\ntype\n(\ntrain_dataset\n[\n0\n])\n\n\n\n\n\n\n\ntuple\n\n\n\n\n\n\n\nInspecting training dataset first element of tuple\n\n\nThis means to access the image, you need to access the first element in the tuple.\n\n\n# Input Matrix\n\n\ntrain_dataset\n[\n0\n][\n0\n]\n.\nsize\n()\n\n\n\n\n\n\n\n# A 28x28 sized image of a digit\n\n\ntorch\n.\nSize\n([\n1\n,\n \n28\n,\n \n28\n])\n\n\n\n\n\n\n\nInspecting training dataset second element of tuple\n\n\nThe second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5.\n\n\n# Label\n\n\ntrain_dataset\n[\n0\n][\n1\n]\n\n\n\n\n\n\n\ntensor\n(\n5\n)\n\n\n\n\n\nDisplaying MNIST\n\u00b6\n\n\n\n\nVerifying shape of MNIST image\n\n\nAs mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel.\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n%\nmatplotlib\n \ninline\n  \n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n\n\ntrain_dataset\n[\n0\n][\n0\n]\n.\nnumpy\n()\n.\nshape\n\n\n\n\n\n\n\n(\n1\n,\n \n28\n,\n \n28\n)\n\n\n\n\n\n\n\nPlot image of MNIST image\n\n\nshow_img\n \n=\n \ntrain_dataset\n[\n0\n][\n0\n]\n.\nnumpy\n()\n.\nreshape\n(\n28\n,\n \n28\n)\n\n\n\n\n\nplt\n.\nimshow\n(\nshow_img\n,\n \ncmap\n=\n'gray'\n)\n\n\n\n\n\n\n\n\n\n\n\nSecond element of tuple shows label\n\n\nAs you would expect, the label is 5.\n\n# Label\n\n\ntrain_dataset\n[\n0\n][\n1\n]\n\n\n\n\n\n\ntensor\n(\n5\n)\n\n\n\n\n\n\n\nPlot second image of MNIST image\n\n\nshow_img\n \n=\n \ntrain_dataset\n[\n1\n][\n0\n]\n.\nnumpy\n()\n.\nreshape\n(\n28\n,\n \n28\n)\n\n\n\n\n\nplt\n.\nimshow\n(\nshow_img\n,\n \ncmap\n=\n'gray'\n)\n\n\n\n\n\n\n\n\n\n\n\nSecond element of tuple shows label\n\n\nWe should see 0 here as the label.\n\n# Label\n\n\ntrain_dataset\n[\n1\n][\n1\n]\n\n\n\n\n\n\ntensor\n(\n0\n)\n\n\n\n\n\nStep 1b: Loading MNIST Test Dataset\n\u00b6\n\n\n\n\nShow our algorithm works beyond the data we have trained on.\n\n\nOut-of-sample\n\n\n\n\n\n\nLoad test dataset\n\n\nCompared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance.\n\ntest_dataset\n \n=\n \ndsets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \n                           \ntrain\n=\nFalse\n,\n \n                           \ntransform\n=\ntransforms\n.\nToTensor\n())\n\n\n\n\nlen\n(\ntest_dataset\n)\n\n\n\n\n\n\n\n10000\n\n\n\n\n\n\n\nTest dataset elements\n\n\nExactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels.\n\ntype\n(\ntest_dataset\n[\n0\n])\n\n\n\n\n\n\ntuple\n\n\n\n\n\n\n\nTest dataset first element in tuple\n\n\nThis contains the image matrix, similar to the training set.\n\n# Image matrix\n\n\ntest_dataset\n[\n0\n][\n0\n]\n.\nsize\n()\n\n\n\n\n\n\ntorch\n.\nSize\n([\n1\n,\n \n28\n,\n \n28\n])\n\n\n\n\n\n\n\nPlot image sample from test dataset\n\n\nshow_img\n \n=\n \ntest_dataset\n[\n0\n][\n0\n]\n.\nnumpy\n()\n.\nreshape\n(\n28\n,\n \n28\n)\n\n\nplt\n.\nimshow\n(\nshow_img\n,\n \ncmap\n=\n'gray'\n)\n\n\n\n\n\n\n\n\n\n\n\nTest dataset second element in tuple\n\n\n# Label\n\n\ntest_dataset\n[\n0\n][\n1\n]\n\n\n\n\n\n\n\ntensor\n(\n7\n)\n\n\n\n\n\nStep 2: Make Dataset Iterable\n\u00b6\n\n\n\n\nAim: make the dataset iterable\n\n\ntotaldata\n: 60000\n\n\nminibatch\n: 100\n\n\nNumber of examples in 1 iteration\n\n\n\n\n\n\niterations\n: 3000\n\n\n1 iteration: one mini-batch forward & backward pass\n\n\n\n\n\n\nepochs\n\n\n1 epoch: running through the whole dataset once\n\n\n$epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 $\n\n\n\n\n\n\n\n\n\n\nRecap training dataset\n\n\nRemember training dataset has 60k images and testing dataset has 10k images.\n\nlen\n(\ntrain_dataset\n)\n\n\n\n\n\n\n60000\n\n\n\n\n\n\n\nDefining epochs\n\n\nWhen the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. \n\n\nHowever, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.\n\n\nbatch_size\n \n=\n \n100\n\n\n\n\n\nWe arbitrarily set 3000 iterations here which means the model would update 3000 times.\n\nn_iters\n \n=\n \n3000\n\n\n\n\nOne epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. \n\n\nnum_epochs\n \n=\n \nn_iters\n \n/\n \n(\nlen\n(\ntrain_dataset\n)\n \n/\n \nbatch_size\n)\n\n\nnum_epochs\n \n=\n \nint\n(\nnum_epochs\n)\n\n\nnum_epochs\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\nCreate Iterable Object: Training Dataset\n\n\ntrain_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \n                                           \nbatch_size\n=\nbatch_size\n,\n \n                                           \nshuffle\n=\nTrue\n)\n\n\n\n\n\n\n\n\n\nCheck Iterability\n\n\nimport\n \ncollections\n\n\nisinstance\n(\ntrain_loader\n,\n \ncollections\n.\nIterable\n)\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nCreate Iterable Object: Testing Dataset\n\n\n# Iterable object\n\n\ntest_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntest_dataset\n,\n \n                                          \nbatch_size\n=\nbatch_size\n,\n \n                                          \nshuffle\n=\nFalse\n)\n\n\n\n\n\n\n\n\n\nCheck iterability of testing dataset\n\n\nisinstance\n(\ntest_loader\n,\n \ncollections\n.\nIterable\n)\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nIterate through dataset\n\n\nThis is just a simplified example of what we're doing above where we're creating an iterable object \nlst\n to loop through so we can access all the images \nimg_1\n and \nimg_2\n.\n\n\nAbove, the equivalent of \nlst\n is \ntrain_loader\n and \ntest_loader\n.\n\n\nimg_1\n \n=\n \nnp\n.\nones\n((\n28\n,\n \n28\n))\n\n\nimg_2\n \n=\n \nnp\n.\nones\n((\n28\n,\n \n28\n))\n\n\nlst\n \n=\n \n[\nimg_1\n,\n \nimg_2\n]\n\n\n\n\n\n# Need to iterate\n\n\n# Think of numbers as the images\n\n\nfor\n \ni\n \nin\n \nlst\n:\n\n    \nprint\n(\ni\n.\nshape\n)\n\n\n\n\n\n\n\n(\n28\n,\n \n28\n)\n\n\n(\n28\n,\n \n28\n)\n\n\n\n\n\nStep 3: Building Model\n\u00b6\n\n\n\n\nCreate model class\n\n\n# Same as linear regression! \n\n\nclass\n \nLogisticRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLogisticRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n\n\n\n\nStep 4: Instantiate Model Class\n\u00b6\n\n\n\n\nInput dimension: \n\n\nSize of image\n\n\n28 \\times 28 = 784\n28 \\times 28 = 784\n\n\n\n\n\n\nOutput dimension: 10\n\n\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n\n\n\n\n\n\n\n\n\n\nCheck size of dataset\n\n\nThis should be 28x28.\n\n# Size of images\n\n\ntrain_dataset\n[\n0\n][\n0\n]\n.\nsize\n()\n\n\n\n\n\n\ntorch\n.\nSize\n([\n1\n,\n \n28\n,\n \n28\n])\n\n\n\n\n\n\n\nInstantiate model class based on input and out dimensions\n\n\nAs we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. \n\n\nAnd we're feeding the model with 28x28 images, hence our input dimension is 28x28.\n\ninput_dim\n \n=\n \n28\n*\n28\n\n\noutput_dim\n \n=\n \n10\n\n\n\nmodel\n \n=\n \nLogisticRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n\n\n\nStep 5: Instantiate Loss Class\n\u00b6\n\n\n\n\nLogistic Regression\n: Cross Entropy Loss\n\n\nLinear Regression: MSE\n\n\n\n\n\n\n\n\n\n\nCreate Cross Entry Loss Class\n\n\nUnlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters.\n\n\ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n  \n\n\n\n\n\n\n\n\nWhat happens in nn.CrossEntropyLoss()?\n\n\nIt does 2 things at the same time. \n\n\n 1. Computes softmax (logistic/softmax function)\n\n 2. Computes cross entropy\n\n\n\n\n\n\nStep 6: Instantiate Optimizer Class\n\u00b6\n\n\n\n\nSimplified equation\n\n\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\n\\theta = \\theta - \\eta \\cdot \\nabla_\\theta\n\n\n\\theta\n\\theta\n: parameters (our variables)\n\n\n\\eta\n\\eta\n: learning rate (how fast we want to learn)\n\n\n\\nabla_\\theta\n\\nabla_\\theta\n: parameters' gradients\n\n\n\n\n\n\n\n\n\n\nEven simplier equation\n\n\nparameters = parameters - learning_rate * parameters_gradients\n\n\nAt every iteration, we update our model's parameters\n\n\n\n\n\n\n\n\n\n\nCreate optimizer\n\n\nSimilar to what we've covered above, this calculates the parameters' gradients and update them subsequently.\n\nlearning_rate\n \n=\n \n0.001\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n  \n\n\n\n\n\n\n\nParameters In-Depth\n\n\nYou'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the \ny = AX + b\ny = AX + b\n equation where X is our input of size 784.\n\n\nWe'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. \n\n\n# Type of parameter object\n\n\nprint\n(\nmodel\n.\nparameters\n())\n\n\n\n# Length of parameters\n\n\nprint\n(\nlen\n(\nlist\n(\nmodel\n.\nparameters\n())))\n\n\n\n# FC 1 Parameters \n\n\nprint\n(\nlist\n(\nmodel\n.\nparameters\n())[\n0\n]\n.\nsize\n())\n\n\n\n# FC 1 Bias Parameters\n\n\nprint\n(\nlist\n(\nmodel\n.\nparameters\n())[\n1\n]\n.\nsize\n())\n\n\n\n\n\n\n\n<\ngenerator\n \nobject\n \nModule\n.\nparameters\n \nat\n \n0x7ff7c884f830\n>\n\n\n2\n\n\ntorch\n.\nSize\n([\n10\n,\n \n784\n])\n\n\ntorch\n.\nSize\n([\n10\n])\n\n\n\n\n\n\n\nQuick Dot Product Review\n\n\n\n\nExample 1: \ndot product\n\n\nA: (100, 10)\nA: (100, 10)\n\n\nB: (10, 1)\nB: (10, 1)\n\n\nA \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1)\nA \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1)\n\n\n\n\n\n\nExample 2: \ndot product\n\n\nA: (50, 5)\nA: (50, 5)\n\n\nB: (5, 2)\nB: (5, 2)\n\n\nA \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2)\nA \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2)\n\n\n\n\n\n\nExample 3: \nelement-wise addition\n\n\nA: (10, 1)\nA: (10, 1)\n\n\nB: (10, 1)\nB: (10, 1)\n\n\nA + B = (10, 1)\nA + B = (10, 1)\n\n\n\n\n\n\n\n\n\n\n\n\nStep 7: Train Model\n\u00b6\n\n\n\n\n7 step process for training models\n\n\n\n\nProcess \n\n\nConvert inputs/labels to tensors with gradients\n\n\nClear gradient buffets\n\n\nGet output given inputs\n\n\nGet loss\n\n\nGet gradients w.r.t. parameters\n\n\nUpdate parameters using gradients\n\n\nparameters = parameters - learning_rate * parameters_gradients\n\n\n\n\n\n\nREPEAT\n\n\n\n\n\n\n\n\niter\n \n=\n \n0\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nnum_epochs\n):\n\n    \nfor\n \ni\n,\n \n(\nimages\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n        \n# Load images as Variable\n\n        \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n        \nlabels\n \n=\n \nlabels\n\n\n        \n# Clear gradients w.r.t. parameters\n\n        \noptimizer\n.\nzero_grad\n()\n\n\n        \n# Forward pass to get output/logits\n\n        \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n\n        \n# Calculate Loss: softmax --> cross entropy loss\n\n        \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n        \n# Getting gradients w.r.t. parameters\n\n        \nloss\n.\nbackward\n()\n\n\n        \n# Updating parameters\n\n        \noptimizer\n.\nstep\n()\n\n\n        \niter\n \n+=\n \n1\n\n\n        \nif\n \niter\n \n%\n \n500\n \n==\n \n0\n:\n\n            \n# Calculate Accuracy         \n\n            \ncorrect\n \n=\n \n0\n\n            \ntotal\n \n=\n \n0\n\n            \n# Iterate through test dataset\n\n            \nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n                \n# Load images to a Torch Variable\n\n                \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n\n                \n# Forward pass only to get logits/output\n\n                \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n\n                \n# Get predictions from the maximum value\n\n                \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n                \n# Total number of labels\n\n                \ntotal\n \n+=\n \nlabels\n.\nsize\n(\n0\n)\n\n\n                \n# Total correct predictions\n\n                \ncorrect\n \n+=\n \n(\npredicted\n \n==\n \nlabels\n)\n.\nsum\n()\n\n\n            \naccuracy\n \n=\n \n100\n \n*\n \ncorrect\n \n/\n \ntotal\n\n\n            \n# Print Loss\n\n            \nprint\n(\n'Iteration: {}. Loss: {}. Accuracy: {}'\n.\nformat\n(\niter\n,\n \nloss\n.\nitem\n(),\n \naccuracy\n))\n\n\n\n\n\n\n\nIteration\n:\n \n500.\n \nLoss\n:\n \n1.8513233661651611\n.\n \nAccuracy\n:\n \n70\n\n\nIteration\n:\n \n1000.\n \nLoss\n:\n \n1.5732524394989014\n.\n \nAccuracy\n:\n \n77\n\n\nIteration\n:\n \n1500.\n \nLoss\n:\n \n1.3840199708938599\n.\n \nAccuracy\n:\n \n79\n\n\nIteration\n:\n \n2000.\n \nLoss\n:\n \n1.1711134910583496\n.\n \nAccuracy\n:\n \n81\n\n\nIteration\n:\n \n2500.\n \nLoss\n:\n \n1.1094708442687988\n.\n \nAccuracy\n:\n \n82\n\n\nIteration\n:\n \n3000.\n \nLoss\n:\n \n1.002761721611023\n.\n \nAccuracy\n:\n \n82\n\n\n\n\n\nBreak Down Accuracy Calculation\n\u00b6\n\n\n\n\nPrinting outputs of our model\n\n\nAs we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.\n\n\nThis would print out the output of the model's predictions on your notebook.\n\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'OUTPUTS'\n)\n\n        \nprint\n(\noutputs\n)\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n\n\n\n\n\nOUTPUTS\n\n\ntensor\n([[\n-\n0.4181\n,\n \n-\n1.0784\n,\n \n-\n0.4840\n,\n \n-\n0.0985\n,\n \n-\n0.2394\n,\n \n-\n0.1801\n,\n \n-\n1.1639\n,\n\n          \n2.9352\n,\n \n-\n0.1552\n,\n  \n0.8852\n],\n\n        \n[\n \n0.5117\n,\n \n-\n0.1099\n,\n  \n1.5295\n,\n  \n0.8863\n,\n \n-\n1.8813\n,\n  \n0.5967\n,\n  \n1.3632\n,\n\n         \n-\n1.8977\n,\n  \n0.4183\n,\n \n-\n1.4990\n],\n\n        \n[\n-\n1.0126\n,\n  \n2.4112\n,\n  \n0.2373\n,\n  \n0.0857\n,\n \n-\n0.7007\n,\n \n-\n0.2015\n,\n \n-\n0.3428\n,\n\n         \n-\n0.2548\n,\n  \n0.1659\n,\n \n-\n0.4703\n],\n\n        \n[\n \n2.8072\n,\n \n-\n2.2973\n,\n \n-\n0.0984\n,\n \n-\n0.4313\n,\n \n-\n0.9619\n,\n  \n0.8670\n,\n  \n1.2201\n,\n\n          \n0.3752\n,\n \n-\n0.2873\n,\n \n-\n0.3272\n],\n\n        \n[\n-\n0.0343\n,\n \n-\n2.0043\n,\n  \n0.5081\n,\n \n-\n0.6452\n,\n  \n1.8647\n,\n \n-\n0.6924\n,\n  \n0.1435\n,\n\n          \n0.4330\n,\n  \n0.2958\n,\n  \n1.0339\n],\n\n        \n[\n-\n1.5392\n,\n  \n2.9070\n,\n  \n0.2297\n,\n  \n0.3139\n,\n \n-\n0.6863\n,\n \n-\n0.2734\n,\n \n-\n0.8377\n,\n\n         \n-\n0.1238\n,\n  \n0.3285\n,\n \n-\n0.3004\n],\n\n        \n[\n-\n1.2037\n,\n \n-\n1.3739\n,\n \n-\n0.5947\n,\n  \n0.3530\n,\n  \n1.4205\n,\n  \n0.0593\n,\n \n-\n0.7307\n,\n\n          \n0.6642\n,\n  \n0.3937\n,\n  \n0.8004\n],\n\n        \n[\n-\n1.4439\n,\n \n-\n0.3284\n,\n \n-\n0.7652\n,\n \n-\n0.0952\n,\n  \n0.9323\n,\n  \n0.3006\n,\n  \n0.0238\n,\n\n         \n-\n0.0810\n,\n  \n0.0612\n,\n  \n1.3295\n],\n\n        \n[\n \n0.5409\n,\n \n-\n0.5266\n,\n  \n0.9914\n,\n \n-\n1.2369\n,\n  \n0.6583\n,\n  \n0.0992\n,\n  \n0.8525\n,\n\n         \n-\n1.0562\n,\n  \n0.2013\n,\n  \n0.0462\n],\n\n        \n[\n-\n0.6548\n,\n \n-\n0.7253\n,\n \n-\n0.9825\n,\n \n-\n1.1663\n,\n  \n0.9076\n,\n \n-\n0.0694\n,\n \n-\n0.3708\n,\n\n          \n1.8270\n,\n  \n0.2457\n,\n  \n1.5921\n],\n\n        \n[\n \n3.2147\n,\n \n-\n1.7689\n,\n  \n0.8531\n,\n  \n1.2320\n,\n \n-\n0.8126\n,\n  \n1.1251\n,\n \n-\n0.2776\n,\n\n         \n-\n1.4244\n,\n  \n0.5930\n,\n \n-\n1.6183\n],\n\n        \n[\n \n0.7470\n,\n \n-\n0.5545\n,\n  \n1.0251\n,\n  \n0.0529\n,\n  \n0.4384\n,\n \n-\n0.5934\n,\n  \n0.7666\n,\n\n         \n-\n1.0084\n,\n  \n0.5313\n,\n \n-\n0.3465\n],\n\n        \n[\n-\n0.7916\n,\n \n-\n1.7064\n,\n \n-\n0.7805\n,\n \n-\n1.1588\n,\n  \n1.3284\n,\n \n-\n0.1708\n,\n \n-\n0.2092\n,\n\n          \n0.9495\n,\n  \n0.1033\n,\n  \n2.0208\n],\n\n        \n[\n \n3.0602\n,\n \n-\n2.3578\n,\n \n-\n0.2576\n,\n \n-\n0.2198\n,\n \n-\n0.2372\n,\n  \n0.9765\n,\n \n-\n0.1514\n,\n\n         \n-\n0.5380\n,\n  \n0.7970\n,\n  \n0.1374\n],\n\n        \n[\n-\n1.2613\n,\n  \n2.8594\n,\n \n-\n0.0874\n,\n  \n0.1974\n,\n \n-\n1.2018\n,\n \n-\n0.0064\n,\n \n-\n0.0923\n,\n\n         \n-\n0.2142\n,\n  \n0.2575\n,\n \n-\n0.3218\n],\n\n        \n[\n \n0.4348\n,\n \n-\n0.7216\n,\n  \n0.0021\n,\n  \n1.2864\n,\n \n-\n0.5062\n,\n  \n0.7761\n,\n \n-\n0.3236\n,\n\n         \n-\n0.5667\n,\n  \n0.5431\n,\n \n-\n0.7781\n],\n\n        \n[\n-\n0.2157\n,\n \n-\n2.0200\n,\n  \n0.1829\n,\n \n-\n0.6882\n,\n  \n1.3815\n,\n \n-\n0.7609\n,\n \n-\n0.0902\n,\n\n          \n0.8647\n,\n  \n0.3679\n,\n  \n1.8843\n],\n\n        \n[\n \n0.0950\n,\n \n-\n1.5009\n,\n \n-\n0.6347\n,\n  \n0.3662\n,\n \n-\n0.4679\n,\n \n-\n0.0359\n,\n \n-\n0.7671\n,\n\n          \n2.7155\n,\n \n-\n0.3991\n,\n  \n0.5737\n],\n\n        \n[\n-\n0.7005\n,\n \n-\n0.5366\n,\n \n-\n0.0434\n,\n  \n1.1289\n,\n \n-\n0.5873\n,\n  \n0.2555\n,\n  \n0.8187\n,\n\n         \n-\n0.6557\n,\n  \n0.1241\n,\n \n-\n0.4297\n],\n\n        \n[\n-\n1.0635\n,\n \n-\n1.5991\n,\n \n-\n0.4677\n,\n \n-\n0.1231\n,\n  \n2.0445\n,\n  \n0.1128\n,\n \n-\n0.1825\n,\n\n          \n0.1075\n,\n  \n0.0348\n,\n  \n1.4317\n],\n\n        \n[\n-\n1.0319\n,\n \n-\n0.1595\n,\n \n-\n1.3415\n,\n  \n0.1095\n,\n  \n0.5339\n,\n  \n0.1973\n,\n \n-\n1.3272\n,\n\n          \n1.5765\n,\n  \n0.4784\n,\n  \n1.4176\n],\n\n        \n[\n-\n0.4928\n,\n \n-\n1.5653\n,\n \n-\n0.0672\n,\n  \n0.3325\n,\n  \n0.5359\n,\n  \n0.5368\n,\n  \n2.1542\n,\n\n         \n-\n1.4276\n,\n  \n0.3605\n,\n  \n0.0587\n],\n\n        \n[\n-\n0.4761\n,\n  \n0.2958\n,\n  \n0.6597\n,\n \n-\n0.2658\n,\n  \n1.1279\n,\n \n-\n1.0676\n,\n  \n1.2506\n,\n\n         \n-\n0.2059\n,\n \n-\n0.1489\n,\n  \n0.1051\n],\n\n        \n[\n-\n0.0764\n,\n \n-\n0.9274\n,\n \n-\n0.6838\n,\n  \n0.3464\n,\n \n-\n0.2656\n,\n  \n1.4099\n,\n  \n0.4486\n,\n\n         \n-\n0.9527\n,\n  \n0.5682\n,\n  \n0.0156\n],\n\n        \n[\n-\n0.6900\n,\n \n-\n0.9611\n,\n  \n0.1395\n,\n \n-\n0.0079\n,\n  \n1.5424\n,\n \n-\n0.3208\n,\n \n-\n0.2682\n,\n\n          \n0.3586\n,\n \n-\n0.2771\n,\n  \n1.0389\n],\n\n        \n[\n \n4.3606\n,\n \n-\n2.8621\n,\n  \n0.6310\n,\n \n-\n0.9657\n,\n \n-\n0.2486\n,\n  \n1.2009\n,\n  \n1.1873\n,\n\n         \n-\n0.8255\n,\n \n-\n0.2103\n,\n \n-\n1.2172\n],\n\n        \n[\n-\n0.1000\n,\n \n-\n1.4268\n,\n \n-\n0.4627\n,\n \n-\n0.1041\n,\n  \n0.2959\n,\n \n-\n0.1392\n,\n \n-\n0.6855\n,\n\n          \n1.8622\n,\n \n-\n0.2580\n,\n  \n1.1347\n],\n\n        \n[\n-\n0.3625\n,\n \n-\n2.1323\n,\n \n-\n0.2224\n,\n \n-\n0.8754\n,\n  \n2.4684\n,\n  \n0.0295\n,\n  \n0.1161\n,\n\n         \n-\n0.2660\n,\n  \n0.3037\n,\n  \n1.4570\n],\n\n        \n[\n \n2.8688\n,\n \n-\n2.4517\n,\n  \n0.1782\n,\n  \n1.1149\n,\n \n-\n1.0898\n,\n  \n1.1062\n,\n \n-\n0.0681\n,\n\n         \n-\n0.5697\n,\n  \n0.8888\n,\n \n-\n0.6965\n],\n\n        \n[\n-\n1.0429\n,\n  \n1.4446\n,\n \n-\n0.3349\n,\n  \n0.1254\n,\n \n-\n0.5017\n,\n  \n0.2286\n,\n  \n0.2328\n,\n\n         \n-\n0.3290\n,\n  \n0.3949\n,\n \n-\n0.2586\n],\n\n        \n[\n-\n0.8476\n,\n \n-\n0.0004\n,\n \n-\n1.1003\n,\n  \n2.2806\n,\n \n-\n1.2226\n,\n  \n0.9251\n,\n \n-\n0.3165\n,\n\n          \n0.4957\n,\n  \n0.0690\n,\n  \n0.0232\n],\n\n        \n[\n-\n0.9108\n,\n  \n1.1355\n,\n \n-\n0.2715\n,\n  \n0.2233\n,\n \n-\n0.3681\n,\n  \n0.1442\n,\n \n-\n0.0001\n,\n\n         \n-\n0.0174\n,\n  \n0.1454\n,\n  \n0.2286\n],\n\n        \n[\n-\n1.0663\n,\n \n-\n0.8466\n,\n \n-\n0.7147\n,\n  \n2.5685\n,\n \n-\n0.2090\n,\n  \n1.2993\n,\n \n-\n0.3057\n,\n\n         \n-\n0.8314\n,\n  \n0.7046\n,\n \n-\n0.0176\n],\n\n        \n[\n \n1.7013\n,\n \n-\n1.8051\n,\n  \n0.7541\n,\n \n-\n1.5248\n,\n  \n0.8972\n,\n  \n0.1518\n,\n  \n1.4876\n,\n\n         \n-\n0.8454\n,\n \n-\n0.2022\n,\n \n-\n0.2829\n],\n\n        \n[\n-\n0.8179\n,\n \n-\n0.1239\n,\n  \n0.8630\n,\n \n-\n0.2137\n,\n \n-\n0.2275\n,\n \n-\n0.5411\n,\n \n-\n1.3448\n,\n\n          \n1.7354\n,\n  \n0.7751\n,\n  \n0.6234\n],\n\n        \n[\n \n0.6515\n,\n \n-\n1.0431\n,\n  \n2.7165\n,\n  \n0.1873\n,\n \n-\n1.0623\n,\n  \n0.1286\n,\n  \n0.3597\n,\n\n         \n-\n0.2739\n,\n  \n0.3871\n,\n \n-\n1.6699\n],\n\n        \n[\n-\n0.2828\n,\n \n-\n1.4663\n,\n  \n0.1182\n,\n \n-\n0.0896\n,\n \n-\n0.3640\n,\n \n-\n0.5129\n,\n \n-\n0.4905\n,\n\n          \n2.2914\n,\n \n-\n0.2227\n,\n  \n0.9463\n],\n\n        \n[\n-\n1.2596\n,\n  \n2.0468\n,\n \n-\n0.4405\n,\n \n-\n0.0411\n,\n \n-\n0.8073\n,\n  \n0.0490\n,\n \n-\n0.0604\n,\n\n         \n-\n0.1206\n,\n  \n0.3504\n,\n \n-\n0.1059\n],\n\n        \n[\n \n0.6089\n,\n  \n0.5885\n,\n  \n0.7898\n,\n  \n1.1318\n,\n \n-\n1.9008\n,\n  \n0.5875\n,\n  \n0.4227\n,\n\n         \n-\n1.1815\n,\n  \n0.5652\n,\n \n-\n1.3590\n],\n\n        \n[\n-\n1.4551\n,\n  \n2.9537\n,\n \n-\n0.2805\n,\n  \n0.2372\n,\n \n-\n1.4180\n,\n  \n0.0297\n,\n \n-\n0.1515\n,\n\n         \n-\n0.6111\n,\n  \n0.6140\n,\n \n-\n0.3354\n],\n\n        \n[\n-\n0.7182\n,\n  \n1.6778\n,\n  \n0.0553\n,\n  \n0.0461\n,\n \n-\n0.5446\n,\n \n-\n0.0338\n,\n \n-\n0.0215\n,\n\n         \n-\n0.0881\n,\n  \n0.1506\n,\n \n-\n0.2107\n],\n\n        \n[\n-\n0.8027\n,\n \n-\n0.7854\n,\n \n-\n0.1275\n,\n \n-\n0.3177\n,\n \n-\n0.1600\n,\n \n-\n0.1964\n,\n \n-\n0.6084\n,\n\n          \n2.1285\n,\n \n-\n0.1815\n,\n  \n1.1911\n],\n\n        \n[\n-\n2.0656\n,\n \n-\n0.4959\n,\n \n-\n0.1154\n,\n \n-\n0.1363\n,\n  \n2.2426\n,\n \n-\n0.7441\n,\n \n-\n0.8413\n,\n\n          \n0.4675\n,\n  \n0.3269\n,\n  \n1.7279\n],\n\n        \n[\n-\n0.3004\n,\n  \n1.0166\n,\n  \n1.1175\n,\n \n-\n0.0618\n,\n \n-\n0.0937\n,\n \n-\n0.4221\n,\n  \n0.1943\n,\n\n         \n-\n1.1020\n,\n  \n0.3670\n,\n \n-\n0.4683\n],\n\n        \n[\n-\n1.0720\n,\n  \n0.2252\n,\n  \n0.0175\n,\n  \n1.3644\n,\n \n-\n0.7409\n,\n  \n0.4655\n,\n  \n0.5439\n,\n\n          \n0.0380\n,\n  \n0.1279\n,\n \n-\n0.2302\n],\n\n        \n[\n \n0.2409\n,\n \n-\n1.2622\n,\n \n-\n0.6336\n,\n  \n1.8240\n,\n \n-\n0.5951\n,\n  \n1.3408\n,\n  \n0.2130\n,\n\n         \n-\n1.3789\n,\n  \n0.8363\n,\n \n-\n0.2101\n],\n\n        \n[\n-\n1.3849\n,\n  \n0.3773\n,\n \n-\n0.0585\n,\n  \n0.6896\n,\n \n-\n0.0998\n,\n  \n0.2804\n,\n  \n0.0696\n,\n\n         \n-\n0.2529\n,\n  \n0.3143\n,\n  \n0.3409\n],\n\n        \n[\n-\n0.9103\n,\n \n-\n0.1578\n,\n  \n1.6673\n,\n \n-\n0.4817\n,\n  \n0.4088\n,\n \n-\n0.5484\n,\n  \n0.6103\n,\n\n         \n-\n0.2287\n,\n \n-\n0.0665\n,\n  \n0.0055\n],\n\n        \n[\n-\n1.1692\n,\n \n-\n2.8531\n,\n \n-\n1.2499\n,\n \n-\n0.0257\n,\n  \n2.8580\n,\n  \n0.2616\n,\n \n-\n0.7122\n,\n\n         \n-\n0.0551\n,\n  \n0.8112\n,\n  \n2.3233\n],\n\n        \n[\n-\n0.2790\n,\n \n-\n1.9494\n,\n  \n0.6096\n,\n \n-\n0.5653\n,\n  \n2.2792\n,\n \n-\n1.0687\n,\n  \n0.1634\n,\n\n          \n0.3122\n,\n  \n0.1053\n,\n  \n1.0884\n],\n\n        \n[\n \n0.1267\n,\n \n-\n1.2297\n,\n \n-\n0.1315\n,\n  \n0.2428\n,\n \n-\n0.5436\n,\n  \n0.4123\n,\n  \n2.3060\n,\n\n         \n-\n0.9278\n,\n \n-\n0.1528\n,\n \n-\n0.4224\n],\n\n        \n[\n-\n0.0235\n,\n \n-\n0.9137\n,\n \n-\n0.1457\n,\n  \n1.6858\n,\n \n-\n0.7552\n,\n  \n0.7293\n,\n  \n0.2510\n,\n\n         \n-\n0.3955\n,\n \n-\n0.2187\n,\n \n-\n0.1505\n],\n\n        \n[\n \n0.5643\n,\n \n-\n1.2783\n,\n \n-\n1.4149\n,\n  \n0.0304\n,\n  \n0.8375\n,\n  \n1.5018\n,\n  \n0.0338\n,\n\n         \n-\n0.3875\n,\n \n-\n0.0117\n,\n  \n0.5751\n],\n\n        \n[\n \n0.2926\n,\n \n-\n0.7486\n,\n \n-\n0.3238\n,\n  \n1.0384\n,\n  \n0.0308\n,\n  \n0.6792\n,\n \n-\n0.0170\n,\n\n         \n-\n0.5797\n,\n  \n0.2819\n,\n \n-\n0.3510\n],\n\n        \n[\n \n0.1219\n,\n \n-\n0.5862\n,\n  \n1.5817\n,\n \n-\n0.1297\n,\n  \n0.4730\n,\n \n-\n0.9171\n,\n  \n0.7886\n,\n\n         \n-\n0.7022\n,\n \n-\n0.0501\n,\n \n-\n0.2812\n],\n\n        \n[\n \n1.7587\n,\n \n-\n2.4511\n,\n \n-\n0.7369\n,\n  \n0.4082\n,\n \n-\n0.6426\n,\n  \n1.1784\n,\n  \n0.6052\n,\n\n         \n-\n0.7178\n,\n  \n1.6161\n,\n \n-\n0.2220\n],\n\n        \n[\n-\n0.1267\n,\n \n-\n2.6719\n,\n  \n0.0505\n,\n \n-\n0.4972\n,\n  \n2.9027\n,\n \n-\n0.1461\n,\n  \n0.2807\n,\n\n         \n-\n0.2921\n,\n  \n0.2231\n,\n  \n1.1327\n],\n\n        \n[\n-\n0.9892\n,\n  \n2.4401\n,\n  \n0.1274\n,\n  \n0.2838\n,\n \n-\n0.7535\n,\n \n-\n0.1684\n,\n \n-\n0.6493\n,\n\n         \n-\n0.1908\n,\n  \n0.2290\n,\n \n-\n0.2150\n],\n\n        \n[\n-\n0.2071\n,\n \n-\n2.1351\n,\n \n-\n0.9191\n,\n \n-\n0.9309\n,\n  \n1.7747\n,\n \n-\n0.3046\n,\n  \n0.0183\n,\n\n          \n1.0136\n,\n \n-\n0.1016\n,\n  \n2.1288\n],\n\n        \n[\n-\n0.0103\n,\n  \n0.3280\n,\n \n-\n0.6974\n,\n \n-\n0.2504\n,\n  \n0.3187\n,\n  \n0.4390\n,\n \n-\n0.1879\n,\n\n          \n0.3954\n,\n  \n0.2332\n,\n \n-\n0.1971\n],\n\n        \n[\n-\n0.2280\n,\n \n-\n1.6754\n,\n \n-\n0.7438\n,\n  \n0.5078\n,\n  \n0.2544\n,\n \n-\n0.1020\n,\n \n-\n0.2503\n,\n\n          \n2.0799\n,\n \n-\n0.5033\n,\n  \n0.5890\n],\n\n        \n[\n \n0.3972\n,\n \n-\n0.9369\n,\n  \n1.2696\n,\n \n-\n1.6713\n,\n \n-\n0.4159\n,\n \n-\n0.0221\n,\n  \n0.6489\n,\n\n         \n-\n0.4777\n,\n  \n1.2497\n,\n  \n0.3931\n],\n\n        \n[\n-\n0.7566\n,\n \n-\n0.8230\n,\n \n-\n0.0785\n,\n \n-\n0.3083\n,\n  \n0.7821\n,\n  \n0.1880\n,\n  \n0.1037\n,\n\n         \n-\n0.0956\n,\n  \n0.4219\n,\n  \n1.0798\n],\n\n        \n[\n-\n1.0328\n,\n \n-\n0.1700\n,\n  \n1.3806\n,\n  \n0.5445\n,\n \n-\n0.2624\n,\n \n-\n0.0780\n,\n \n-\n0.3595\n,\n\n         \n-\n0.6253\n,\n  \n0.4309\n,\n  \n0.1813\n],\n\n        \n[\n-\n1.0360\n,\n \n-\n0.4704\n,\n  \n0.1948\n,\n \n-\n0.7066\n,\n  \n0.6600\n,\n \n-\n0.4633\n,\n \n-\n0.3602\n,\n\n          \n1.7494\n,\n  \n0.1522\n,\n  \n0.6086\n],\n\n        \n[\n-\n1.2032\n,\n \n-\n0.7903\n,\n \n-\n0.5754\n,\n  \n0.4722\n,\n  \n0.6068\n,\n  \n0.5752\n,\n  \n0.2151\n,\n\n         \n-\n0.2495\n,\n  \n0.3420\n,\n  \n0.9278\n],\n\n        \n[\n \n0.2247\n,\n \n-\n0.1361\n,\n  \n0.9374\n,\n \n-\n0.1543\n,\n  \n0.4921\n,\n \n-\n0.6553\n,\n  \n0.5885\n,\n\n          \n0.2617\n,\n \n-\n0.2216\n,\n \n-\n0.3736\n],\n\n        \n[\n-\n0.2867\n,\n \n-\n1.4486\n,\n  \n0.6658\n,\n \n-\n0.8755\n,\n  \n2.3195\n,\n \n-\n0.7627\n,\n \n-\n0.2132\n,\n\n          \n0.2488\n,\n  \n0.3484\n,\n  \n1.0860\n],\n\n        \n[\n-\n1.4031\n,\n \n-\n0.4518\n,\n \n-\n0.3181\n,\n  \n2.8268\n,\n \n-\n0.5371\n,\n  \n1.0154\n,\n \n-\n0.9247\n,\n\n         \n-\n0.7385\n,\n  \n1.1031\n,\n  \n0.0422\n],\n\n        \n[\n \n2.8604\n,\n \n-\n1.5413\n,\n  \n0.6241\n,\n \n-\n0.8017\n,\n \n-\n1.4104\n,\n  \n0.6314\n,\n  \n0.4614\n,\n\n         \n-\n0.0218\n,\n \n-\n0.3411\n,\n \n-\n0.2609\n],\n\n        \n[\n \n0.2113\n,\n \n-\n1.2348\n,\n \n-\n0.8535\n,\n \n-\n0.1041\n,\n \n-\n0.2703\n,\n \n-\n0.1294\n,\n \n-\n0.7057\n,\n\n          \n2.7552\n,\n \n-\n0.4429\n,\n  \n0.4517\n],\n\n        \n[\n \n4.5191\n,\n \n-\n2.7407\n,\n  \n1.1091\n,\n  \n0.3975\n,\n \n-\n0.9456\n,\n  \n1.2277\n,\n  \n0.3616\n,\n\n         \n-\n1.6564\n,\n  \n0.5063\n,\n \n-\n1.4274\n],\n\n        \n[\n \n1.4615\n,\n \n-\n1.0765\n,\n  \n1.8388\n,\n  \n1.5006\n,\n \n-\n1.2351\n,\n  \n0.2781\n,\n  \n0.2830\n,\n\n         \n-\n0.8491\n,\n  \n0.2222\n,\n \n-\n1.7779\n],\n\n        \n[\n-\n1.2160\n,\n  \n0.8502\n,\n  \n0.2413\n,\n \n-\n0.0798\n,\n \n-\n0.7880\n,\n \n-\n0.4286\n,\n \n-\n0.8060\n,\n\n          \n0.7194\n,\n  \n1.2663\n,\n  \n0.6412\n],\n\n        \n[\n-\n1.3318\n,\n  \n2.3388\n,\n \n-\n0.4003\n,\n \n-\n0.1094\n,\n \n-\n1.0285\n,\n  \n0.1021\n,\n \n-\n0.0388\n,\n\n         \n-\n0.0497\n,\n  \n0.5137\n,\n \n-\n0.2507\n],\n\n        \n[\n-\n1.7853\n,\n  \n0.5884\n,\n \n-\n0.6108\n,\n \n-\n0.5557\n,\n  \n0.8696\n,\n \n-\n0.6226\n,\n \n-\n0.7983\n,\n\n          \n1.7169\n,\n \n-\n0.0145\n,\n  \n0.8231\n],\n\n        \n[\n-\n0.1739\n,\n  \n0.1562\n,\n \n-\n0.2933\n,\n  \n2.3195\n,\n \n-\n0.9480\n,\n  \n1.2019\n,\n \n-\n0.4834\n,\n\n         \n-\n1.0567\n,\n  \n0.5685\n,\n \n-\n0.6841\n],\n\n        \n[\n-\n0.7920\n,\n \n-\n0.3339\n,\n  \n0.7452\n,\n \n-\n0.6529\n,\n \n-\n0.3307\n,\n \n-\n0.6092\n,\n \n-\n0.0950\n,\n\n          \n1.7311\n,\n \n-\n0.3481\n,\n  \n0.3801\n],\n\n        \n[\n-\n1.7810\n,\n  \n1.0676\n,\n \n-\n0.7611\n,\n  \n0.3658\n,\n \n-\n0.0431\n,\n \n-\n0.1012\n,\n \n-\n0.6048\n,\n\n          \n0.3089\n,\n  \n0.9998\n,\n  \n0.7164\n],\n\n        \n[\n-\n0.5856\n,\n \n-\n0.5261\n,\n \n-\n0.4859\n,\n \n-\n1.0551\n,\n \n-\n0.1838\n,\n \n-\n0.2144\n,\n \n-\n1.2599\n,\n\n          \n3.3891\n,\n  \n0.4691\n,\n  \n0.7566\n],\n\n        \n[\n-\n0.4984\n,\n \n-\n1.7770\n,\n \n-\n1.1998\n,\n \n-\n0.1075\n,\n  \n1.0882\n,\n  \n0.4539\n,\n \n-\n0.5651\n,\n\n          \n1.4381\n,\n \n-\n0.5678\n,\n  \n1.7479\n],\n\n        \n[\n \n0.2938\n,\n \n-\n1.8536\n,\n  \n0.4259\n,\n \n-\n0.5429\n,\n  \n0.0066\n,\n  \n0.4120\n,\n  \n2.3793\n,\n\n         \n-\n0.3666\n,\n \n-\n0.2604\n,\n  \n0.0382\n],\n\n        \n[\n-\n0.4080\n,\n \n-\n0.9851\n,\n  \n4.0264\n,\n  \n0.1099\n,\n \n-\n0.1766\n,\n \n-\n1.1557\n,\n  \n0.6419\n,\n\n         \n-\n0.8147\n,\n  \n0.7535\n,\n \n-\n1.1452\n],\n\n        \n[\n-\n0.4636\n,\n \n-\n1.7323\n,\n \n-\n0.6433\n,\n \n-\n0.0274\n,\n  \n0.7227\n,\n \n-\n0.1799\n,\n \n-\n0.9336\n,\n\n          \n2.1881\n,\n \n-\n0.2073\n,\n  \n1.6522\n],\n\n        \n[\n-\n0.9617\n,\n \n-\n0.0348\n,\n \n-\n0.3980\n,\n \n-\n0.4738\n,\n  \n0.7790\n,\n  \n0.4671\n,\n \n-\n0.6115\n,\n\n         \n-\n0.7067\n,\n  \n1.3036\n,\n  \n0.4923\n],\n\n        \n[\n-\n1.0151\n,\n \n-\n2.5385\n,\n \n-\n0.6072\n,\n  \n0.2902\n,\n  \n3.1570\n,\n  \n0.1062\n,\n \n-\n0.2169\n,\n\n         \n-\n0.4491\n,\n  \n0.6326\n,\n  \n1.6829\n],\n\n        \n[\n-\n1.8852\n,\n  \n0.6066\n,\n \n-\n0.2840\n,\n \n-\n0.4475\n,\n \n-\n0.1147\n,\n \n-\n0.7858\n,\n \n-\n1.1805\n,\n\n          \n3.0723\n,\n  \n0.3960\n,\n  \n0.9720\n],\n\n        \n[\n \n0.0344\n,\n \n-\n1.4878\n,\n \n-\n0.9675\n,\n  \n1.9649\n,\n \n-\n0.3146\n,\n  \n1.2183\n,\n  \n0.6730\n,\n\n         \n-\n0.3650\n,\n  \n0.0646\n,\n \n-\n0.0898\n],\n\n        \n[\n-\n0.2118\n,\n \n-\n2.0350\n,\n  \n0.9917\n,\n \n-\n0.8993\n,\n  \n1.2334\n,\n \n-\n0.6723\n,\n  \n2.5847\n,\n\n         \n-\n0.0454\n,\n \n-\n0.4149\n,\n  \n0.3927\n],\n\n        \n[\n-\n1.7365\n,\n  \n3.0447\n,\n  \n0.5115\n,\n  \n0.0786\n,\n \n-\n0.7544\n,\n \n-\n0.2158\n,\n \n-\n0.4876\n,\n\n         \n-\n0.2891\n,\n  \n0.5089\n,\n \n-\n0.6719\n],\n\n        \n[\n \n0.3652\n,\n \n-\n0.5457\n,\n \n-\n0.1167\n,\n  \n2.9056\n,\n \n-\n1.1622\n,\n  \n0.8192\n,\n \n-\n1.3245\n,\n\n         \n-\n0.6414\n,\n  \n0.8097\n,\n \n-\n0.4958\n],\n\n        \n[\n-\n0.8755\n,\n \n-\n0.6983\n,\n  \n0.2208\n,\n \n-\n0.6463\n,\n  \n0.5276\n,\n  \n0.1145\n,\n  \n2.7229\n,\n\n         \n-\n1.0316\n,\n  \n0.1905\n,\n  \n0.2090\n],\n\n        \n[\n-\n0.9702\n,\n  \n0.1265\n,\n \n-\n0.0007\n,\n \n-\n0.5106\n,\n  \n0.4970\n,\n \n-\n0.0804\n,\n  \n0.0017\n,\n\n          \n0.0607\n,\n  \n0.6164\n,\n  \n0.4490\n],\n\n        \n[\n-\n0.8271\n,\n \n-\n0.6822\n,\n \n-\n0.7434\n,\n  \n2.6457\n,\n \n-\n1.6143\n,\n  \n1.1486\n,\n \n-\n1.0705\n,\n\n          \n0.5611\n,\n  \n0.6422\n,\n  \n0.1250\n],\n\n        \n[\n-\n1.9979\n,\n  \n1.8175\n,\n \n-\n0.1658\n,\n \n-\n0.0343\n,\n \n-\n0.6292\n,\n  \n0.1774\n,\n  \n0.3150\n,\n\n         \n-\n0.4633\n,\n  \n0.9266\n,\n  \n0.0252\n],\n\n        \n[\n-\n0.9039\n,\n \n-\n0.6030\n,\n \n-\n0.2173\n,\n \n-\n1.1768\n,\n  \n2.3198\n,\n \n-\n0.5072\n,\n  \n0.3418\n,\n\n         \n-\n0.1551\n,\n  \n0.1282\n,\n  \n1.4250\n],\n\n        \n[\n-\n0.9891\n,\n  \n0.5212\n,\n \n-\n0.4518\n,\n  \n0.3267\n,\n \n-\n0.0759\n,\n  \n0.3826\n,\n \n-\n0.0341\n,\n\n          \n0.0382\n,\n  \n0.2451\n,\n  \n0.3658\n],\n\n        \n[\n-\n2.1217\n,\n  \n1.5102\n,\n \n-\n0.7828\n,\n  \n0.3554\n,\n \n-\n0.4192\n,\n \n-\n0.0772\n,\n  \n0.0578\n,\n\n          \n0.8070\n,\n  \n0.1701\n,\n  \n0.5880\n],\n\n        \n[\n \n1.0665\n,\n \n-\n1.3826\n,\n  \n0.6243\n,\n \n-\n0.8096\n,\n \n-\n0.4227\n,\n  \n0.5925\n,\n  \n1.8112\n,\n\n         \n-\n0.9946\n,\n  \n0.2010\n,\n \n-\n0.7731\n],\n\n        \n[\n-\n1.1263\n,\n \n-\n1.7484\n,\n  \n0.0041\n,\n \n-\n0.5439\n,\n  \n1.7242\n,\n \n-\n0.9475\n,\n \n-\n0.3835\n,\n\n          \n0.8452\n,\n  \n0.3077\n,\n  \n2.2689\n]])\n\n\n\n\n\n\n\nPrinting output size\n\n\nThis produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting.\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'OUTPUTS'\n)\n\n        \nprint\n(\noutputs\n.\nsize\n())\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n\n\n\n\nOUTPUTS\n\n\ntorch\n.\nSize\n([\n100\n,\n \n10\n])\n\n\n\n\n\n\n\nPrinting one output\n\n\nThis would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.\n\n\nnumber 0: -0.4181\n\n number 1: -1.0784\n\n...\n\n number 7: 2.9352\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'OUTPUTS'\n)\n\n        \nprint\n(\noutputs\n[\n0\n,\n \n:])\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n\n\n\n\nOUTPUTS\ntensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,\n         2.9352, -0.1552,  0.8852])\n\n\n\n\n\n\nPrinting prediction output\n\n\nBecause our output is of size 100 (our batch size), our prediction size would also of the size 100.\n\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'PREDICTION'\n)\n\n        \nprint\n(\npredicted\n.\nsize\n())\n\n\n\n\n\n\n\nPREDICTION\n\n\ntorch\n.\nSize\n([\n100\n])\n\n\n\n\n\n\n\nPrint prediction value\n\n\nWe are printing our prediction which as verified above, should be digit 7.\n\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'PREDICTION'\n)\n\n        \nprint\n(\npredicted\n[\n0\n])\n\n\n\n\n\n\n\nPREDICTION\n\n\ntensor\n(\n7\n)\n\n\n\n\n\n\n\nPrint prediction, label and label size\n\n\nWe are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!\n\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'PREDICTION'\n)\n\n        \nprint\n(\npredicted\n[\n0\n])\n\n\n        \nprint\n(\n'LABEL SIZE'\n)\n\n        \nprint\n(\nlabels\n.\nsize\n())\n\n\n        \nprint\n(\n'LABEL FOR IMAGE 0'\n)\n\n        \nprint\n(\nlabels\n[\n0\n])\n\n\n\n\n\n\n\nPREDICTION\n\n\ntensor\n(\n7\n)\n\n\n\nLABEL\n \nSIZE\n\n\ntorch\n.\nSize\n([\n100\n])\n\n\n\nLABEL\n \nFOR\n \nIMAGE\n \n0\n\n\ntensor\n(\n7\n)\n\n\n\n\n\n\n\nPrint second prediction and ground truth\n\n\nAgain, the prediction is correct. Naturally, as our model is quite competent in this simple task.\n\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n    \nif\n \niter_test\n \n==\n \n1\n:\n\n        \nprint\n(\n'PREDICTION'\n)\n\n        \nprint\n(\npredicted\n[\n1\n])\n\n\n        \nprint\n(\n'LABEL SIZE'\n)\n\n        \nprint\n(\nlabels\n.\nsize\n())\n\n\n        \nprint\n(\n'LABEL FOR IMAGE 1'\n)\n\n        \nprint\n(\nlabels\n[\n1\n])\n\n\n\n\n\n\n\nPREDICTION\n\n\ntensor\n(\n2\n)\n\n\n\nLABEL\n \nSIZE\n\n\ntorch\n.\nSize\n([\n100\n])\n\n\n\nLABEL\n \nFOR\n \nIMAGE\n \n1\n\n\ntensor\n(\n2\n)\n\n\n\n\n\n\n\nPrint accuracy\n\n\nNow we know what each object represents, we can understand how we arrived at our accuracy numbers.\n\n\nOne last thing to note is that \ncorrect.item()\n has this syntax is because \ncorrect\n is a PyTorch tensor and to get the value to compute with \ntotal\n which is an integer, we need to do this.\n\ncorrect\n \n=\n \n0\n\n\ntotal\n \n=\n \n0\n\n\niter_test\n \n=\n \n0\n\n\nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n    \niter_test\n \n+=\n \n1\n\n    \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n    \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n    \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n    \n# Total number of labels\n\n    \ntotal\n \n+=\n \nlabels\n.\nsize\n(\n0\n)\n\n\n    \n# Total correct predictions\n\n    \ncorrect\n \n+=\n \n(\npredicted\n \n==\n \nlabels\n)\n.\nsum\n()\n\n\n\naccuracy\n \n=\n \n100\n \n*\n \n(\ncorrect\n.\nitem\n()\n \n/\n \ntotal\n)\n\n\n\nprint\n(\naccuracy\n)\n\n\n\n\n\n\n82.94\n\n\n\n\n\n\n\nExplanation of Python's .sum() function\n\n\nPython's .sum() function allows you to do a comparison between two matrices and sum the ones that return \nTrue\n or in our case, those predictions that match actual labels (correct predictions).\n\n\n# Explaining .sum() python built-in function\n\n\n# correct += (predicted == labels).sum()\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\na\n \n=\n \nnp\n.\nones\n((\n10\n))\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \nnp\n.\nones\n((\n10\n))\n\n\nprint\n(\nb\n)\n\n\n\nprint\n(\na\n \n==\n \nb\n)\n\n\n\nprint\n((\na\n \n==\n \nb\n)\n.\nsum\n())\n\n\n\n\n\n\n\n# matrix a\n\n\n[\n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n]\n\n\n\n# matrix b\n\n\n[\n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n \n1.\n]\n\n\n\n# boolean array\n\n\n[\n \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n  \nTrue\n]\n\n\n\n# number of elementswhere a matches b\n\n\n10\n\n\n\n\n\nSaving Model\n\u00b6\n\n\n\n\nSaving PyTorch model\n\n\nThis is how you save your model. Feel free to just change \nsave_model = True\n to save your model\n\nsave_model\n \n=\n \nFalse\n\n\nif\n \nsave_model\n \nis\n \nTrue\n:\n\n    \n# Saves only parameters\n\n    \ntorch\n.\nsave\n(\nmodel\n.\nstate_dict\n(),\n \n'awesome_model.pkl'\n)\n\n\n\n\n\n\nBuilding a Logistic Regression Model with PyTorch (GPU)\n\u00b6\n\n\n\n\nCPU version\n\n\nThe usual 7-step process, getting repetitive by now which we like. \n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorchvision.transforms\n \nas\n \ntransforms\n\n\nimport\n \ntorchvision.datasets\n \nas\n \ndsets\n\n\n\n'''\n\n\nSTEP 1: LOADING DATASET\n\n\n'''\n\n\n\ntrain_dataset\n \n=\n \ndsets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \n                            \ntrain\n=\nTrue\n,\n \n                            \ntransform\n=\ntransforms\n.\nToTensor\n(),\n\n                            \ndownload\n=\nTrue\n)\n\n\n\ntest_dataset\n \n=\n \ndsets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \n                           \ntrain\n=\nFalse\n,\n \n                           \ntransform\n=\ntransforms\n.\nToTensor\n())\n\n\n\n'''\n\n\nSTEP 2: MAKING DATASET ITERABLE\n\n\n'''\n\n\n\nbatch_size\n \n=\n \n100\n\n\nn_iters\n \n=\n \n3000\n\n\nnum_epochs\n \n=\n \nn_iters\n \n/\n \n(\nlen\n(\ntrain_dataset\n)\n \n/\n \nbatch_size\n)\n\n\nnum_epochs\n \n=\n \nint\n(\nnum_epochs\n)\n\n\n\ntrain_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \n                                           \nbatch_size\n=\nbatch_size\n,\n \n                                           \nshuffle\n=\nTrue\n)\n\n\n\ntest_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntest_dataset\n,\n \n                                          \nbatch_size\n=\nbatch_size\n,\n \n                                          \nshuffle\n=\nFalse\n)\n\n\n\n'''\n\n\nSTEP 3: CREATE MODEL CLASS\n\n\n'''\n\n\nclass\n \nLogisticRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_size\n,\n \nnum_classes\n):\n\n        \nsuper\n(\nLogisticRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n'''\n\n\nSTEP 4: INSTANTIATE MODEL CLASS\n\n\n'''\n\n\ninput_dim\n \n=\n \n28\n*\n28\n\n\noutput_dim\n \n=\n \n10\n\n\n\nmodel\n \n=\n \nLogisticRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n'''\n\n\nSTEP 5: INSTANTIATE LOSS CLASS\n\n\n'''\n\n\ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n\n\n\n'''\n\n\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n\n\n'''\n\n\nlearning_rate\n \n=\n \n0.001\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n'''\n\n\nSTEP 7: TRAIN THE MODEL\n\n\n'''\n\n\niter\n \n=\n \n0\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nnum_epochs\n):\n\n    \nfor\n \ni\n,\n \n(\nimages\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n        \n# Load images as Variable\n\n        \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n        \nlabels\n \n=\n \nlabels\n\n\n        \n# Clear gradients w.r.t. parameters\n\n        \noptimizer\n.\nzero_grad\n()\n\n\n        \n# Forward pass to get output/logits\n\n        \n# 100 x 10\n\n        \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n\n        \n# Calculate Loss: softmax --> cross entropy loss\n\n        \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n        \n# Getting gradients w.r.t. parameters\n\n        \nloss\n.\nbackward\n()\n\n\n        \n# Updating parameters\n\n        \noptimizer\n.\nstep\n()\n\n\n        \niter\n \n+=\n \n1\n\n\n        \nif\n \niter\n \n%\n \n500\n \n==\n \n0\n:\n\n            \n# Calculate Accuracy         \n\n            \ncorrect\n \n=\n \n0\n\n            \ntotal\n \n=\n \n0\n\n            \n# Iterate through test dataset\n\n            \nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n                \n# Load images to a Torch Variable\n\n                \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n\n\n                \n# Forward pass only to get logits/output\n\n                \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n\n                \n# Get predictions from the maximum value\n\n                \n# 100 x 1\n\n                \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n                \n# Total number of labels\n\n                \ntotal\n \n+=\n \nlabels\n.\nsize\n(\n0\n)\n\n\n                \n# Total correct predictions\n\n                \ncorrect\n \n+=\n \n(\npredicted\n \n==\n \nlabels\n)\n.\nsum\n()\n\n\n            \naccuracy\n \n=\n \n100\n \n*\n \ncorrect\n.\nitem\n()\n \n/\n \ntotal\n\n\n            \n# Print Loss\n\n            \nprint\n(\n'Iteration: {}. Loss: {}. Accuracy: {}'\n.\nformat\n(\niter\n,\n \nloss\n.\nitem\n(),\n \naccuracy\n))\n\n\n\n\n\n\n\nIteration\n:\n \n500.\n \nLoss\n:\n \n1.876196026802063\n.\n \nAccuracy\n:\n \n64.44\n\n\nIteration\n:\n \n1000.\n \nLoss\n:\n \n1.5153584480285645\n.\n \nAccuracy\n:\n \n75.68\n\n\nIteration\n:\n \n1500.\n \nLoss\n:\n \n1.3521136045455933\n.\n \nAccuracy\n:\n \n78.98\n\n\nIteration\n:\n \n2000.\n \nLoss\n:\n \n1.2136967182159424\n.\n \nAccuracy\n:\n \n80.95\n\n\nIteration\n:\n \n2500.\n \nLoss\n:\n \n1.0934826135635376\n.\n \nAccuracy\n:\n \n81.97\n\n\nIteration\n:\n \n3000.\n \nLoss\n:\n \n1.024120569229126\n.\n \nAccuracy\n:\n \n82.49\n\n\n\n\n\n\n\nGPU version\n\n\n2 things must be on GPU\n\n- \nmodel\n\n\n- \nvariables\n\n\nRemember step 4 and 7 will be affected and this will be the same for all model building moving forward.\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorchvision.transforms\n \nas\n \ntransforms\n\n\nimport\n \ntorchvision.datasets\n \nas\n \ndsets\n\n\n\n'''\n\n\nSTEP 1: LOADING DATASET\n\n\n'''\n\n\n\ntrain_dataset\n \n=\n \ndsets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \n                            \ntrain\n=\nTrue\n,\n \n                            \ntransform\n=\ntransforms\n.\nToTensor\n(),\n\n                            \ndownload\n=\nTrue\n)\n\n\n\ntest_dataset\n \n=\n \ndsets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \n                           \ntrain\n=\nFalse\n,\n \n                           \ntransform\n=\ntransforms\n.\nToTensor\n())\n\n\n\n'''\n\n\nSTEP 2: MAKING DATASET ITERABLE\n\n\n'''\n\n\n\nbatch_size\n \n=\n \n100\n\n\nn_iters\n \n=\n \n3000\n\n\nnum_epochs\n \n=\n \nn_iters\n \n/\n \n(\nlen\n(\ntrain_dataset\n)\n \n/\n \nbatch_size\n)\n\n\nnum_epochs\n \n=\n \nint\n(\nnum_epochs\n)\n\n\n\ntrain_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \n                                           \nbatch_size\n=\nbatch_size\n,\n \n                                           \nshuffle\n=\nTrue\n)\n\n\n\ntest_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntest_dataset\n,\n \n                                          \nbatch_size\n=\nbatch_size\n,\n \n                                          \nshuffle\n=\nFalse\n)\n\n\n\n'''\n\n\nSTEP 3: CREATE MODEL CLASS\n\n\n'''\n\n\nclass\n \nLogisticRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_size\n,\n \nnum_classes\n):\n\n        \nsuper\n(\nLogisticRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n'''\n\n\nSTEP 4: INSTANTIATE MODEL CLASS\n\n\n'''\n\n\ninput_dim\n \n=\n \n28\n*\n28\n\n\noutput_dim\n \n=\n \n10\n\n\n\nmodel\n \n=\n \nLogisticRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n#######################\n\n\n#  USE GPU FOR MODEL  #\n\n\n#######################\n\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\nmodel\n.\nto\n(\ndevice\n)\n\n\n\n'''\n\n\nSTEP 5: INSTANTIATE LOSS CLASS\n\n\n'''\n\n\ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n\n\n\n'''\n\n\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n\n\n'''\n\n\nlearning_rate\n \n=\n \n0.001\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n'''\n\n\nSTEP 7: TRAIN THE MODEL\n\n\n'''\n\n\niter\n \n=\n \n0\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nnum_epochs\n):\n\n    \nfor\n \ni\n,\n \n(\nimages\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n\n        \n#######################\n\n        \n#  USE GPU FOR MODEL  #\n\n        \n#######################\n\n        \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nrequires_grad_\n()\n.\nto\n(\ndevice\n)\n\n        \nlabels\n \n=\n \nlabels\n.\nto\n(\ndevice\n)\n\n\n        \n# Clear gradients w.r.t. parameters\n\n        \noptimizer\n.\nzero_grad\n()\n\n\n        \n# Forward pass to get output/logits\n\n        \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n\n        \n# Calculate Loss: softmax --> cross entropy loss\n\n        \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n        \n# Getting gradients w.r.t. parameters\n\n        \nloss\n.\nbackward\n()\n\n\n        \n# Updating parameters\n\n        \noptimizer\n.\nstep\n()\n\n\n        \niter\n \n+=\n \n1\n\n\n        \nif\n \niter\n \n%\n \n500\n \n==\n \n0\n:\n\n            \n# Calculate Accuracy         \n\n            \ncorrect\n \n=\n \n0\n\n            \ntotal\n \n=\n \n0\n\n            \n# Iterate through test dataset\n\n            \nfor\n \nimages\n,\n \nlabels\n \nin\n \ntest_loader\n:\n\n                \n#######################\n\n                \n#  USE GPU FOR MODEL  #\n\n                \n#######################\n\n                \nimages\n \n=\n \nimages\n.\nview\n(\n-\n1\n,\n \n28\n*\n28\n)\n.\nto\n(\ndevice\n)\n\n\n                \n# Forward pass only to get logits/output\n\n                \noutputs\n \n=\n \nmodel\n(\nimages\n)\n\n\n                \n# Get predictions from the maximum value\n\n                \n_\n,\n \npredicted\n \n=\n \ntorch\n.\nmax\n(\noutputs\n.\ndata\n,\n \n1\n)\n\n\n                \n# Total number of labels\n\n                \ntotal\n \n+=\n \nlabels\n.\nsize\n(\n0\n)\n\n\n                \n#######################\n\n                \n#  USE GPU FOR MODEL  #\n\n                \n#######################\n\n                \n# Total correct predictions\n\n                \nif\n \ntorch\n.\ncuda\n.\nis_available\n():\n\n                    \ncorrect\n \n+=\n \n(\npredicted\n.\ncpu\n()\n \n==\n \nlabels\n.\ncpu\n())\n.\nsum\n()\n\n                \nelse\n:\n\n                    \ncorrect\n \n+=\n \n(\npredicted\n \n==\n \nlabels\n)\n.\nsum\n()\n\n\n            \naccuracy\n \n=\n \n100\n \n*\n \ncorrect\n.\nitem\n()\n \n/\n \ntotal\n\n\n            \n# Print Loss\n\n            \nprint\n(\n'Iteration: {}. Loss: {}. Accuracy: {}'\n.\nformat\n(\niter\n,\n \nloss\n.\nitem\n(),\n \naccuracy\n))\n\n\n\n\n\n\n\nIteration\n:\n \n500.\n \nLoss\n:\n \n1.8571407794952393\n.\n \nAccuracy\n:\n \n68.99\n\n\nIteration\n:\n \n1000.\n \nLoss\n:\n \n1.5415704250335693\n.\n \nAccuracy\n:\n \n75.86\n\n\nIteration\n:\n \n1500.\n \nLoss\n:\n \n1.2755383253097534\n.\n \nAccuracy\n:\n \n78.92\n\n\nIteration\n:\n \n2000.\n \nLoss\n:\n \n1.2468739748001099\n.\n \nAccuracy\n:\n \n80.72\n\n\nIteration\n:\n \n2500.\n \nLoss\n:\n \n1.0708973407745361\n.\n \nAccuracy\n:\n \n81.73\n\n\nIteration\n:\n \n3000.\n \nLoss\n:\n \n1.0359245538711548\n.\n \nAccuracy\n:\n \n82.74\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n \nLogistic regression\n basics\n\n\n \nProblems\n of \nlinear regression\n\n\n \nIn-depth\n Logistic Regression\n\n\n Get logits\n\n\n Get softmax\n\n\n Get cross-entropy loss\n\n\n\n\n\n\n \nAim\n: reduce cross-entropy loss\n\n\n Built a \nlogistic regression model\n in \nCPU and GPU\n\n\n Step 1: Load Dataset\n\n\n Step 2: Make Dataset Iterable\n\n\n Step 3: Create Model Class\n\n\n Step 4: Instantiate Model Class\n\n\n Step 5: Instantiate Loss Class\n\n\n Step 6: Instantiate Optimizer Class\n\n\n Step 7: Train Model\n\n\n\n\n\n\n Important things to be on \nGPU\n\n\n \nmodel\n\n\n \ntensors with gradients",
            "title": "PyTorch Fundamentals - Logistic Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-with-pytorch",
            "text": "",
            "title": "Logistic Regression with PyTorch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#about-logistic-regression",
            "text": "",
            "title": "About Logistic Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-basics",
            "text": "",
            "title": "Logistic Regression Basics"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#classification-algorithm",
            "text": "Example: Spam vs No Spam  Input: Bunch of words  Output: Probability spam or not",
            "title": "Classification algorithm"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#basic-comparison",
            "text": "Linear regression  Output: numeric value given inputs    Logistic regression :  Output: probability [0, 1] given input belonging to a class",
            "title": "Basic Comparison"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#inputoutput-comparison",
            "text": "Linear regression: Multiplication  Input: [1]  Output: 2    Input: [2]  Output: 4    Trying to model the relationship  y = 2x    Logistic regression: Spam  Input: \"Sign up to get 1 million dollars by tonight\"  Output: p = 0.8    Input: \"This is a receipt for your recent purchase with Amazon\"  Output: p = 0.3    p: probability it is spam",
            "title": "Input/Output Comparison"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#problems-of-linear-regression",
            "text": "Example  Fever  Input : temperature  Output : fever or no fever    Remember  Linear regression : minimize error between points and line      Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1)  If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import   numpy   as   np  import   matplotlib.pyplot   as   plt  % matplotlib   inline  x   =   [ 1 ,   5 ,   10 ,   10 ,   25 ,   50 ,   70 ,   75 ,   100 ,]  y   =   [ 0 ,   0 ,   0 ,   0 ,   0 ,   1 ,   1 ,   1 ,   1 ]  colors   =   np . random . rand ( len ( x ))  plt . plot ( np . unique ( x ),   np . poly1d ( np . polyfit ( x ,   y ,   1 ))( np . unique ( x )))  plt . ylabel ( \"Fever\" )  plt . xlabel ( \"Temperature\" )  plt . scatter ( x ,   y ,   c = colors ,   alpha = 0.5 )  plt . show ()      Linear Regression Problem 2: Fever points are not predicted with the presence of outliers  Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import   numpy   as   np  import   matplotlib.pyplot   as   plt  x   =   [ 1 ,   5 ,   10 ,   10 ,   25 ,   50 ,   70 ,   75 ,   300 ]  y   =   [ 0 ,   0 ,   0 ,   0 ,   0 ,   1 ,   1 ,   1 ,   1 ]  colors   =   np . random . rand ( len ( x ))  plt . plot ( np . unique ( x ),   np . poly1d ( np . polyfit ( x ,   y ,   1 ))( np . unique ( x )))  plt . ylabel ( \"Fever\" )  plt . xlabel ( \"Temperature\" )  plt . scatter ( x ,   y ,   c = colors ,   alpha = 0.5 )  plt . show ()",
            "title": "Problems of Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-in-depth",
            "text": "",
            "title": "Logistic Regression In-Depth"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#predicting-probability",
            "text": "Linear regression doesn't work  Instead of predicting direct values:  predict probability",
            "title": "Predicting Probability"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-function-g",
            "text": "Two-class logistic regression  y = A x + b y = A x + b  g(y) = A x + b g(y) = A x + b  g(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}} g(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}}  g(y) g(y)  = Estimated probability that  y = 1 y = 1  given  x x",
            "title": "Logistic Function g()"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#softmax-function-g",
            "text": "Multi-class logistic regression  Generalization of logistic function",
            "title": "Softmax Function g()"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d",
            "text": "D(S, L) = L log S - (1-L)log(1-S) D(S, L) = L log S - (1-L)log(1-S)  If L = 0 (label)  D(S, 0) = - log(1-S) D(S, 0) = - log(1-S)  - log(1-S) - log(1-S) : less positive if  S \\longrightarrow 0 S \\longrightarrow 0  - log(1-S) - log(1-S) : more positive if  S \\longrightarrow 1 S \\longrightarrow 1  (BIGGER LOSS)      If L = 1 (label)  D(S, 1) = log S D(S, 1) = log S  logS logS : less negative if  S \\longrightarrow 1 S \\longrightarrow 1  logS logS : more negative if  S \\longrightarrow 0 S \\longrightarrow 0  (BIGGER LOSS)          Numerical example of bigger or small loss  You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import   math  print ( - math . log ( 1   -   0.00001 ))   You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1   -   0.99999 ))    You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( math . log ( 0.99999 ))   You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( math . log ( 0.00001 ))    1.0000050000287824e-05  11.51292546497478  - 1.0000050000287824e-05  - 11.512925464970229",
            "title": "Cross Entropy Function D()"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-loss-l",
            "text": "Goal: Minimizing Cross Entropy Loss  L = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i) L = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i)",
            "title": "Cross Entropy Loss L"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch",
            "text": "",
            "title": "Building a Logistic Regression Model with PyTorch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#steps",
            "text": "Step 1: Load Dataset  Step 2: Make Dataset Iterable  Step 3: Create Model Class  Step 4: Instantiate Model Class  Step 5: Instantiate Loss Class  Step 6: Instantiate Optimizer Class  Step 7: Train Model",
            "title": "Steps"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1a-loading-mnist-train-dataset",
            "text": "Images from 1 to 9   Inspect length of training dataset  You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import   torch  import   torch.nn   as   nn  import   torchvision.transforms   as   transforms  import   torchvision.datasets   as   dsets   train_dataset   =   dsets . MNIST ( root = './data' ,  \n                             train = True ,  \n                             transform = transforms . ToTensor (), \n                             download = True )   len ( train_dataset )    60000    Inspecting a single image  So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ]    ( tensor ([[[   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0118 ,    0.0706 , \n             0.0706 ,    0.0706 ,    0.4941 ,    0.5333 ,    0.6863 ,    0.1020 ,    0.6510 , \n             1.0000 ,    0.9686 ,    0.4980 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.1176 ,    0.1412 ,    0.3686 ,    0.6039 ,    0.6667 ,    0.9922 , \n             0.9922 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.8824 ,    0.6745 ,    0.9922 , \n             0.9490 ,    0.7647 ,    0.2510 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.1922 ,    0.9333 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9922 , \n             0.9922 ,    0.9922 ,    0.9922 ,    0.9843 ,    0.3647 ,    0.3216 ,    0.3216 , \n             0.2196 ,    0.1529 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0706 ,    0.8588 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9922 , \n             0.7765 ,    0.7137 ,    0.9686 ,    0.9451 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.3137 ,    0.6118 ,    0.4196 ,    0.9922 ,    0.9922 ,    0.8039 , \n             0.0431 ,    0.0000 ,    0.1686 ,    0.6039 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0549 ,    0.0039 ,    0.6039 ,    0.9922 ,    0.3529 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.5451 ,    0.9922 ,    0.7451 , \n             0.0078 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0431 ,    0.7451 ,    0.9922 , \n             0.2745 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.1373 ,    0.9451 , \n             0.8824 ,    0.6275 ,    0.4235 ,    0.0039 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.3176 , \n             0.9412 ,    0.9922 ,    0.9922 ,    0.4667 ,    0.0980 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.1765 ,    0.7294 ,    0.9922 ,    0.9922 ,    0.5882 ,    0.1059 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0627 ,    0.3647 ,    0.9882 ,    0.9922 ,    0.7333 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.9765 ,    0.9922 ,    0.9765 ,    0.2510 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.1804 ,    0.5098 ,    0.7176 ,    0.9922 ,    0.9922 ,    0.8118 ,    0.0078 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.1529 ,    0.5804 , \n             0.8980 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9804 ,    0.7137 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0941 ,    0.4471 ,    0.8667 ,    0.9922 , \n             0.9922 ,    0.9922 ,    0.9922 ,    0.7882 ,    0.3059 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0902 ,    0.2588 ,    0.8353 ,    0.9922 ,    0.9922 ,    0.9922 , \n             0.9922 ,    0.7765 ,    0.3176 ,    0.0078 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0706 , \n             0.6706 ,    0.8588 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.7647 , \n             0.3137 ,    0.0353 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.2157 ,    0.6745 ,    0.8863 , \n             0.9922 ,    0.9922 ,    0.9922 ,    0.9922 ,    0.9569 ,    0.5216 ,    0.0431 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.5333 ,    0.9922 ,    0.9922 , \n             0.9922 ,    0.8314 ,    0.5294 ,    0.5176 ,    0.0627 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ], \n           [   0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 , \n             0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ,    0.0000 ]]]), \n  tensor ( 5 ))    Inspecting a single data point in the training dataset  When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label.  type ( train_dataset [ 0 ])    tuple    Inspecting training dataset first element of tuple  This means to access the image, you need to access the first element in the tuple.  # Input Matrix  train_dataset [ 0 ][ 0 ] . size ()    # A 28x28 sized image of a digit  torch . Size ([ 1 ,   28 ,   28 ])    Inspecting training dataset second element of tuple  The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5.  # Label  train_dataset [ 0 ][ 1 ]    tensor ( 5 )",
            "title": "Step 1a: Loading MNIST Train Dataset"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#displaying-mnist",
            "text": "Verifying shape of MNIST image  As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel.  import   matplotlib.pyplot   as   plt  % matplotlib   inline    import   numpy   as   np   train_dataset [ 0 ][ 0 ] . numpy () . shape    ( 1 ,   28 ,   28 )    Plot image of MNIST image  show_img   =   train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 ,   28 )   plt . imshow ( show_img ,   cmap = 'gray' )      Second element of tuple shows label  As you would expect, the label is 5. # Label  train_dataset [ 0 ][ 1 ]    tensor ( 5 )    Plot second image of MNIST image  show_img   =   train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 ,   28 )   plt . imshow ( show_img ,   cmap = 'gray' )      Second element of tuple shows label  We should see 0 here as the label. # Label  train_dataset [ 1 ][ 1 ]    tensor ( 0 )",
            "title": "Displaying MNIST"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1b-loading-mnist-test-dataset",
            "text": "Show our algorithm works beyond the data we have trained on.  Out-of-sample    Load test dataset  Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset   =   dsets . MNIST ( root = './data' ,  \n                            train = False ,  \n                            transform = transforms . ToTensor ())   len ( test_dataset )    10000    Test dataset elements  Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ])    tuple    Test dataset first element in tuple  This contains the image matrix, similar to the training set. # Image matrix  test_dataset [ 0 ][ 0 ] . size ()    torch . Size ([ 1 ,   28 ,   28 ])    Plot image sample from test dataset  show_img   =   test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 ,   28 )  plt . imshow ( show_img ,   cmap = 'gray' )      Test dataset second element in tuple  # Label  test_dataset [ 0 ][ 1 ]    tensor ( 7 )",
            "title": "Step 1b: Loading MNIST Test Dataset"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-2-make-dataset-iterable",
            "text": "Aim: make the dataset iterable  totaldata : 60000  minibatch : 100  Number of examples in 1 iteration    iterations : 3000  1 iteration: one mini-batch forward & backward pass    epochs  1 epoch: running through the whole dataset once  $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 $      Recap training dataset  Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset )    60000    Defining epochs  When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch.   However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.  batch_size   =   100   We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters   =   3000   One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations.   num_epochs   =   n_iters   /   ( len ( train_dataset )   /   batch_size )  num_epochs   =   int ( num_epochs )  num_epochs    5    Create Iterable Object: Training Dataset  train_loader   =   torch . utils . data . DataLoader ( dataset = train_dataset ,  \n                                            batch_size = batch_size ,  \n                                            shuffle = True )     Check Iterability  import   collections  isinstance ( train_loader ,   collections . Iterable )    True    Create Iterable Object: Testing Dataset  # Iterable object  test_loader   =   torch . utils . data . DataLoader ( dataset = test_dataset ,  \n                                           batch_size = batch_size ,  \n                                           shuffle = False )     Check iterability of testing dataset  isinstance ( test_loader ,   collections . Iterable )    True    Iterate through dataset  This is just a simplified example of what we're doing above where we're creating an iterable object  lst  to loop through so we can access all the images  img_1  and  img_2 .  Above, the equivalent of  lst  is  train_loader  and  test_loader .  img_1   =   np . ones (( 28 ,   28 ))  img_2   =   np . ones (( 28 ,   28 ))  lst   =   [ img_1 ,   img_2 ]   # Need to iterate  # Think of numbers as the images  for   i   in   lst : \n     print ( i . shape )    ( 28 ,   28 )  ( 28 ,   28 )",
            "title": "Step 2: Make Dataset Iterable"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-3-building-model",
            "text": "Create model class  # Same as linear regression!   class   LogisticRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LogisticRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim ) \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out",
            "title": "Step 3: Building Model"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-4-instantiate-model-class",
            "text": "Input dimension:   Size of image  28 \\times 28 = 784 28 \\times 28 = 784    Output dimension: 10  0, 1, 2, 3, 4, 5, 6, 7, 8, 9      Check size of dataset  This should be 28x28. # Size of images  train_dataset [ 0 ][ 0 ] . size ()    torch . Size ([ 1 ,   28 ,   28 ])    Instantiate model class based on input and out dimensions  As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10.   And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim   =   28 * 28  output_dim   =   10  model   =   LogisticRegressionModel ( input_dim ,   output_dim )",
            "title": "Step 4: Instantiate Model Class"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-5-instantiate-loss-class",
            "text": "Logistic Regression : Cross Entropy Loss  Linear Regression: MSE      Create Cross Entry Loss Class  Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters.  criterion   =   nn . CrossEntropyLoss ()       What happens in nn.CrossEntropyLoss()?  It does 2 things at the same time.    1. Computes softmax (logistic/softmax function)  2. Computes cross entropy",
            "title": "Step 5: Instantiate Loss Class"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-6-instantiate-optimizer-class",
            "text": "Simplified equation  \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta  \\theta \\theta : parameters (our variables)  \\eta \\eta : learning rate (how fast we want to learn)  \\nabla_\\theta \\nabla_\\theta : parameters' gradients      Even simplier equation  parameters = parameters - learning_rate * parameters_gradients  At every iteration, we update our model's parameters      Create optimizer  Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate   =   0.001  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )       Parameters In-Depth  You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the  y = AX + b y = AX + b  equation where X is our input of size 784.  We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output.   # Type of parameter object  print ( model . parameters ())  # Length of parameters  print ( len ( list ( model . parameters ())))  # FC 1 Parameters   print ( list ( model . parameters ())[ 0 ] . size ())  # FC 1 Bias Parameters  print ( list ( model . parameters ())[ 1 ] . size ())    < generator   object   Module . parameters   at   0x7ff7c884f830 >  2  torch . Size ([ 10 ,   784 ])  torch . Size ([ 10 ])    Quick Dot Product Review   Example 1:  dot product  A: (100, 10) A: (100, 10)  B: (10, 1) B: (10, 1)  A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1)    Example 2:  dot product  A: (50, 5) A: (50, 5)  B: (5, 2) B: (5, 2)  A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2)    Example 3:  element-wise addition  A: (10, 1) A: (10, 1)  B: (10, 1) B: (10, 1)  A + B = (10, 1) A + B = (10, 1)",
            "title": "Step 6: Instantiate Optimizer Class"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#step-7-train-model",
            "text": "7 step process for training models   Process   Convert inputs/labels to tensors with gradients  Clear gradient buffets  Get output given inputs  Get loss  Get gradients w.r.t. parameters  Update parameters using gradients  parameters = parameters - learning_rate * parameters_gradients    REPEAT     iter   =   0  for   epoch   in   range ( num_epochs ): \n     for   i ,   ( images ,   labels )   in   enumerate ( train_loader ): \n         # Load images as Variable \n         images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n         labels   =   labels \n\n         # Clear gradients w.r.t. parameters \n         optimizer . zero_grad () \n\n         # Forward pass to get output/logits \n         outputs   =   model ( images ) \n\n         # Calculate Loss: softmax --> cross entropy loss \n         loss   =   criterion ( outputs ,   labels ) \n\n         # Getting gradients w.r.t. parameters \n         loss . backward () \n\n         # Updating parameters \n         optimizer . step () \n\n         iter   +=   1 \n\n         if   iter   %   500   ==   0 : \n             # Calculate Accuracy          \n             correct   =   0 \n             total   =   0 \n             # Iterate through test dataset \n             for   images ,   labels   in   test_loader : \n                 # Load images to a Torch Variable \n                 images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n\n                 # Forward pass only to get logits/output \n                 outputs   =   model ( images ) \n\n                 # Get predictions from the maximum value \n                 _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n\n                 # Total number of labels \n                 total   +=   labels . size ( 0 ) \n\n                 # Total correct predictions \n                 correct   +=   ( predicted   ==   labels ) . sum () \n\n             accuracy   =   100   *   correct   /   total \n\n             # Print Loss \n             print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter ,   loss . item (),   accuracy ))    Iteration :   500.   Loss :   1.8513233661651611 .   Accuracy :   70  Iteration :   1000.   Loss :   1.5732524394989014 .   Accuracy :   77  Iteration :   1500.   Loss :   1.3840199708938599 .   Accuracy :   79  Iteration :   2000.   Loss :   1.1711134910583496 .   Accuracy :   81  Iteration :   2500.   Loss :   1.1094708442687988 .   Accuracy :   82  Iteration :   3000.   Loss :   1.002761721611023 .   Accuracy :   82",
            "title": "Step 7: Train Model"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#break-down-accuracy-calculation",
            "text": "Printing outputs of our model  As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.  This would print out the output of the model's predictions on your notebook.  iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     if   iter_test   ==   1 : \n         print ( 'OUTPUTS' ) \n         print ( outputs ) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 )    OUTPUTS  tensor ([[ - 0.4181 ,   - 1.0784 ,   - 0.4840 ,   - 0.0985 ,   - 0.2394 ,   - 0.1801 ,   - 1.1639 , \n           2.9352 ,   - 0.1552 ,    0.8852 ], \n         [   0.5117 ,   - 0.1099 ,    1.5295 ,    0.8863 ,   - 1.8813 ,    0.5967 ,    1.3632 , \n          - 1.8977 ,    0.4183 ,   - 1.4990 ], \n         [ - 1.0126 ,    2.4112 ,    0.2373 ,    0.0857 ,   - 0.7007 ,   - 0.2015 ,   - 0.3428 , \n          - 0.2548 ,    0.1659 ,   - 0.4703 ], \n         [   2.8072 ,   - 2.2973 ,   - 0.0984 ,   - 0.4313 ,   - 0.9619 ,    0.8670 ,    1.2201 , \n           0.3752 ,   - 0.2873 ,   - 0.3272 ], \n         [ - 0.0343 ,   - 2.0043 ,    0.5081 ,   - 0.6452 ,    1.8647 ,   - 0.6924 ,    0.1435 , \n           0.4330 ,    0.2958 ,    1.0339 ], \n         [ - 1.5392 ,    2.9070 ,    0.2297 ,    0.3139 ,   - 0.6863 ,   - 0.2734 ,   - 0.8377 , \n          - 0.1238 ,    0.3285 ,   - 0.3004 ], \n         [ - 1.2037 ,   - 1.3739 ,   - 0.5947 ,    0.3530 ,    1.4205 ,    0.0593 ,   - 0.7307 , \n           0.6642 ,    0.3937 ,    0.8004 ], \n         [ - 1.4439 ,   - 0.3284 ,   - 0.7652 ,   - 0.0952 ,    0.9323 ,    0.3006 ,    0.0238 , \n          - 0.0810 ,    0.0612 ,    1.3295 ], \n         [   0.5409 ,   - 0.5266 ,    0.9914 ,   - 1.2369 ,    0.6583 ,    0.0992 ,    0.8525 , \n          - 1.0562 ,    0.2013 ,    0.0462 ], \n         [ - 0.6548 ,   - 0.7253 ,   - 0.9825 ,   - 1.1663 ,    0.9076 ,   - 0.0694 ,   - 0.3708 , \n           1.8270 ,    0.2457 ,    1.5921 ], \n         [   3.2147 ,   - 1.7689 ,    0.8531 ,    1.2320 ,   - 0.8126 ,    1.1251 ,   - 0.2776 , \n          - 1.4244 ,    0.5930 ,   - 1.6183 ], \n         [   0.7470 ,   - 0.5545 ,    1.0251 ,    0.0529 ,    0.4384 ,   - 0.5934 ,    0.7666 , \n          - 1.0084 ,    0.5313 ,   - 0.3465 ], \n         [ - 0.7916 ,   - 1.7064 ,   - 0.7805 ,   - 1.1588 ,    1.3284 ,   - 0.1708 ,   - 0.2092 , \n           0.9495 ,    0.1033 ,    2.0208 ], \n         [   3.0602 ,   - 2.3578 ,   - 0.2576 ,   - 0.2198 ,   - 0.2372 ,    0.9765 ,   - 0.1514 , \n          - 0.5380 ,    0.7970 ,    0.1374 ], \n         [ - 1.2613 ,    2.8594 ,   - 0.0874 ,    0.1974 ,   - 1.2018 ,   - 0.0064 ,   - 0.0923 , \n          - 0.2142 ,    0.2575 ,   - 0.3218 ], \n         [   0.4348 ,   - 0.7216 ,    0.0021 ,    1.2864 ,   - 0.5062 ,    0.7761 ,   - 0.3236 , \n          - 0.5667 ,    0.5431 ,   - 0.7781 ], \n         [ - 0.2157 ,   - 2.0200 ,    0.1829 ,   - 0.6882 ,    1.3815 ,   - 0.7609 ,   - 0.0902 , \n           0.8647 ,    0.3679 ,    1.8843 ], \n         [   0.0950 ,   - 1.5009 ,   - 0.6347 ,    0.3662 ,   - 0.4679 ,   - 0.0359 ,   - 0.7671 , \n           2.7155 ,   - 0.3991 ,    0.5737 ], \n         [ - 0.7005 ,   - 0.5366 ,   - 0.0434 ,    1.1289 ,   - 0.5873 ,    0.2555 ,    0.8187 , \n          - 0.6557 ,    0.1241 ,   - 0.4297 ], \n         [ - 1.0635 ,   - 1.5991 ,   - 0.4677 ,   - 0.1231 ,    2.0445 ,    0.1128 ,   - 0.1825 , \n           0.1075 ,    0.0348 ,    1.4317 ], \n         [ - 1.0319 ,   - 0.1595 ,   - 1.3415 ,    0.1095 ,    0.5339 ,    0.1973 ,   - 1.3272 , \n           1.5765 ,    0.4784 ,    1.4176 ], \n         [ - 0.4928 ,   - 1.5653 ,   - 0.0672 ,    0.3325 ,    0.5359 ,    0.5368 ,    2.1542 , \n          - 1.4276 ,    0.3605 ,    0.0587 ], \n         [ - 0.4761 ,    0.2958 ,    0.6597 ,   - 0.2658 ,    1.1279 ,   - 1.0676 ,    1.2506 , \n          - 0.2059 ,   - 0.1489 ,    0.1051 ], \n         [ - 0.0764 ,   - 0.9274 ,   - 0.6838 ,    0.3464 ,   - 0.2656 ,    1.4099 ,    0.4486 , \n          - 0.9527 ,    0.5682 ,    0.0156 ], \n         [ - 0.6900 ,   - 0.9611 ,    0.1395 ,   - 0.0079 ,    1.5424 ,   - 0.3208 ,   - 0.2682 , \n           0.3586 ,   - 0.2771 ,    1.0389 ], \n         [   4.3606 ,   - 2.8621 ,    0.6310 ,   - 0.9657 ,   - 0.2486 ,    1.2009 ,    1.1873 , \n          - 0.8255 ,   - 0.2103 ,   - 1.2172 ], \n         [ - 0.1000 ,   - 1.4268 ,   - 0.4627 ,   - 0.1041 ,    0.2959 ,   - 0.1392 ,   - 0.6855 , \n           1.8622 ,   - 0.2580 ,    1.1347 ], \n         [ - 0.3625 ,   - 2.1323 ,   - 0.2224 ,   - 0.8754 ,    2.4684 ,    0.0295 ,    0.1161 , \n          - 0.2660 ,    0.3037 ,    1.4570 ], \n         [   2.8688 ,   - 2.4517 ,    0.1782 ,    1.1149 ,   - 1.0898 ,    1.1062 ,   - 0.0681 , \n          - 0.5697 ,    0.8888 ,   - 0.6965 ], \n         [ - 1.0429 ,    1.4446 ,   - 0.3349 ,    0.1254 ,   - 0.5017 ,    0.2286 ,    0.2328 , \n          - 0.3290 ,    0.3949 ,   - 0.2586 ], \n         [ - 0.8476 ,   - 0.0004 ,   - 1.1003 ,    2.2806 ,   - 1.2226 ,    0.9251 ,   - 0.3165 , \n           0.4957 ,    0.0690 ,    0.0232 ], \n         [ - 0.9108 ,    1.1355 ,   - 0.2715 ,    0.2233 ,   - 0.3681 ,    0.1442 ,   - 0.0001 , \n          - 0.0174 ,    0.1454 ,    0.2286 ], \n         [ - 1.0663 ,   - 0.8466 ,   - 0.7147 ,    2.5685 ,   - 0.2090 ,    1.2993 ,   - 0.3057 , \n          - 0.8314 ,    0.7046 ,   - 0.0176 ], \n         [   1.7013 ,   - 1.8051 ,    0.7541 ,   - 1.5248 ,    0.8972 ,    0.1518 ,    1.4876 , \n          - 0.8454 ,   - 0.2022 ,   - 0.2829 ], \n         [ - 0.8179 ,   - 0.1239 ,    0.8630 ,   - 0.2137 ,   - 0.2275 ,   - 0.5411 ,   - 1.3448 , \n           1.7354 ,    0.7751 ,    0.6234 ], \n         [   0.6515 ,   - 1.0431 ,    2.7165 ,    0.1873 ,   - 1.0623 ,    0.1286 ,    0.3597 , \n          - 0.2739 ,    0.3871 ,   - 1.6699 ], \n         [ - 0.2828 ,   - 1.4663 ,    0.1182 ,   - 0.0896 ,   - 0.3640 ,   - 0.5129 ,   - 0.4905 , \n           2.2914 ,   - 0.2227 ,    0.9463 ], \n         [ - 1.2596 ,    2.0468 ,   - 0.4405 ,   - 0.0411 ,   - 0.8073 ,    0.0490 ,   - 0.0604 , \n          - 0.1206 ,    0.3504 ,   - 0.1059 ], \n         [   0.6089 ,    0.5885 ,    0.7898 ,    1.1318 ,   - 1.9008 ,    0.5875 ,    0.4227 , \n          - 1.1815 ,    0.5652 ,   - 1.3590 ], \n         [ - 1.4551 ,    2.9537 ,   - 0.2805 ,    0.2372 ,   - 1.4180 ,    0.0297 ,   - 0.1515 , \n          - 0.6111 ,    0.6140 ,   - 0.3354 ], \n         [ - 0.7182 ,    1.6778 ,    0.0553 ,    0.0461 ,   - 0.5446 ,   - 0.0338 ,   - 0.0215 , \n          - 0.0881 ,    0.1506 ,   - 0.2107 ], \n         [ - 0.8027 ,   - 0.7854 ,   - 0.1275 ,   - 0.3177 ,   - 0.1600 ,   - 0.1964 ,   - 0.6084 , \n           2.1285 ,   - 0.1815 ,    1.1911 ], \n         [ - 2.0656 ,   - 0.4959 ,   - 0.1154 ,   - 0.1363 ,    2.2426 ,   - 0.7441 ,   - 0.8413 , \n           0.4675 ,    0.3269 ,    1.7279 ], \n         [ - 0.3004 ,    1.0166 ,    1.1175 ,   - 0.0618 ,   - 0.0937 ,   - 0.4221 ,    0.1943 , \n          - 1.1020 ,    0.3670 ,   - 0.4683 ], \n         [ - 1.0720 ,    0.2252 ,    0.0175 ,    1.3644 ,   - 0.7409 ,    0.4655 ,    0.5439 , \n           0.0380 ,    0.1279 ,   - 0.2302 ], \n         [   0.2409 ,   - 1.2622 ,   - 0.6336 ,    1.8240 ,   - 0.5951 ,    1.3408 ,    0.2130 , \n          - 1.3789 ,    0.8363 ,   - 0.2101 ], \n         [ - 1.3849 ,    0.3773 ,   - 0.0585 ,    0.6896 ,   - 0.0998 ,    0.2804 ,    0.0696 , \n          - 0.2529 ,    0.3143 ,    0.3409 ], \n         [ - 0.9103 ,   - 0.1578 ,    1.6673 ,   - 0.4817 ,    0.4088 ,   - 0.5484 ,    0.6103 , \n          - 0.2287 ,   - 0.0665 ,    0.0055 ], \n         [ - 1.1692 ,   - 2.8531 ,   - 1.2499 ,   - 0.0257 ,    2.8580 ,    0.2616 ,   - 0.7122 , \n          - 0.0551 ,    0.8112 ,    2.3233 ], \n         [ - 0.2790 ,   - 1.9494 ,    0.6096 ,   - 0.5653 ,    2.2792 ,   - 1.0687 ,    0.1634 , \n           0.3122 ,    0.1053 ,    1.0884 ], \n         [   0.1267 ,   - 1.2297 ,   - 0.1315 ,    0.2428 ,   - 0.5436 ,    0.4123 ,    2.3060 , \n          - 0.9278 ,   - 0.1528 ,   - 0.4224 ], \n         [ - 0.0235 ,   - 0.9137 ,   - 0.1457 ,    1.6858 ,   - 0.7552 ,    0.7293 ,    0.2510 , \n          - 0.3955 ,   - 0.2187 ,   - 0.1505 ], \n         [   0.5643 ,   - 1.2783 ,   - 1.4149 ,    0.0304 ,    0.8375 ,    1.5018 ,    0.0338 , \n          - 0.3875 ,   - 0.0117 ,    0.5751 ], \n         [   0.2926 ,   - 0.7486 ,   - 0.3238 ,    1.0384 ,    0.0308 ,    0.6792 ,   - 0.0170 , \n          - 0.5797 ,    0.2819 ,   - 0.3510 ], \n         [   0.1219 ,   - 0.5862 ,    1.5817 ,   - 0.1297 ,    0.4730 ,   - 0.9171 ,    0.7886 , \n          - 0.7022 ,   - 0.0501 ,   - 0.2812 ], \n         [   1.7587 ,   - 2.4511 ,   - 0.7369 ,    0.4082 ,   - 0.6426 ,    1.1784 ,    0.6052 , \n          - 0.7178 ,    1.6161 ,   - 0.2220 ], \n         [ - 0.1267 ,   - 2.6719 ,    0.0505 ,   - 0.4972 ,    2.9027 ,   - 0.1461 ,    0.2807 , \n          - 0.2921 ,    0.2231 ,    1.1327 ], \n         [ - 0.9892 ,    2.4401 ,    0.1274 ,    0.2838 ,   - 0.7535 ,   - 0.1684 ,   - 0.6493 , \n          - 0.1908 ,    0.2290 ,   - 0.2150 ], \n         [ - 0.2071 ,   - 2.1351 ,   - 0.9191 ,   - 0.9309 ,    1.7747 ,   - 0.3046 ,    0.0183 , \n           1.0136 ,   - 0.1016 ,    2.1288 ], \n         [ - 0.0103 ,    0.3280 ,   - 0.6974 ,   - 0.2504 ,    0.3187 ,    0.4390 ,   - 0.1879 , \n           0.3954 ,    0.2332 ,   - 0.1971 ], \n         [ - 0.2280 ,   - 1.6754 ,   - 0.7438 ,    0.5078 ,    0.2544 ,   - 0.1020 ,   - 0.2503 , \n           2.0799 ,   - 0.5033 ,    0.5890 ], \n         [   0.3972 ,   - 0.9369 ,    1.2696 ,   - 1.6713 ,   - 0.4159 ,   - 0.0221 ,    0.6489 , \n          - 0.4777 ,    1.2497 ,    0.3931 ], \n         [ - 0.7566 ,   - 0.8230 ,   - 0.0785 ,   - 0.3083 ,    0.7821 ,    0.1880 ,    0.1037 , \n          - 0.0956 ,    0.4219 ,    1.0798 ], \n         [ - 1.0328 ,   - 0.1700 ,    1.3806 ,    0.5445 ,   - 0.2624 ,   - 0.0780 ,   - 0.3595 , \n          - 0.6253 ,    0.4309 ,    0.1813 ], \n         [ - 1.0360 ,   - 0.4704 ,    0.1948 ,   - 0.7066 ,    0.6600 ,   - 0.4633 ,   - 0.3602 , \n           1.7494 ,    0.1522 ,    0.6086 ], \n         [ - 1.2032 ,   - 0.7903 ,   - 0.5754 ,    0.4722 ,    0.6068 ,    0.5752 ,    0.2151 , \n          - 0.2495 ,    0.3420 ,    0.9278 ], \n         [   0.2247 ,   - 0.1361 ,    0.9374 ,   - 0.1543 ,    0.4921 ,   - 0.6553 ,    0.5885 , \n           0.2617 ,   - 0.2216 ,   - 0.3736 ], \n         [ - 0.2867 ,   - 1.4486 ,    0.6658 ,   - 0.8755 ,    2.3195 ,   - 0.7627 ,   - 0.2132 , \n           0.2488 ,    0.3484 ,    1.0860 ], \n         [ - 1.4031 ,   - 0.4518 ,   - 0.3181 ,    2.8268 ,   - 0.5371 ,    1.0154 ,   - 0.9247 , \n          - 0.7385 ,    1.1031 ,    0.0422 ], \n         [   2.8604 ,   - 1.5413 ,    0.6241 ,   - 0.8017 ,   - 1.4104 ,    0.6314 ,    0.4614 , \n          - 0.0218 ,   - 0.3411 ,   - 0.2609 ], \n         [   0.2113 ,   - 1.2348 ,   - 0.8535 ,   - 0.1041 ,   - 0.2703 ,   - 0.1294 ,   - 0.7057 , \n           2.7552 ,   - 0.4429 ,    0.4517 ], \n         [   4.5191 ,   - 2.7407 ,    1.1091 ,    0.3975 ,   - 0.9456 ,    1.2277 ,    0.3616 , \n          - 1.6564 ,    0.5063 ,   - 1.4274 ], \n         [   1.4615 ,   - 1.0765 ,    1.8388 ,    1.5006 ,   - 1.2351 ,    0.2781 ,    0.2830 , \n          - 0.8491 ,    0.2222 ,   - 1.7779 ], \n         [ - 1.2160 ,    0.8502 ,    0.2413 ,   - 0.0798 ,   - 0.7880 ,   - 0.4286 ,   - 0.8060 , \n           0.7194 ,    1.2663 ,    0.6412 ], \n         [ - 1.3318 ,    2.3388 ,   - 0.4003 ,   - 0.1094 ,   - 1.0285 ,    0.1021 ,   - 0.0388 , \n          - 0.0497 ,    0.5137 ,   - 0.2507 ], \n         [ - 1.7853 ,    0.5884 ,   - 0.6108 ,   - 0.5557 ,    0.8696 ,   - 0.6226 ,   - 0.7983 , \n           1.7169 ,   - 0.0145 ,    0.8231 ], \n         [ - 0.1739 ,    0.1562 ,   - 0.2933 ,    2.3195 ,   - 0.9480 ,    1.2019 ,   - 0.4834 , \n          - 1.0567 ,    0.5685 ,   - 0.6841 ], \n         [ - 0.7920 ,   - 0.3339 ,    0.7452 ,   - 0.6529 ,   - 0.3307 ,   - 0.6092 ,   - 0.0950 , \n           1.7311 ,   - 0.3481 ,    0.3801 ], \n         [ - 1.7810 ,    1.0676 ,   - 0.7611 ,    0.3658 ,   - 0.0431 ,   - 0.1012 ,   - 0.6048 , \n           0.3089 ,    0.9998 ,    0.7164 ], \n         [ - 0.5856 ,   - 0.5261 ,   - 0.4859 ,   - 1.0551 ,   - 0.1838 ,   - 0.2144 ,   - 1.2599 , \n           3.3891 ,    0.4691 ,    0.7566 ], \n         [ - 0.4984 ,   - 1.7770 ,   - 1.1998 ,   - 0.1075 ,    1.0882 ,    0.4539 ,   - 0.5651 , \n           1.4381 ,   - 0.5678 ,    1.7479 ], \n         [   0.2938 ,   - 1.8536 ,    0.4259 ,   - 0.5429 ,    0.0066 ,    0.4120 ,    2.3793 , \n          - 0.3666 ,   - 0.2604 ,    0.0382 ], \n         [ - 0.4080 ,   - 0.9851 ,    4.0264 ,    0.1099 ,   - 0.1766 ,   - 1.1557 ,    0.6419 , \n          - 0.8147 ,    0.7535 ,   - 1.1452 ], \n         [ - 0.4636 ,   - 1.7323 ,   - 0.6433 ,   - 0.0274 ,    0.7227 ,   - 0.1799 ,   - 0.9336 , \n           2.1881 ,   - 0.2073 ,    1.6522 ], \n         [ - 0.9617 ,   - 0.0348 ,   - 0.3980 ,   - 0.4738 ,    0.7790 ,    0.4671 ,   - 0.6115 , \n          - 0.7067 ,    1.3036 ,    0.4923 ], \n         [ - 1.0151 ,   - 2.5385 ,   - 0.6072 ,    0.2902 ,    3.1570 ,    0.1062 ,   - 0.2169 , \n          - 0.4491 ,    0.6326 ,    1.6829 ], \n         [ - 1.8852 ,    0.6066 ,   - 0.2840 ,   - 0.4475 ,   - 0.1147 ,   - 0.7858 ,   - 1.1805 , \n           3.0723 ,    0.3960 ,    0.9720 ], \n         [   0.0344 ,   - 1.4878 ,   - 0.9675 ,    1.9649 ,   - 0.3146 ,    1.2183 ,    0.6730 , \n          - 0.3650 ,    0.0646 ,   - 0.0898 ], \n         [ - 0.2118 ,   - 2.0350 ,    0.9917 ,   - 0.8993 ,    1.2334 ,   - 0.6723 ,    2.5847 , \n          - 0.0454 ,   - 0.4149 ,    0.3927 ], \n         [ - 1.7365 ,    3.0447 ,    0.5115 ,    0.0786 ,   - 0.7544 ,   - 0.2158 ,   - 0.4876 , \n          - 0.2891 ,    0.5089 ,   - 0.6719 ], \n         [   0.3652 ,   - 0.5457 ,   - 0.1167 ,    2.9056 ,   - 1.1622 ,    0.8192 ,   - 1.3245 , \n          - 0.6414 ,    0.8097 ,   - 0.4958 ], \n         [ - 0.8755 ,   - 0.6983 ,    0.2208 ,   - 0.6463 ,    0.5276 ,    0.1145 ,    2.7229 , \n          - 1.0316 ,    0.1905 ,    0.2090 ], \n         [ - 0.9702 ,    0.1265 ,   - 0.0007 ,   - 0.5106 ,    0.4970 ,   - 0.0804 ,    0.0017 , \n           0.0607 ,    0.6164 ,    0.4490 ], \n         [ - 0.8271 ,   - 0.6822 ,   - 0.7434 ,    2.6457 ,   - 1.6143 ,    1.1486 ,   - 1.0705 , \n           0.5611 ,    0.6422 ,    0.1250 ], \n         [ - 1.9979 ,    1.8175 ,   - 0.1658 ,   - 0.0343 ,   - 0.6292 ,    0.1774 ,    0.3150 , \n          - 0.4633 ,    0.9266 ,    0.0252 ], \n         [ - 0.9039 ,   - 0.6030 ,   - 0.2173 ,   - 1.1768 ,    2.3198 ,   - 0.5072 ,    0.3418 , \n          - 0.1551 ,    0.1282 ,    1.4250 ], \n         [ - 0.9891 ,    0.5212 ,   - 0.4518 ,    0.3267 ,   - 0.0759 ,    0.3826 ,   - 0.0341 , \n           0.0382 ,    0.2451 ,    0.3658 ], \n         [ - 2.1217 ,    1.5102 ,   - 0.7828 ,    0.3554 ,   - 0.4192 ,   - 0.0772 ,    0.0578 , \n           0.8070 ,    0.1701 ,    0.5880 ], \n         [   1.0665 ,   - 1.3826 ,    0.6243 ,   - 0.8096 ,   - 0.4227 ,    0.5925 ,    1.8112 , \n          - 0.9946 ,    0.2010 ,   - 0.7731 ], \n         [ - 1.1263 ,   - 1.7484 ,    0.0041 ,   - 0.5439 ,    1.7242 ,   - 0.9475 ,   - 0.3835 , \n           0.8452 ,    0.3077 ,    2.2689 ]])    Printing output size  This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     if   iter_test   ==   1 : \n         print ( 'OUTPUTS' ) \n         print ( outputs . size ()) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 )    OUTPUTS  torch . Size ([ 100 ,   10 ])    Printing one output  This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.  number 0: -0.4181  number 1: -1.0784 ...  number 7: 2.9352 iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     if   iter_test   ==   1 : \n         print ( 'OUTPUTS' ) \n         print ( outputs [ 0 ,   :]) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 )    OUTPUTS\ntensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639,\n         2.9352, -0.1552,  0.8852])   Printing prediction output  Because our output is of size 100 (our batch size), our prediction size would also of the size 100.  iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n     if   iter_test   ==   1 : \n         print ( 'PREDICTION' ) \n         print ( predicted . size ())    PREDICTION  torch . Size ([ 100 ])    Print prediction value  We are printing our prediction which as verified above, should be digit 7.  iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n     if   iter_test   ==   1 : \n         print ( 'PREDICTION' ) \n         print ( predicted [ 0 ])    PREDICTION  tensor ( 7 )    Print prediction, label and label size  We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!  iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n     if   iter_test   ==   1 : \n         print ( 'PREDICTION' ) \n         print ( predicted [ 0 ]) \n\n         print ( 'LABEL SIZE' ) \n         print ( labels . size ()) \n\n         print ( 'LABEL FOR IMAGE 0' ) \n         print ( labels [ 0 ])    PREDICTION  tensor ( 7 )  LABEL   SIZE  torch . Size ([ 100 ])  LABEL   FOR   IMAGE   0  tensor ( 7 )    Print second prediction and ground truth  Again, the prediction is correct. Naturally, as our model is quite competent in this simple task.  iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n\n     if   iter_test   ==   1 : \n         print ( 'PREDICTION' ) \n         print ( predicted [ 1 ]) \n\n         print ( 'LABEL SIZE' ) \n         print ( labels . size ()) \n\n         print ( 'LABEL FOR IMAGE 1' ) \n         print ( labels [ 1 ])    PREDICTION  tensor ( 2 )  LABEL   SIZE  torch . Size ([ 100 ])  LABEL   FOR   IMAGE   1  tensor ( 2 )    Print accuracy  Now we know what each object represents, we can understand how we arrived at our accuracy numbers.  One last thing to note is that  correct.item()  has this syntax is because  correct  is a PyTorch tensor and to get the value to compute with  total  which is an integer, we need to do this. correct   =   0  total   =   0  iter_test   =   0  for   images ,   labels   in   test_loader : \n     iter_test   +=   1 \n     images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n     outputs   =   model ( images ) \n     _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n\n     # Total number of labels \n     total   +=   labels . size ( 0 ) \n\n     # Total correct predictions \n     correct   +=   ( predicted   ==   labels ) . sum ()  accuracy   =   100   *   ( correct . item ()   /   total )  print ( accuracy )    82.94    Explanation of Python's .sum() function  Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return  True  or in our case, those predictions that match actual labels (correct predictions).  # Explaining .sum() python built-in function  # correct += (predicted == labels).sum()  import   numpy   as   np  a   =   np . ones (( 10 ))  print ( a )  b   =   np . ones (( 10 ))  print ( b )  print ( a   ==   b )  print (( a   ==   b ) . sum ())    # matrix a  [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1. ]  # matrix b  [ 1.   1.   1.   1.   1.   1.   1.   1.   1.   1. ]  # boolean array  [   True    True    True    True    True    True    True    True    True    True ]  # number of elementswhere a matches b  10",
            "title": "Break Down Accuracy Calculation"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#saving-model",
            "text": "Saving PyTorch model  This is how you save your model. Feel free to just change  save_model = True  to save your model save_model   =   False  if   save_model   is   True : \n     # Saves only parameters \n     torch . save ( model . state_dict (),   'awesome_model.pkl' )",
            "title": "Saving Model"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch-gpu",
            "text": "CPU version  The usual 7-step process, getting repetitive by now which we like.   import   torch  import   torch.nn   as   nn  import   torchvision.transforms   as   transforms  import   torchvision.datasets   as   dsets  '''  STEP 1: LOADING DATASET  '''  train_dataset   =   dsets . MNIST ( root = './data' ,  \n                             train = True ,  \n                             transform = transforms . ToTensor (), \n                             download = True )  test_dataset   =   dsets . MNIST ( root = './data' ,  \n                            train = False ,  \n                            transform = transforms . ToTensor ())  '''  STEP 2: MAKING DATASET ITERABLE  '''  batch_size   =   100  n_iters   =   3000  num_epochs   =   n_iters   /   ( len ( train_dataset )   /   batch_size )  num_epochs   =   int ( num_epochs )  train_loader   =   torch . utils . data . DataLoader ( dataset = train_dataset ,  \n                                            batch_size = batch_size ,  \n                                            shuffle = True )  test_loader   =   torch . utils . data . DataLoader ( dataset = test_dataset ,  \n                                           batch_size = batch_size ,  \n                                           shuffle = False )  '''  STEP 3: CREATE MODEL CLASS  '''  class   LogisticRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_size ,   num_classes ): \n         super ( LogisticRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim ) \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out  '''  STEP 4: INSTANTIATE MODEL CLASS  '''  input_dim   =   28 * 28  output_dim   =   10  model   =   LogisticRegressionModel ( input_dim ,   output_dim )  '''  STEP 5: INSTANTIATE LOSS CLASS  '''  criterion   =   nn . CrossEntropyLoss ()  '''  STEP 6: INSTANTIATE OPTIMIZER CLASS  '''  learning_rate   =   0.001  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )  '''  STEP 7: TRAIN THE MODEL  '''  iter   =   0  for   epoch   in   range ( num_epochs ): \n     for   i ,   ( images ,   labels )   in   enumerate ( train_loader ): \n         # Load images as Variable \n         images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n         labels   =   labels \n\n         # Clear gradients w.r.t. parameters \n         optimizer . zero_grad () \n\n         # Forward pass to get output/logits \n         # 100 x 10 \n         outputs   =   model ( images ) \n\n         # Calculate Loss: softmax --> cross entropy loss \n         loss   =   criterion ( outputs ,   labels ) \n\n         # Getting gradients w.r.t. parameters \n         loss . backward () \n\n         # Updating parameters \n         optimizer . step () \n\n         iter   +=   1 \n\n         if   iter   %   500   ==   0 : \n             # Calculate Accuracy          \n             correct   =   0 \n             total   =   0 \n             # Iterate through test dataset \n             for   images ,   labels   in   test_loader : \n                 # Load images to a Torch Variable \n                 images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () \n\n                 # Forward pass only to get logits/output \n                 outputs   =   model ( images ) \n\n                 # Get predictions from the maximum value \n                 # 100 x 1 \n                 _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n\n                 # Total number of labels \n                 total   +=   labels . size ( 0 ) \n\n                 # Total correct predictions \n                 correct   +=   ( predicted   ==   labels ) . sum () \n\n             accuracy   =   100   *   correct . item ()   /   total \n\n             # Print Loss \n             print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter ,   loss . item (),   accuracy ))    Iteration :   500.   Loss :   1.876196026802063 .   Accuracy :   64.44  Iteration :   1000.   Loss :   1.5153584480285645 .   Accuracy :   75.68  Iteration :   1500.   Loss :   1.3521136045455933 .   Accuracy :   78.98  Iteration :   2000.   Loss :   1.2136967182159424 .   Accuracy :   80.95  Iteration :   2500.   Loss :   1.0934826135635376 .   Accuracy :   81.97  Iteration :   3000.   Loss :   1.024120569229126 .   Accuracy :   82.49    GPU version  2 things must be on GPU -  model  -  variables  Remember step 4 and 7 will be affected and this will be the same for all model building moving forward.  import   torch  import   torch.nn   as   nn  import   torchvision.transforms   as   transforms  import   torchvision.datasets   as   dsets  '''  STEP 1: LOADING DATASET  '''  train_dataset   =   dsets . MNIST ( root = './data' ,  \n                             train = True ,  \n                             transform = transforms . ToTensor (), \n                             download = True )  test_dataset   =   dsets . MNIST ( root = './data' ,  \n                            train = False ,  \n                            transform = transforms . ToTensor ())  '''  STEP 2: MAKING DATASET ITERABLE  '''  batch_size   =   100  n_iters   =   3000  num_epochs   =   n_iters   /   ( len ( train_dataset )   /   batch_size )  num_epochs   =   int ( num_epochs )  train_loader   =   torch . utils . data . DataLoader ( dataset = train_dataset ,  \n                                            batch_size = batch_size ,  \n                                            shuffle = True )  test_loader   =   torch . utils . data . DataLoader ( dataset = test_dataset ,  \n                                           batch_size = batch_size ,  \n                                           shuffle = False )  '''  STEP 3: CREATE MODEL CLASS  '''  class   LogisticRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_size ,   num_classes ): \n         super ( LogisticRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim ) \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out  '''  STEP 4: INSTANTIATE MODEL CLASS  '''  input_dim   =   28 * 28  output_dim   =   10  model   =   LogisticRegressionModel ( input_dim ,   output_dim )  #######################  #  USE GPU FOR MODEL  #  #######################  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  model . to ( device )  '''  STEP 5: INSTANTIATE LOSS CLASS  '''  criterion   =   nn . CrossEntropyLoss ()  '''  STEP 6: INSTANTIATE OPTIMIZER CLASS  '''  learning_rate   =   0.001  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )  '''  STEP 7: TRAIN THE MODEL  '''  iter   =   0  for   epoch   in   range ( num_epochs ): \n     for   i ,   ( images ,   labels )   in   enumerate ( train_loader ): \n\n         ####################### \n         #  USE GPU FOR MODEL  # \n         ####################### \n         images   =   images . view ( - 1 ,   28 * 28 ) . requires_grad_ () . to ( device ) \n         labels   =   labels . to ( device ) \n\n         # Clear gradients w.r.t. parameters \n         optimizer . zero_grad () \n\n         # Forward pass to get output/logits \n         outputs   =   model ( images ) \n\n         # Calculate Loss: softmax --> cross entropy loss \n         loss   =   criterion ( outputs ,   labels ) \n\n         # Getting gradients w.r.t. parameters \n         loss . backward () \n\n         # Updating parameters \n         optimizer . step () \n\n         iter   +=   1 \n\n         if   iter   %   500   ==   0 : \n             # Calculate Accuracy          \n             correct   =   0 \n             total   =   0 \n             # Iterate through test dataset \n             for   images ,   labels   in   test_loader : \n                 ####################### \n                 #  USE GPU FOR MODEL  # \n                 ####################### \n                 images   =   images . view ( - 1 ,   28 * 28 ) . to ( device ) \n\n                 # Forward pass only to get logits/output \n                 outputs   =   model ( images ) \n\n                 # Get predictions from the maximum value \n                 _ ,   predicted   =   torch . max ( outputs . data ,   1 ) \n\n                 # Total number of labels \n                 total   +=   labels . size ( 0 ) \n\n                 ####################### \n                 #  USE GPU FOR MODEL  # \n                 ####################### \n                 # Total correct predictions \n                 if   torch . cuda . is_available (): \n                     correct   +=   ( predicted . cpu ()   ==   labels . cpu ()) . sum () \n                 else : \n                     correct   +=   ( predicted   ==   labels ) . sum () \n\n             accuracy   =   100   *   correct . item ()   /   total \n\n             # Print Loss \n             print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter ,   loss . item (),   accuracy ))    Iteration :   500.   Loss :   1.8571407794952393 .   Accuracy :   68.99  Iteration :   1000.   Loss :   1.5415704250335693 .   Accuracy :   75.86  Iteration :   1500.   Loss :   1.2755383253097534 .   Accuracy :   78.92  Iteration :   2000.   Loss :   1.2468739748001099 .   Accuracy :   80.72  Iteration :   2500.   Loss :   1.0708973407745361 .   Accuracy :   81.73  Iteration :   3000.   Loss :   1.0359245538711548 .   Accuracy :   82.74",
            "title": "Building a Logistic Regression Model with PyTorch (GPU)"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_logistic_regression/#summary",
            "text": "We've learnt to...   Success     Logistic regression  basics    Problems  of  linear regression    In-depth  Logistic Regression   Get logits   Get softmax   Get cross-entropy loss      Aim : reduce cross-entropy loss   Built a  logistic regression model  in  CPU and GPU   Step 1: Load Dataset   Step 2: Make Dataset Iterable   Step 3: Create Model Class   Step 4: Instantiate Model Class   Step 5: Instantiate Loss Class   Step 6: Instantiate Optimizer Class   Step 7: Train Model     Important things to be on  GPU    model    tensors with gradients",
            "title": "Summary"
        },
        {
            "location": "/news/news/",
            "text": "Welcome to our Blog\n\u00b6\n\n\nHere, we post news related to Deep Learning Wizard's releases, features and achievements \n\n\nNotable News\n\u00b6\n\n\n\n\n Facebook PyTorch Developer Conference, San Francisco, USA, September 2018\n\n\n Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018\n\n\n Reached 2200+ students, 2018\n\n\n Featured on PyTorch Website, January 2018\n\n\n Reached 1000+ students, 2017\n\n\n Hosted NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017\n\n\n NVIDIA Inception Partner, May 2017\n\n\n And more...",
            "title": "Welcome"
        },
        {
            "location": "/news/news/#welcome-to-our-blog",
            "text": "Here, we post news related to Deep Learning Wizard's releases, features and achievements",
            "title": "Welcome to our Blog"
        },
        {
            "location": "/news/news/#notable-news",
            "text": "Facebook PyTorch Developer Conference, San Francisco, USA, September 2018   Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018   Reached 2200+ students, 2018   Featured on PyTorch Website, January 2018   Reached 1000+ students, 2017   Hosted NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017   NVIDIA Inception Partner, May 2017   And more...",
            "title": "Notable News"
        },
        {
            "location": "/news/facebook_pytorch_developer_conference_2018_09_05/",
            "text": "Facebook PyTorch Developer Conference\n\u00b6\n\n\nWe are heading down!\n\u00b6\n\n\n\n\nIn barely 2 short years, PyTorch (Facebook) will be hosting their first \nPyTorch Developer Conference\n in San Francisco, USA.\n\n\nI will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. \n\n\nThe PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it!\n\n\nCheers,\n\nRitchie Ng",
            "title": "Facebook PyTorch Developer Conference, San Francisco, September 2018"
        },
        {
            "location": "/news/facebook_pytorch_developer_conference_2018_09_05/#facebook-pytorch-developer-conference",
            "text": "",
            "title": "Facebook PyTorch Developer Conference"
        },
        {
            "location": "/news/facebook_pytorch_developer_conference_2018_09_05/#we-are-heading-down",
            "text": "In barely 2 short years, PyTorch (Facebook) will be hosting their first  PyTorch Developer Conference  in San Francisco, USA.  I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there.   The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it!  Cheers, Ritchie Ng",
            "title": "We are heading down!"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/",
            "text": "NVIDIA Workshop at NUS-MIT-NUHS Datathon\n\u00b6\n\n\nImage Recognition Workshop by Ritchie Ng\n\u00b6\n\n\nThe NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.\n\n\nIn \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.\n\n\nLink to the NUS-MIT-NUHS Datathon \nworkshop\n.",
            "title": "NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/#nvidia-workshop-at-nus-mit-nuhs-datathon",
            "text": "",
            "title": "NVIDIA Workshop at NUS-MIT-NUHS Datathon"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng",
            "text": "The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.  In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.  Link to the NUS-MIT-NUHS Datathon  workshop .",
            "title": "Image Recognition Workshop by Ritchie Ng"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/",
            "text": "Featured on PyTorch Website\n\u00b6\n\n\nPyTorch a Year Later\n\u00b6\n\n\n\n\nWe are featured on \nPyTorch website's post\n \n\n\nI used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.\n\n\nA year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!\n\n\nA big shoutout for \nAlfredo Canziani\n who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.\n\n\nTo more great years ahead for PyTorch \n\n\nCheers,\n\nRitchie Ng",
            "title": "Featured on PyTorch Website 2018"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website",
            "text": "",
            "title": "Featured on PyTorch Website"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later",
            "text": "We are featured on  PyTorch website's post    I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.  A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!  A big shoutout for  Alfredo Canziani  who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.  To more great years ahead for PyTorch   Cheers, Ritchie Ng",
            "title": "PyTorch a Year Later"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/",
            "text": "NVIDIA Self-Driving Cars and Healthcare Workshop\n\u00b6\n\n\nHosted by Ritchie Ng\n\u00b6\n\n\nA talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.\n\n\nWe will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.\n\n\nDetails:\nWednesday, June 21\nst\n\n1:00 PM to 3:30 PM\n\n\nThe Hangar by NUS Enterprise\n21 Heng Mui Keng Terrace, Singapore 119613",
            "title": "NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/#nvidia-self-driving-cars-and-healthcare-workshop",
            "text": "",
            "title": "NVIDIA Self-Driving Cars and Healthcare Workshop"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng",
            "text": "A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.  We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.  Details:\nWednesday, June 21 st \n1:00 PM to 3:30 PM  The Hangar by NUS Enterprise\n21 Heng Mui Keng Terrace, Singapore 119613",
            "title": "Hosted by Ritchie Ng"
        },
        {
            "location": "/news/deep_learning_wizard_nvidia_inception_2018_05_01/",
            "text": "We Are an NVIDIA Inception Partner\n\u00b6\n\n\nWe did it!\n\u00b6\n\n\nAfter almost a year, we are an NVIDIA Inception Partner now! \n\n\n\n\n\"NVIDIA Inception Partner\n\n\n\n\nDeep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. \n\n\n\n\nCheers,\n\nRitchie Ng",
            "title": "NVIDIA Inception Partner Status, Singapore, May 2017"
        },
        {
            "location": "/news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-are-an-nvidia-inception-partner",
            "text": "",
            "title": "We Are an NVIDIA Inception Partner"
        },
        {
            "location": "/news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-did-it",
            "text": "After almost a year, we are an NVIDIA Inception Partner now!    \"NVIDIA Inception Partner   Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.    Cheers, Ritchie Ng",
            "title": "We did it!"
        }
    ]
}