{
    "docs": [
        {
            "location": "/",
            "text": "About Us\n\u00b6\n\n\nWe deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our \nvideo course\n and join over 2300 deep learning wizards. \n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nPyTorch as our Preferred Deep Learning Library\n\u00b6\n\n\nWe chose \nPyTorch\n because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.\n\n\n# It is this easy! \n\n\nimport\n \ntorch\n\n\n\n# Create a variable of value 1 each.\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\nb\n \n=\n \ntorch\n.\nTensor\n([\n1\n])\n\n\n\n# Add the 2 variables to give you 2, it's that simple!\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\n\n\n\nMade for Visual and Book Lovers\n\u00b6\n\n\nWe are visual creatures, that is why we offer detailed \nvideo courses\n on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. \n\n\nFor book lovers, you will be happy to know \nDeep Learning Wizard's wikipedia\n will always be updated first prior to our release of video courses.\n\n\nExperienced Research and Applied Team\n\u00b6\n\n\n\n\nRitchie Ng\n\n\nCurrently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.\n\n\nMy passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.\n\n\nI was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.\n\n\nCheck out my profile link at \nritchieng.com\n\n\n\n\n\n\nJie Fu\n\n\nI am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.\n\n\nI earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.\n\n\nI am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.\n\n\nCheck out my profile link at \nbigaidream.github.io",
            "title": "Home"
        },
        {
            "location": "/#about-us",
            "text": "We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our  video course  and join over 2300 deep learning wizards.   To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.",
            "title": "About Us"
        },
        {
            "location": "/#pytorch-as-our-preferred-deep-learning-library",
            "text": "We chose  PyTorch  because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook.  # It is this easy!   import   torch  # Create a variable of value 1 each.  a   =   torch . Tensor ([ 1 ])  b   =   torch . Tensor ([ 1 ])  # Add the 2 variables to give you 2, it's that simple!  c   =   a   +   b",
            "title": "PyTorch as our Preferred Deep Learning Library"
        },
        {
            "location": "/#made-for-visual-and-book-lovers",
            "text": "We are visual creatures, that is why we offer detailed  video courses  on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch.   For book lovers, you will be happy to know  Deep Learning Wizard's wikipedia  will always be updated first prior to our release of video courses.",
            "title": "Made for Visual and Book Lovers"
        },
        {
            "location": "/#experienced-research-and-applied-team",
            "text": "Ritchie Ng  Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA.  My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial.  I was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI.  Check out my profile link at  ritchieng.com    Jie Fu  I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal.  I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low.  I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner.  Check out my profile link at  bigaidream.github.io",
            "title": "Experienced Research and Applied Team"
        },
        {
            "location": "/supporters/",
            "text": "Supporters\n\u00b6\n\n\nMore than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.\n\n\nIndividuals\n\u00b6\n\n\n\n\nAlfredo Canziani\n\n\nAlfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.\n\n\nHe is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun.\n\n\nDo check out his latest \nmini-course on PyTorch\n that was held in Princeton University.\n\n\n\n\n\n\nMarek Bardonski\n\n\nSince graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.\n\n\nNASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.\n\n\nSince his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at \nSigmoidal\n.\n\n\n\n\nCorporations\n\u00b6\n\n\n\n\n NVIDIA (NVIDIA Inception Partner)\n\n\n Facebook\n\n\n Amazon\n\n\n\n\nResearch Institutions\n\u00b6\n\n\n\n\n Montreal Institute of Learning Algorithms (MILA), Montreal, Canada\n\n\n Imperial College London, UK\n\n\n Massachusetts Institute of Technology (MIT), USA  \n\n\n National University of Singapore (NUS), Singapore\n\n\n Nanyang Technological University (NTU), Singapore\n\n\n\n\nNVIDIA Inception Partner\n\u00b6\n\n\n\n\n\"NVIDIA Inception Partner\n\n\n\n\nDeep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#supporters",
            "text": "More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.",
            "title": "Supporters"
        },
        {
            "location": "/supporters/#individuals",
            "text": "Alfredo Canziani  Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch.  He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun.  Do check out his latest  mini-course on PyTorch  that was held in Princeton University.    Marek Bardonski  Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months.  NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation.  Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at  Sigmoidal .",
            "title": "Individuals"
        },
        {
            "location": "/supporters/#corporations",
            "text": "NVIDIA (NVIDIA Inception Partner)   Facebook   Amazon",
            "title": "Corporations"
        },
        {
            "location": "/supporters/#research-institutions",
            "text": "Montreal Institute of Learning Algorithms (MILA), Montreal, Canada   Imperial College London, UK   Massachusetts Institute of Technology (MIT), USA     National University of Singapore (NUS), Singapore   Nanyang Technological University (NTU), Singapore",
            "title": "Research Institutions"
        },
        {
            "location": "/supporters/#nvidia-inception-partner",
            "text": "\"NVIDIA Inception Partner   Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.",
            "title": "NVIDIA Inception Partner"
        },
        {
            "location": "/review/",
            "text": "Reviews\n\u00b6\n\n\nTo this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.\n\n\nThese are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.\n\n\n\n\nRoberto Trevi\u00f1o Cervantes\n\n\nCongratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.\n\n\n\n\n\n\nMuktabh Mayank\n\n\nThis course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.\n\n\n\n\n\n\nCharles Neiswender\n\n\nI really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.\n\n\n\n\n\n\nIan Lipton\n\n\nThis was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.\n\n\n\n\nAnd check out hundreds of more reviews for our \nvideo course\n!",
            "title": "Reviews"
        },
        {
            "location": "/review/#reviews",
            "text": "To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.  These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors.   Roberto Trevi\u00f1o Cervantes  Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year.    Muktabh Mayank  This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch.    Charles Neiswender  I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing.    Ian Lipton  This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math.   And check out hundreds of more reviews for our  video course !",
            "title": "Reviews"
        },
        {
            "location": "/deep_learning/intro/",
            "text": "Deep Learning Theory and Programming Tutorials\n\u00b6\n\n\nOur main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our \nvideo course\n.\n\n\n\n\nWork-in-progress\n\n\nThis open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact \nRitchie Ng\n if you would like to contribute via our \nFacebook\n page.\n\n\nAlso take note that these notes are best used as a referral if you are new to deep learning and programming. Go head and take our \nvideo course\n that provides a much easier experience.\n\n\nAll of our code allows you to run in a notebook for this deep learning section. Please use a \njupyter notebook\n and run the examples from the start of the page to the end.",
            "title": "Introduction"
        },
        {
            "location": "/deep_learning/intro/#deep-learning-theory-and-programming-tutorials",
            "text": "Our main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our  video course .   Work-in-progress  This open-source portion is still a work in progress. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact  Ritchie Ng  if you would like to contribute via our  Facebook  page.  Also take note that these notes are best used as a referral if you are new to deep learning and programming. Go head and take our  video course  that provides a much easier experience.  All of our code allows you to run in a notebook for this deep learning section. Please use a  jupyter notebook  and run the examples from the start of the page to the end.",
            "title": "Deep Learning Theory and Programming Tutorials"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/",
            "text": "PyTorch Fundamentals - Matrices\n\u00b6\n\n\nMatrices\n\u00b6\n\n\nMatrices Brief Introduction\n\u00b6\n\n\n\n\n Basic definition: rectangular array of numbers.\n\n\n Tensors (PyTorch)\n\n\n Ndarrays (NumPy)\n\n\n\n\n2 x 2 Matrix (R x C)\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n2 x 3 Matrix\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nCreating Matrices\n\u00b6\n\n\n\n\nCreate list\n\n\n# Creating a 2x2 array\n\n\narr\n \n=\n \n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\nprint\n(\narr\n)\n\n\n\n\n\n\n\n[[\n1\n,\n \n2\n],\n \n[\n3\n,\n \n4\n]]\n\n\n\n\n\n\n\nCreate numpy array via list\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n\n# Convert to NumPy\n\n\nnp\n.\narray\n(\narr\n)\n\n\n\n\n\n\narray\n([[\n1\n,\n \n2\n],\n\n       \n[\n3\n,\n \n4\n]])\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\nimport\n \ntorch\n\n\n\n\n\n# Convert to PyTorch Tensor\n\n\ntorch\n.\nTensor\n(\narr\n)\n\n\n\n\n\n\n\n1\n  \n2\n\n\n3\n  \n4\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nCreate Matrices with Default Values\n\u00b6\n\n\n\n\nCreate 2x2 numpy array of 1's\n\n\nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\narray\n([[\n \n1.\n,\n  \n1.\n],\n\n       \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\nCreate 2x2 torch tensor of 1's\n\n\ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\n\n\n 1  1\n 1  1\n[torch.FloatTensor of size 2x2]\n\n\n\n\n\n\nCreate 2x2 numpy array of random numbers\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\narray\n([[\n \n0.68270631\n,\n  \n0.87721678\n],\n\n       \n[\n \n0.07420986\n,\n  \n0.79669375\n]])\n\n\n\n\n\n\n\nCreate 2x2 PyTorch tensor of random numbers\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n0.3900\n  \n0.8268\n\n\n0.3888\n  \n0.5914\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nSeeds for Reproducibility\n\u00b6\n\n\n\n\nWhy do we need seeds?\n\n\nWe need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.\n\n\n\n\n\n\nCreate seed to enable fixed numbers for random number generation \n\n\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\nRepeat random array generation to check\n\n\nIf you do not set the seed, you would not get the same set of numbers like here.\n\n# Seed\n\n\nnp\n.\nrandom\n.\nseed\n(\n0\n)\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.5488135\n \n,\n  \n0.71518937\n],\n\n       \n[\n \n0.60276338\n,\n  \n0.54488318\n]])\n\n\n\n\n\n\n\nCreate a numpy array without seed\n\n\nNotice how you get different numbers compared to the first 2 tries?\n\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.56804456\n,\n  \n0.92559664\n],\n\n       \n[\n \n0.07103606\n,\n  \n0.0871293\n \n]])\n\n\n\n\n\n\n\nRepeat numpy array generation without seed\n\n\nYou get the point now, you get a totally different set of numbers.\n\n# No seed\n\n\nnp\n.\nrandom\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\narray\n([[\n \n0.0202184\n \n,\n  \n0.83261985\n],\n\n       \n[\n \n0.77815675\n,\n  \n0.87001215\n]])\n\n\n\n\n\n\n\nCreate a PyTorch tensor with a fixed seed\n\n\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n\n\nRepeat creating a PyTorch fixed seed tensor\n\n\n# Torch Seed\n\n\ntorch\n.\nmanual_seed\n(\n0\n)\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n\n0.5488\n  \n0.5928\n\n\n0.7152\n  \n0.8443\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreating a PyTorch tensor without seed\n\n\nLike with a numpy array of random numbers without seed, you will not get the same results as above.\n\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n0.6028\n  \n0.8579\n\n\n0.5449\n  \n0.8473\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nRepeat creating a PyTorch tensor without seed\n\n\nNotice how these are different numbers again?\n\n# Torch No Seed\n\n\ntorch\n.\nrand\n(\n2\n,\n \n2\n)\n\n\n\n\n\n\n0.4237\n  \n0.6236\n\n\n0.6459\n  \n0.3844\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nSeed for GPU is different for now...\n\n\n\n\nFix a seed for GPU tensors\n\n\nWhen you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. \n\nif\n \ntorch\n.\ncuda\n.\nis_available\n():\n\n    \ntorch\n.\ncuda\n.\nmanual_seed_all\n(\n0\n)\n\n\n\n\n\n\nNumPy and Torch Bridge\n\u00b6\n\n\nNumPy to Torch\n\u00b6\n\n\n\n\nCreate a numpy array of 1's\n\n\n# Numpy array\n\n\nnp_array\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n\n\nprint\n(\nnp_array\n)\n\n\n\n\n\n\n\n[[\n \n1.\n  \n1.\n]\n\n\n[\n \n1.\n  \n1.\n]]\n\n\n\n\n\n\n\nGet the type of class for the numpy array\n\n\nprint\n(\ntype\n(\nnp_array\n))\n\n\n\n\n\n\n\n<\nclass\n \n'\nnumpy\n.\nndarray\n'>\n\n\n\n\n\n\n\nConvert numpy array to PyTorch tensor\n\n\n# Convert to Torch Tensor\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nfrom_numpy\n(\nnp_array\n)\n\n\n\n\n\nprint\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nGet type of class for PyTorch tensor\n\n\nNotice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type.\n\nprint\n(\ntype\n(\ntorch_tensor\n))\n\n\n\n\n\n\n<\nclass\n \n'\ntorch\n.\nDoubleTensor\n'>\n\n\n\n\n\n\n\nCreate PyTorch tensor from a different numpy datatype\n\n\nYou will get an error running this code because PyTorch tensor don't support all datatype. \n\n# Data types matter: intentional error\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n---------------------------------------------------------------------------\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n57\n-\nb8b085f9b39d\n>\n \nin\n \n<\nmodule\n>\n()\n\n      \n1\n \n# Data types matter\n\n      \n2\n \nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint8\n)\n\n\n---->\n \n3\n \ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\nRuntimeError\n:\n \ncan\n't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\n\n\n\n\n\n\n\nWhat conversion support does Numpy to PyTorch tensor bridge gives?\n\n\n\n\ndouble\n\n\nfloat\n \n\n\nint64\n, \nint32\n, \nuint8\n \n\n\n\n\n\n\n\n\nCreate PyTorch long tensor\n\n\nSee how a int64 numpy array gives you a PyTorch long tensor?\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n1  1\n1  1\n[torch.LongTensor of size 2x2]\n\n\n\n\n\n\nCreate PyTorch int tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nint32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nIntTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch byte tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nuint8\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nByteTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch Double Tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat64\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\nAlternatively you can do this too via \nnp.double\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\ndouble\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nDoubleTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreate PyTorch Float Tensor\n\n\n# Data types matter\n\n\nnp_array_new\n \n=\n \nnp\n.\nones\n((\n2\n,\n \n2\n),\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\ntorch\n.\nfrom_numpy\n(\nnp_array_new\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nTensor Type Bug Guide\n\n\nThese things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!\n\n\n\n\n\n\n\n\n\n\nNumPy Array Type\n\n\nTorch Tensor Type\n\n\n\n\n\n\n\n\n\n\nint64\n\n\nLongTensor\n\n\n\n\n\n\nint32\n\n\nIntegerTensor\n\n\n\n\n\n\nuint8\n\n\nByteTensor\n\n\n\n\n\n\nfloat64\n\n\nDoubleTensor\n\n\n\n\n\n\nfloat32\n\n\nFloatTensor\n\n\n\n\n\n\ndouble\n\n\nDoubleTensor\n\n\n\n\n\n\n\n\nTorch to NumPy\n\u00b6\n\n\n\n\nCreate PyTorch tensor of 1's\n\n\nYou would realize this defaults to a float tensor by default if you do this.\n\n\ntorch_tensor\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\n\ntype\n(\ntorch_tensor\n)\n\n\n\n\n\n\n\ntorch\n.\nFloatTensor\n\n\n\n\n\n\n\nConvert tensor to numpy\n\n\nIt's as simple as this.\n\n\ntorch_to_numpy\n \n=\n \ntorch_tensor\n.\nnumpy\n()\n\n\n\n\n\ntype\n(\ntorch_to_numpy\n)\n\n\n\n\n\n\n\n# Wowza, we did it.\n\n\nnumpy\n.\nndarray\n\n\n\n\n\nTensors on CPU vs GPU\n\u00b6\n\n\n\n\nMove tensor to CPU and back\n\n\nThis by default creates a tensor on CPU. You do not need to do anything.\n\n# CPU\n\n\ntensor_cpu\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\nIf you would like to send a tensor to your GPU, you just need to do a simple \n.cuda\n()\n\n\n# CPU to GPU\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\ntensor_cpu\n.\nto\n(\ndevice\n)\n\n\n\n\n\nAnd if you want to move that tensor on the GPU back to the CPU, just do the following.\n\n\n# GPU to CPU\n\n\ntensor_cpu\n.\ncpu\n()\n\n\n\n\n\n\n\nTensor Operations\n\u00b6\n\n\nResizing Tensor\n\u00b6\n\n\n\n\nCreating a 2x2 tensor\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nGetting size of tensor\n\n\nprint\n(\na\n.\nsize\n())\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n2\n,\n \n2\n])\n\n\n\n\n\n\n\nResize tensor to 4x1\n\n\na\n.\nview\n(\n4\n)\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n4\n]\n\n\n\n\n\n\n\nGet size of resized tensor\n\n\na\n.\nview\n(\n4\n)\n.\nsize\n()\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n4\n])\n\n\n\n\n\nElement-wise Addition\n\u00b6\n\n\n\n\nCreating first 2x2 tensor\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nCreating second 2x2 tensor\n\n\nb\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise addition of 2 tensors\n\n\n# Element-wise addition\n\n\nc\n \n=\n \na\n \n+\n \nb\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nAlternative element-wise addition of 2 tensors\n\n\n# Element-wise addition\n\n\nc\n \n=\n \ntorch\n.\nadd\n(\na\n,\n \nb\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nIn-place element-wise addition\n\n\nThis would replace the c tensor values with the new addition. \n\n\n# In-place addition\n\n\nprint\n(\n'Old c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\nc\n.\nadd_\n(\na\n)\n\n\n\nprint\n(\n'-'\n*\n60\n)\n\n\nprint\n(\n'New c tensor'\n)\n\n\nprint\n(\nc\n)\n\n\n\n\n\n\n\nOld\n \nc\n \ntensor\n\n\n \n2\n  \n2\n\n \n2\n  \n2\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n------------------------------------------------------------\n\n\nNew\n \nc\n \ntensor\n\n\n \n3\n  \n3\n\n \n3\n  \n3\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-wise Subtraction\n\u00b6\n\n\n\n\nCheck values of tensor a and b'\n\n\nTake note that you've created tensor a and b of sizes 2x2 filled with 1's each above. \n\nprint\n(\na\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n \n1\n  \n1\n\n \n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 1\n\n\na\n \n-\n \nb\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 2\n\n\n# Not in-place\n\n\nprint\n(\na\n.\nsub\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise subtraction: method 3\n\n\nThis will replace a with the final result filled with 2's\n\n# Inplace\n\n\nprint\n(\na\n.\nsub_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-Wise Multiplication\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 1\n\n\na\n \n*\n \nb\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 2\n\n\n# Not in-place\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise multiplication: method 3\n\n\n# In-place\n\n\nprint\n(\na\n.\nmul_\n(\nb\n))\n\n\nprint\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nElement-Wise Division\n\u00b6\n\n\n\n\nCreate tensor a and b of sizes 2x2 filled with 1's and 0's\n\n\na\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\nprint\n(\na\n)\n\n\nb\n \n=\n \ntorch\n.\nzeros\n(\n2\n,\n \n2\n)\n\n\nprint\n(\nb\n)\n\n\n\n\n\n\n\n1\n  \n1\n\n\n1\n  \n1\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 1\n\n\nb\n \n/\n \na\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 2\n\n\ntorch\n.\ndiv\n(\nb\n,\n \na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\n\n\nElement-wise division: method 3\n\n\n# Inplace\n\n\nb\n.\ndiv_\n(\na\n)\n\n\n\n\n\n\n\n0\n  \n0\n\n\n0\n  \n0\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx2\n]\n\n\n\n\n\nTensor Mean\n\u00b6\n\n\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55\n\n\n\n\n\n\n mean = 55 /10 = 5.5 \n\n\n mean = 55 /10 = 5.5 \n\n\n\n\n\n\nCreate tensor of size 10 filled from 1 to 10\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nsize\n()\n\n\n\n\n\n\n\ntorch\n.\nSize\n([\n10\n])\n\n\n\n\n\n\n\nGet tensor mean\n\n\nHere we get 5.5 as we've calculated manually above.\n\n\na\n.\nmean\n(\ndim\n=\n0\n)\n\n\n\n\n\n\n\n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.\n\n\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\nRuntimeError\n                              \nTraceback\n \n(\nmost\n \nrecent\n \ncall\n \nlast\n)\n\n\n\n<\nipython\n-\ninput\n-\n7\n-\n81\naec0cf1c00\n>\n \nin\n \n<\nmodule\n>\n()\n\n\n---->\n \n1\n \na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\nRuntimeError\n:\n \ndimension\n \nout\n \nof\n \nrange\n \n(\nexpected\n \nto\n \nbe\n \nin\n \nrange\n \nof\n \n[\n-\n1\n,\n \n0\n],\n \nbut\n \ngot\n \n1\n)\n\n\n\n\n\n\n\nCreate a 2x10 Tensor, of 1-10 digits each\n\n\na\n \n=\n \ntorch\n.\nTensor\n([[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n],\n \n[\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n]])\n\n\n\n\na\n.\nsize\n()\n\n\n\n\n\n\ntorch\n.\nSize\n([\n2\n,\n \n10\n])\n\n\n\n\n\n\n\nGet tensor mean on second dimension\n\n\nHere we won't get an error like previously because we've a tensor of size 2x10\n\n\na\n.\nmean\n(\ndim\n=\n1\n)\n\n\n\n\n\n\n\n \n5.5000\n\n \n5.5000\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n2\nx1\n]\n\n\n\n\n\nTensor Standard Deviation\n\u00b6\n\n\n\n\nGet standard deviation of tensor\n\n\n\n\na\n \n=\n \ntorch\n.\nTensor\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n])\n\n\na\n.\nstd\n(\ndim\n=\n0\n)\n\n\n\n\n\n \n3.0277\n\n\n[\ntorch\n.\nFloatTensor\n \nof\n \nsize\n \n1\n]\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Create Matrices\n\n\n Create Matrices with Default Initialization Values\n\n\n Zeros \n\n\n Ones\n\n\n\n\n\n\n Initialize Seeds for Reproducibility on GPU and CPU\n\n\n Convert Matrices: NumPy to Torch and Torch to NumPy\n\n\n Move Tensors: CPU to GPU and GPU to CPU\n\n\n Run Important Tensor Operations\n\n\n Element-wise addition, subtraction, multiplication and division\n\n\n Resize\n\n\n Calculate mean \n\n\n Calculate standard deviation",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#pytorch-fundamentals-matrices",
            "text": "",
            "title": "PyTorch Fundamentals - Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices",
            "text": "",
            "title": "Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction",
            "text": "Basic definition: rectangular array of numbers.   Tensors (PyTorch)   Ndarrays (NumPy)   2 x 2 Matrix (R x C)     1  1      1  1     2 x 3 Matrix     1  1  1      1  1  1",
            "title": "Matrices Brief Introduction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices",
            "text": "Create list  # Creating a 2x2 array  arr   =   [[ 1 ,   2 ],   [ 3 ,   4 ]]  print ( arr )    [[ 1 ,   2 ],   [ 3 ,   4 ]]    Create numpy array via list  import   numpy   as   np   # Convert to NumPy  np . array ( arr )    array ([[ 1 ,   2 ], \n        [ 3 ,   4 ]])    Convert numpy array to PyTorch tensor  import   torch   # Convert to PyTorch Tensor  torch . Tensor ( arr )    1    2  3    4  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Creating Matrices"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values",
            "text": "Create 2x2 numpy array of 1's  np . ones (( 2 ,   2 ))    array ([[   1. ,    1. ], \n        [   1. ,    1. ]])    Create 2x2 torch tensor of 1's  torch . ones (( 2 ,   2 ))     1  1\n 1  1\n[torch.FloatTensor of size 2x2]   Create 2x2 numpy array of random numbers  np . random . rand ( 2 ,   2 )    array ([[   0.68270631 ,    0.87721678 ], \n        [   0.07420986 ,    0.79669375 ]])    Create 2x2 PyTorch tensor of random numbers  torch . rand ( 2 ,   2 )    0.3900    0.8268  0.3888    0.5914  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Create Matrices with Default Values"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility",
            "text": "Why do we need seeds?  We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced.    Create seed to enable fixed numbers for random number generation   # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )    array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])    Repeat random array generation to check  If you do not set the seed, you would not get the same set of numbers like here. # Seed  np . random . seed ( 0 )  np . random . rand ( 2 ,   2 )    array ([[   0.5488135   ,    0.71518937 ], \n        [   0.60276338 ,    0.54488318 ]])    Create a numpy array without seed  Notice how you get different numbers compared to the first 2 tries? # No seed  np . random . rand ( 2 ,   2 )    array ([[   0.56804456 ,    0.92559664 ], \n        [   0.07103606 ,    0.0871293   ]])    Repeat numpy array generation without seed  You get the point now, you get a totally different set of numbers. # No seed  np . random . rand ( 2 ,   2 )    array ([[   0.0202184   ,    0.83261985 ], \n        [   0.77815675 ,    0.87001215 ]])    Create a PyTorch tensor with a fixed seed  # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )     Repeat creating a PyTorch fixed seed tensor  # Torch Seed  torch . manual_seed ( 0 )  torch . rand ( 2 ,   2 )    0.5488    0.5928  0.7152    0.8443  [ torch . FloatTensor   of   size   2 x2 ]    Creating a PyTorch tensor without seed  Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed  torch . rand ( 2 ,   2 )    0.6028    0.8579  0.5449    0.8473  [ torch . FloatTensor   of   size   2 x2 ]    Repeat creating a PyTorch tensor without seed  Notice how these are different numbers again? # Torch No Seed  torch . rand ( 2 ,   2 )    0.4237    0.6236  0.6459    0.3844  [ torch . FloatTensor   of   size   2 x2 ]   Seed for GPU is different for now...   Fix a seed for GPU tensors  When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above.  if   torch . cuda . is_available (): \n     torch . cuda . manual_seed_all ( 0 )",
            "title": "Seeds for Reproducibility"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge",
            "text": "",
            "title": "NumPy and Torch Bridge"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch",
            "text": "Create a numpy array of 1's  # Numpy array  np_array   =   np . ones (( 2 ,   2 ))   print ( np_array )    [[   1.    1. ]  [   1.    1. ]]    Get the type of class for the numpy array  print ( type ( np_array ))    < class   ' numpy . ndarray '>    Convert numpy array to PyTorch tensor  # Convert to Torch Tensor  torch_tensor   =   torch . from_numpy ( np_array )   print ( torch_tensor )      1    1 \n  1    1  [ torch . DoubleTensor   of   size   2 x2 ]    Get type of class for PyTorch tensor  Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor ))    < class   ' torch . DoubleTensor '>    Create PyTorch tensor from a different numpy datatype  You will get an error running this code because PyTorch tensor don't support all datatype.  # Data types matter: intentional error  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  torch . from_numpy ( np_array_new )    ---------------------------------------------------------------------------  RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 57 - b8b085f9b39d >   in   < module > () \n       1   # Data types matter \n       2   np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int8 )  ---->   3   torch . from_numpy ( np_array_new )  RuntimeError :   can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.    What conversion support does Numpy to PyTorch tensor bridge gives?   double  float    int64 ,  int32 ,  uint8       Create PyTorch long tensor  See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int64 )  torch . from_numpy ( np_array_new )    1  1\n1  1\n[torch.LongTensor of size 2x2]   Create PyTorch int tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . int32 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . IntTensor   of   size   2 x2 ]    Create PyTorch byte tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . uint8 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . ByteTensor   of   size   2 x2 ]    Create PyTorch Double Tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float64 )  torch . from_numpy ( np_array_new )   Alternatively you can do this too via  np.double  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . double )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . DoubleTensor   of   size   2 x2 ]    Create PyTorch Float Tensor  # Data types matter  np_array_new   =   np . ones (( 2 ,   2 ),   dtype = np . float32 )  torch . from_numpy ( np_array_new )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Tensor Type Bug Guide  These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide!      NumPy Array Type  Torch Tensor Type      int64  LongTensor    int32  IntegerTensor    uint8  ByteTensor    float64  DoubleTensor    float32  FloatTensor    double  DoubleTensor",
            "title": "NumPy to Torch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy",
            "text": "Create PyTorch tensor of 1's  You would realize this defaults to a float tensor by default if you do this.  torch_tensor   =   torch . ones ( 2 ,   2 )   type ( torch_tensor )    torch . FloatTensor    Convert tensor to numpy  It's as simple as this.  torch_to_numpy   =   torch_tensor . numpy ()   type ( torch_to_numpy )    # Wowza, we did it.  numpy . ndarray",
            "title": "Torch to NumPy"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu",
            "text": "Move tensor to CPU and back  This by default creates a tensor on CPU. You do not need to do anything. # CPU  tensor_cpu   =   torch . ones ( 2 ,   2 )   If you would like to send a tensor to your GPU, you just need to do a simple  .cuda ()  # CPU to GPU  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  tensor_cpu . to ( device )   And if you want to move that tensor on the GPU back to the CPU, just do the following.  # GPU to CPU  tensor_cpu . cpu ()",
            "title": "Tensors on CPU vs GPU"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations",
            "text": "",
            "title": "Tensor Operations"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor",
            "text": "Creating a 2x2 tensor  a   =   torch . ones ( 2 ,   2 )  print ( a )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Getting size of tensor  print ( a . size ())    torch . Size ([ 2 ,   2 ])    Resize tensor to 4x1  a . view ( 4 )    1  1  1  1  [ torch . FloatTensor   of   size   4 ]    Get size of resized tensor  a . view ( 4 ) . size ()    torch . Size ([ 4 ])",
            "title": "Resizing Tensor"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition",
            "text": "Creating first 2x2 tensor  a   =   torch . ones ( 2 ,   2 )  print ( a )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Creating second 2x2 tensor  b   =   torch . ones ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise addition of 2 tensors  # Element-wise addition  c   =   a   +   b  print ( c )      2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]    Alternative element-wise addition of 2 tensors  # Element-wise addition  c   =   torch . add ( a ,   b )  print ( c )      2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]    In-place element-wise addition  This would replace the c tensor values with the new addition.   # In-place addition  print ( 'Old c tensor' )  print ( c )  c . add_ ( a )  print ( '-' * 60 )  print ( 'New c tensor' )  print ( c )    Old   c   tensor \n\n  2    2 \n  2    2  [ torch . FloatTensor   of   size   2 x2 ]  ------------------------------------------------------------  New   c   tensor \n\n  3    3 \n  3    3  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Addition"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction",
            "text": "Check values of tensor a and b'  Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above.  print ( a )  print ( b )      1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ] \n\n\n  1    1 \n  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 1  a   -   b    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 2  # Not in-place  print ( a . sub ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise subtraction: method 3  This will replace a with the final result filled with 2's # Inplace  print ( a . sub_ ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-wise Subtraction"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 1  a   *   b    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 2  # Not in-place  print ( torch . mul ( a ,   b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise multiplication: method 3  # In-place  print ( a . mul_ ( b ))  print ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Multiplication"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division",
            "text": "Create tensor a and b of sizes 2x2 filled with 1's and 0's  a   =   torch . ones ( 2 ,   2 )  print ( a )  b   =   torch . zeros ( 2 ,   2 )  print ( b )    1    1  1    1  [ torch . FloatTensor   of   size   2 x2 ]  0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 1  b   /   a    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 2  torch . div ( b ,   a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]    Element-wise division: method 3  # Inplace  b . div_ ( a )    0    0  0    0  [ torch . FloatTensor   of   size   2 x2 ]",
            "title": "Element-Wise Division"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean",
            "text": "1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55  1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55     mean = 55 /10 = 5.5    mean = 55 /10 = 5.5     Create tensor of size 10 filled from 1 to 10  a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . size ()    torch . Size ([ 10 ])    Get tensor mean  Here we get 5.5 as we've calculated manually above.  a . mean ( dim = 0 )    5.5000  [ torch . FloatTensor   of   size   1 ]    Get tensor mean on second dimension  Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate.  a . mean ( dim = 1 )    RuntimeError                                Traceback   ( most   recent   call   last )  < ipython - input - 7 - 81 aec0cf1c00 >   in   < module > ()  ---->   1   a . mean ( dim = 1 )  RuntimeError :   dimension   out   of   range   ( expected   to   be   in   range   of   [ - 1 ,   0 ],   but   got   1 )    Create a 2x10 Tensor, of 1-10 digits each  a   =   torch . Tensor ([[ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ],   [ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ]])   a . size ()    torch . Size ([ 2 ,   10 ])    Get tensor mean on second dimension  Here we won't get an error like previously because we've a tensor of size 2x10  a . mean ( dim = 1 )      5.5000 \n  5.5000  [ torch . FloatTensor   of   size   2 x1 ]",
            "title": "Tensor Mean"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation",
            "text": "Get standard deviation of tensor   a   =   torch . Tensor ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ])  a . std ( dim = 0 )     3.0277  [ torch . FloatTensor   of   size   1 ]",
            "title": "Tensor Standard Deviation"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_matrices/#summary",
            "text": "We've learnt to...   Success    Create Matrices   Create Matrices with Default Initialization Values   Zeros    Ones     Initialize Seeds for Reproducibility on GPU and CPU   Convert Matrices: NumPy to Torch and Torch to NumPy   Move Tensors: CPU to GPU and GPU to CPU   Run Important Tensor Operations   Element-wise addition, subtraction, multiplication and division   Resize   Calculate mean    Calculate standard deviation",
            "title": "Summary"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/",
            "text": "PyTorch Fundamentals - Gradients\n\u00b6\n\n\nTensors with Gradients\n\u00b6\n\n\nCreating Tensors with Gradients\n\u00b6\n\n\n\n\nAllows accumulation of gradients\n\n\n\n\n\n\nMethod 1: Create tensor with gradients\n\n\nIt is very similar to creating a tensor, all you need to do is to add an additional argument.\n\n\nimport\n \ntorch\n\n\n\n\n\na\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n),\n \nrequires_grad\n=\nTrue\n)\n\n\na\n\n\n\n\n\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\n\n\nCheck if tensor requires gradients\n\n\nThis should return True otherwise you've not done it right.\n\na\n.\nrequires_grad\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nMethod 2: Create tensor with gradients\n\n\nThis allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.\n\n\n# Normal way of creating gradients\n\n\na\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n))\n\n\n\n# Requires gradient\n\n\na\n.\nrequires_grad_\n()\n\n\n\n# Check if requires gradient\n\n\na\n.\nrequires_grad\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\nA tensor without gradients just for comparison\n\n\nIf you do not do either of the methods above, you'll realize you will get False for checking for gradients.\n\n# Not a variable\n\n\nno_gradient\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \n2\n)\n\n\n\n\nno_gradient\n.\nrequires_grad\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\nTensor with gradients addition operation\n\n\n# Behaves similarly to tensors\n\n\nb\n \n=\n \ntorch\n.\nones\n((\n2\n,\n \n2\n),\n \nrequires_grad\n=\nTrue\n)\n\n\nprint\n(\na\n \n+\n \nb\n)\n\n\nprint\n(\ntorch\n.\nadd\n(\na\n,\n \nb\n))\n\n\n\n\n\n\n\ntensor\n([[\n \n2.\n,\n  \n2.\n],\n\n        \n[\n \n2.\n,\n  \n2.\n]])\n\n\n\ntensor\n([[\n \n2.\n,\n  \n2.\n],\n\n        \n[\n \n2.\n,\n  \n2.\n]])\n\n\n\n\n\n\n\nTensor with gradients multiplication operation\n\n\nAs usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation!\n\nprint\n(\na\n \n*\n \nb\n)\n\n\nprint\n(\ntorch\n.\nmul\n(\na\n,\n \nb\n))\n\n\n\n\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\ntensor\n([[\n \n1.\n,\n  \n1.\n],\n\n        \n[\n \n1.\n,\n  \n1.\n]])\n\n\n\n\n\nManually and Automatically Calculating Gradients\n\u00b6\n\n\nWhat exactly is \nrequires_grad\n?\n\n- Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation\n\n\n\n\ny_i = 5(x_i+1)^2\n\n\ny_i = 5(x_i+1)^2\n\n\n\n\n\n\nCreate tensor of size 2x1 filled with 1's that requires gradient\n\n\nx\n \n=\n \ntorch\n.\nones\n(\n2\n,\n \nrequires_grad\n=\nTrue\n)\n\n\nx\n\n\n\n\n\n\n\ntensor\n([\n \n1.\n,\n  \n1.\n])\n\n\n\n\n\n\n\nSimple linear equation with x tensor created\n\n\n\n\ny_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\n\n\ny_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20\n\n\n\n\nWe should get a value of 20 by replicating this simple equation \n\n\ny\n \n=\n \n5\n \n*\n \n(\nx\n \n+\n \n1\n)\n \n**\n \n2\n\n\ny\n\n\n\n\n\n\n\ntensor\n([\n \n20.\n,\n  \n20.\n])\n\n\n\n\n\n\n\nSimple equation with y tensor\n\n\nBackward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable\n\n\nLet's reduce y to a scalar then...\n\n\n\n\no = \\frac{1}{2}\\sum_i y_i\n\n\no = \\frac{1}{2}\\sum_i y_i\n\n\n\n\nAs you can see above, we've a tensor filled with 20's, so average them would return 20\n\n\no\n \n=\n \n(\n1\n/\n2\n)\n \n*\n \ntorch\n.\nsum\n(\ny\n)\n\n\no\n\n\n\n\n\n\n\ntensor\n(\n20.\n)\n\n\n\n\n\n\n\nCalculating first derivative\n\n\n \nRecap \ny\n equation\n: \ny_i = 5(x_i+1)^2\ny_i = 5(x_i+1)^2\n \n\n\n \nRecap \no\n equation\n: \no = \\frac{1}{2}\\sum_i y_i\no = \\frac{1}{2}\\sum_i y_i\n \n\n\n \nSubstitute \ny\n into \no\n equation\n: \no = \\frac{1}{2} \\sum_i 5(x_i+1)^2\no = \\frac{1}{2} \\sum_i 5(x_i+1)^2\n \n\n\n\n\n\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\n\n\n\\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]\n\n\n\n\n\n\n\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\n\n\n\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10\n\n\n\n\nWe should expect to get 10, and it's so simple to do this with PyTorch with the following line...\n\n\nGet first derivative:\n\no\n.\nbackward\n()\n\n\n\n\nPrint out first derivative:\n\n\nx\n.\ngrad\n\n\n\n\n\n\n\ntensor\n([\n \n10.\n,\n  \n10.\n])\n\n\n\n\n\n\n\nIf x requires gradient and you create new objects with it, you get all gradients\n\n\nprint\n(\nx\n.\nrequires_grad\n)\n\n\nprint\n(\ny\n.\nrequires_grad\n)\n\n\nprint\n(\no\n.\nrequires_grad\n)\n\n\n\n\n\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Tensor with Gradients\n\n\n Wraps a tensor for gradient accumulation\n\n\n\n\n\n\n Gradients\n\n\n Define original equation\n\n\n Substitute equation with \nx\n values\n\n\n Reduce to scalar output, \no\n through \nmean\n\n\n Calculate gradients with \no.backward()\n\n\n Then access gradients of the \nx\n tensor with \nrequires_grad\n through \nx.grad",
            "title": "PyTorch Fundamentals - Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#pytorch-fundamentals-gradients",
            "text": "",
            "title": "PyTorch Fundamentals - Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients",
            "text": "",
            "title": "Tensors with Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients",
            "text": "Allows accumulation of gradients    Method 1: Create tensor with gradients  It is very similar to creating a tensor, all you need to do is to add an additional argument.  import   torch   a   =   torch . ones (( 2 ,   2 ),   requires_grad = True )  a    tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])    Check if tensor requires gradients  This should return True otherwise you've not done it right. a . requires_grad    True    Method 2: Create tensor with gradients  This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients.  # Normal way of creating gradients  a   =   torch . ones (( 2 ,   2 ))  # Requires gradient  a . requires_grad_ ()  # Check if requires gradient  a . requires_grad    True    A tensor without gradients just for comparison  If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable  no_gradient   =   torch . ones ( 2 ,   2 )   no_gradient . requires_grad    False    Tensor with gradients addition operation  # Behaves similarly to tensors  b   =   torch . ones (( 2 ,   2 ),   requires_grad = True )  print ( a   +   b )  print ( torch . add ( a ,   b ))    tensor ([[   2. ,    2. ], \n         [   2. ,    2. ]])  tensor ([[   2. ,    2. ], \n         [   2. ,    2. ]])    Tensor with gradients multiplication operation  As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a   *   b )  print ( torch . mul ( a ,   b ))    tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])  tensor ([[   1. ,    1. ], \n         [   1. ,    1. ]])",
            "title": "Creating Tensors with Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients",
            "text": "What exactly is  requires_grad ? \n- Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation   y_i = 5(x_i+1)^2  y_i = 5(x_i+1)^2    Create tensor of size 2x1 filled with 1's that requires gradient  x   =   torch . ones ( 2 ,   requires_grad = True )  x    tensor ([   1. ,    1. ])    Simple linear equation with x tensor created   y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20  y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20   We should get a value of 20 by replicating this simple equation   y   =   5   *   ( x   +   1 )   **   2  y    tensor ([   20. ,    20. ])    Simple equation with y tensor  Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable  Let's reduce y to a scalar then...   o = \\frac{1}{2}\\sum_i y_i  o = \\frac{1}{2}\\sum_i y_i   As you can see above, we've a tensor filled with 20's, so average them would return 20  o   =   ( 1 / 2 )   *   torch . sum ( y )  o    tensor ( 20. )    Calculating first derivative    Recap  y  equation :  y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2      Recap  o  equation :  o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i      Substitute  y  into  o  equation :  o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2     \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]  \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)]    \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10  \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10   We should expect to get 10, and it's so simple to do this with PyTorch with the following line...  Get first derivative: o . backward ()   Print out first derivative:  x . grad    tensor ([   10. ,    10. ])    If x requires gradient and you create new objects with it, you get all gradients  print ( x . requires_grad )  print ( y . requires_grad )  print ( o . requires_grad )    True  True  True",
            "title": "Manually and Automatically Calculating Gradients"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_gradients/#summary",
            "text": "We've learnt to...   Success    Tensor with Gradients   Wraps a tensor for gradient accumulation     Gradients   Define original equation   Substitute equation with  x  values   Reduce to scalar output,  o  through  mean   Calculate gradients with  o.backward()   Then access gradients of the  x  tensor with  requires_grad  through  x.grad",
            "title": "Summary"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/",
            "text": "Linear Regression with PyTorch\n\u00b6\n\n\nAbout Linear Regression\n\u00b6\n\n\nSimple Linear Regression Basics\n\u00b6\n\n\n\n\nAllows us to understand \nrelationship\n between two \ncontinuous variables\n\n\nExample\n\n\nx: independent variable\n\n\nweight\n\n\n\n\n\n\ny: dependent variable\n\n\nheight\n\n\n\n\n\n\n\n\n\n\ny = \\alpha x + \\beta\ny = \\alpha x + \\beta\n\n\n\n\nExample of simple linear regression\n\u00b6\n\n\n\n\nCreate plot for simple linear regression\n\n\nTake note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists.\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n%\nmatplotlib\n \ninline\n\n\n\n# Creates 50 random x and y numbers\n\n\nnp\n.\nrandom\n.\nseed\n(\n1\n)\n\n\nn\n \n=\n \n50\n\n\nx\n \n=\n \nnp\n.\nrandom\n.\nrandn\n(\nn\n)\n\n\ny\n \n=\n \nx\n \n*\n \nnp\n.\nrandom\n.\nrandn\n(\nn\n)\n\n\n\n# Makes the dots colorful\n\n\ncolors\n \n=\n \nnp\n.\nrandom\n.\nrand\n(\nn\n)\n\n\n\n# Plots best-fit line via polyfit\n\n\nplt\n.\nplot\n(\nnp\n.\nunique\n(\nx\n),\n \nnp\n.\npoly1d\n(\nnp\n.\npolyfit\n(\nx\n,\n \ny\n,\n \n1\n))(\nnp\n.\nunique\n(\nx\n)))\n\n\n\n# Plots the random x and y data points we created\n\n\n# Interestingly, alpha makes it more aesthetically pleasing\n\n\nplt\n.\nscatter\n(\nx\n,\n \ny\n,\n \nc\n=\ncolors\n,\n \nalpha\n=\n0.5\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\n\n\n\nAim of Linear Regression\n\u00b6\n\n\n\n\nMinimize the distance between the points and the line (\ny = \\alpha x + \\beta\ny = \\alpha x + \\beta\n)\n\n\nAdjusting\n\n\nCoefficient: \n\\alpha\n\\alpha\n\n\nBias/intercept: \n\\beta\n\\beta\n\n\n\n\n\n\n\n\nBuilding a Linear Regression Model with PyTorch\n\u00b6\n\n\nExample\n\u00b6\n\n\n\n\nCoefficient: \n\\alpha = 2\n\\alpha = 2\n\n\nBias/intercept: \n\\beta = 1\n\\beta = 1\n\n\nEquation: \ny = 2x + 1\ny = 2x + 1\n\n\n\n\nBuilding a Toy Dataset\n\u00b6\n\n\n\n\nCreate a list of values from 0 to 11\n\n\nx_values\n \n=\n \n[\ni\n \nfor\n \ni\n \nin\n \nrange\n(\n11\n)]\n\n\n\n\n\nx_values\n\n\n\n\n\n\n\n[\n0\n,\n \n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n]\n\n\n\n\n\n\n\nConvert list of numbers to numpy array\n\n\n# Convert to numpy\n\n\nx_train\n \n=\n \nnp\n.\narray\n(\nx_values\n,\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\nx_train\n.\nshape\n\n\n\n\n\n\n\n(\n11\n,)\n\n\n\n\n\n\n\nConvert to 2-dimensional array\n\n\nIf you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road.\n\n# IMPORTANT: 2D required\n\n\nx_train\n \n=\n \nx_train\n.\nreshape\n(\n-\n1\n,\n \n1\n)\n\n\nx_train\n.\nshape\n\n\n\n\n\n\n(\n11\n,\n \n1\n)\n\n\n\n\n\n\n\nCreate list of y values\n\n\nWe want y values for every x value we have above. \n\n\ny = 2x + 1\ny = 2x + 1\n\n\ny_values\n \n=\n \n[\n2\n*\ni\n \n+\n \n1\n \nfor\n \ni\n \nin\n \nx_values\n]\n\n\n\n\n\ny_values\n\n\n\n\n\n\n\n[\n1\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n9\n,\n \n11\n,\n \n13\n,\n \n15\n,\n \n17\n,\n \n19\n,\n \n21\n]\n\n\n\n\n\n\n\nAlternative to create list of y values\n\n\nIf you're weak in list iterators, this might be an easier alternative.\n\n# In case you're weak in list iterators...\n\n\ny_values\n \n=\n \n[]\n\n\nfor\n \ni\n \nin\n \nx_values\n:\n\n    \nresult\n \n=\n \n2\n*\ni\n \n+\n \n1\n\n    \ny_values\n.\nappend\n(\nresult\n)\n \n\n\n\ny_values\n\n\n\n\n\n\n\n[\n1\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n9\n,\n \n11\n,\n \n13\n,\n \n15\n,\n \n17\n,\n \n19\n,\n \n21\n]\n\n\n\n\n\n\n\nConvert to numpy array\n\n\nYou will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good.\n\n\ny_train\n \n=\n \nnp\n.\narray\n(\ny_values\n,\n \ndtype\n=\nnp\n.\nfloat32\n)\n\n\ny_train\n.\nshape\n\n\n\n\n\n\n\n(\n11\n,)\n\n\n\n\n\n\n\nReshape y numpy array to 2-dimension\n\n\n# IMPORTANT: 2D required\n\n\ny_train\n \n=\n \ny_train\n.\nreshape\n(\n-\n1\n,\n \n1\n)\n\n\ny_train\n.\nshape\n\n\n\n\n\n\n\n(\n11\n,\n \n1\n)\n\n\n\n\n\nBuilding Model\n\u00b6\n\n\n\n\nCritical Imports\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\n\n\n\n\n\n\n\nCreate Model\n\n\n\n\nLinear model\n\n\nTrue Equation: \ny = 2x + 1\ny = 2x + 1\n\n\n\n\n\n\nForward\n\n\nExample\n\n\nInput $x = 1 $\n\n\nOutput \n\\hat y = ?\n\\hat y = ?\n\n\n\n\n\n\n\n\n\n\n\n\n# Create class\n\n\nclass\n \nLinearRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLinearRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n  \n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n\n\n\n\n\n\nInstantiate Model Class\n\n\n\n\ninput: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\ndesired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n\n\n\n\ninput_dim\n \n=\n \n1\n\n\noutput_dim\n \n=\n \n1\n\n\n\nmodel\n \n=\n \nLinearRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n\n\n\n\n\n\nInstantiate Loss Class\n\n\n\n\nMSE Loss: Mean Squared Error\n\n\nMSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)\nMSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)\n\n\n\\hat y\n\\hat y\n: prediction\n\n\ny\ny\n: true value\n\n\n\n\n\n\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\n\n\n\n\n\n\n\nInstantiate Optimizer Class\n\n\n\n\nSimplified equation\n\n\n$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n\n\n\\theta\n\\theta\n: parameters (our variables)\n\n\n\\eta\n\\eta\n: learning rate (how fast we want to learn)\n\n\n\\nabla_\\theta\n\\nabla_\\theta\n: parameters' gradients\n\n\n\n\n\n\n\n\n\n\nEven simplier equation\n\n\nparameters = parameters - learning_rate * parameters_gradients\n\n\nparameters: \n\\alpha\n\\alpha\n and \n\\beta\n\\beta\n in $ y = \\alpha x + \\beta$\n\n\ndesired parameters: \n\\alpha = 2\n\\alpha = 2\n and \n\\beta = 1\n\\beta = 1\n in $ y = 2x + 1$ \n\n\n\n\n\n\n\n\n\n\n\n\nlearning_rate\n \n=\n \n0.01\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n\n\n\n\n\n\nTrain Model\n\n\n\n\n\n\n1 epoch: going through the whole x_train data once\n\n\n\n\n100 epochs: \n\n\n100x mapping \nx_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\n\n\n\n\n\n\nProcess \n\n\n\n\nConvert inputs/labels to tensors with gradients\n\n\nClear gradient buffets\n\n\nGet output given inputs \n\n\nGet loss\n\n\nGet gradients w.r.t. parameters\n\n\nUpdate parameters using gradients\n\n\nparameters = parameters - learning_rate * parameters_gradients\n\n\n\n\n\n\nREPEAT\n\n\n\n\n\n\n\n\nepochs\n \n=\n \n100\n\n\n\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n    \nepoch\n \n+=\n \n1\n\n    \n# Convert numpy array to torch Variable\n\n    \ninputs\n \n=\n \ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n()\n\n    \nlabels\n \n=\n \ntorch\n.\nfrom_numpy\n(\ny_train\n)\n\n\n    \n# Clear gradients w.r.t. parameters\n\n    \noptimizer\n.\nzero_grad\n()\n \n\n    \n# Forward to get output\n\n    \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n\n    \n# Calculate Loss\n\n    \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n    \n# Getting gradients w.r.t. parameters\n\n    \nloss\n.\nbackward\n()\n\n\n    \n# Updating parameters\n\n    \noptimizer\n.\nstep\n()\n\n\n    \nprint\n(\n'epoch {}, loss {}'\n.\nformat\n(\nepoch\n,\n \nloss\n.\nitem\n()))\n\n\n\n\n\n\n\nepoch\n \n1\n,\n \nloss\n \n140.58143615722656\n\n\nepoch\n \n2\n,\n \nloss\n \n11.467253684997559\n\n\nepoch\n \n3\n,\n \nloss\n \n0.9358152747154236\n\n\nepoch\n \n4\n,\n \nloss\n \n0.07679400593042374\n\n\nepoch\n \n5\n,\n \nloss\n \n0.0067212567664682865\n\n\nepoch\n \n6\n,\n \nloss\n \n0.0010006226366385818\n\n\nepoch\n \n7\n,\n \nloss\n \n0.0005289533291943371\n\n\nepoch\n \n8\n,\n \nloss\n \n0.0004854927829001099\n\n\nepoch\n \n9\n,\n \nloss\n \n0.00047700389404781163\n\n\nepoch\n \n10\n,\n \nloss\n \n0.0004714332753792405\n\n\nepoch\n \n11\n,\n \nloss\n \n0.00046614606981165707\n\n\nepoch\n \n12\n,\n \nloss\n \n0.0004609318566508591\n\n\nepoch\n \n13\n,\n \nloss\n \n0.0004557870561257005\n\n\nepoch\n \n14\n,\n \nloss\n \n0.00045069155748933554\n\n\nepoch\n \n15\n,\n \nloss\n \n0.00044567222357727587\n\n\nepoch\n \n16\n,\n \nloss\n \n0.00044068993884138763\n\n\nepoch\n \n17\n,\n \nloss\n \n0.00043576463940553367\n\n\nepoch\n \n18\n,\n \nloss\n \n0.00043090470717288554\n\n\nepoch\n \n19\n,\n \nloss\n \n0.00042609183583408594\n\n\nepoch\n \n20\n,\n \nloss\n \n0.0004213254142086953\n\n\nepoch\n \n21\n,\n \nloss\n \n0.0004166301223449409\n\n\nepoch\n \n22\n,\n \nloss\n \n0.0004119801160413772\n\n\nepoch\n \n23\n,\n \nloss\n \n0.00040738462121225893\n\n\nepoch\n \n24\n,\n \nloss\n \n0.0004028224211651832\n\n\nepoch\n \n25\n,\n \nloss\n \n0.0003983367350883782\n\n\nepoch\n \n26\n,\n \nloss\n \n0.0003938761365134269\n\n\nepoch\n \n27\n,\n \nloss\n \n0.000389480876037851\n\n\nepoch\n \n28\n,\n \nloss\n \n0.00038514015614055097\n\n\nepoch\n \n29\n,\n \nloss\n \n0.000380824290914461\n\n\nepoch\n \n30\n,\n \nloss\n \n0.00037657516077160835\n\n\nepoch\n \n31\n,\n \nloss\n \n0.000372376263840124\n\n\nepoch\n \n32\n,\n \nloss\n \n0.0003682126116473228\n\n\nepoch\n \n33\n,\n \nloss\n \n0.0003640959912445396\n\n\nepoch\n \n34\n,\n \nloss\n \n0.00036003670538775623\n\n\nepoch\n \n35\n,\n \nloss\n \n0.00035601368290372193\n\n\nepoch\n \n36\n,\n \nloss\n \n0.00035203873994760215\n\n\nepoch\n \n37\n,\n \nloss\n \n0.00034810820943675935\n\n\nepoch\n \n38\n,\n \nloss\n \n0.000344215368386358\n\n\nepoch\n \n39\n,\n \nloss\n \n0.0003403784066904336\n\n\nepoch\n \n40\n,\n \nloss\n \n0.00033658024040050805\n\n\nepoch\n \n41\n,\n \nloss\n \n0.0003328165039420128\n\n\nepoch\n \n42\n,\n \nloss\n \n0.0003291067841928452\n\n\nepoch\n \n43\n,\n \nloss\n \n0.0003254293987993151\n\n\nepoch\n \n44\n,\n \nloss\n \n0.0003217888588551432\n\n\nepoch\n \n45\n,\n \nloss\n \n0.0003182037326041609\n\n\nepoch\n \n46\n,\n \nloss\n \n0.0003146533854305744\n\n\nepoch\n \n47\n,\n \nloss\n \n0.00031113551813177764\n\n\nepoch\n \n48\n,\n \nloss\n \n0.0003076607536058873\n\n\nepoch\n \n49\n,\n \nloss\n \n0.00030422292184084654\n\n\nepoch\n \n50\n,\n \nloss\n \n0.00030083119054324925\n\n\nepoch\n \n51\n,\n \nloss\n \n0.00029746422660537064\n\n\nepoch\n \n52\n,\n \nloss\n \n0.0002941471466328949\n\n\nepoch\n \n53\n,\n \nloss\n \n0.00029085995629429817\n\n\nepoch\n \n54\n,\n \nloss\n \n0.0002876132493838668\n\n\nepoch\n \n55\n,\n \nloss\n \n0.00028440452297218144\n\n\nepoch\n \n56\n,\n \nloss\n \n0.00028122696676291525\n\n\nepoch\n \n57\n,\n \nloss\n \n0.00027808290906250477\n\n\nepoch\n \n58\n,\n \nloss\n \n0.00027497278642840683\n\n\nepoch\n \n59\n,\n \nloss\n \n0.00027190230321139097\n\n\nepoch\n \n60\n,\n \nloss\n \n0.00026887087733484805\n\n\nepoch\n \n61\n,\n \nloss\n \n0.0002658693410921842\n\n\nepoch\n \n62\n,\n \nloss\n \n0.0002629039518069476\n\n\nepoch\n \n63\n,\n \nloss\n \n0.00025996880140155554\n\n\nepoch\n \n64\n,\n \nloss\n \n0.0002570618235040456\n\n\nepoch\n \n65\n,\n \nloss\n \n0.00025419273879379034\n\n\nepoch\n \n66\n,\n \nloss\n \n0.00025135406758636236\n\n\nepoch\n \n67\n,\n \nloss\n \n0.0002485490695107728\n\n\nepoch\n \n68\n,\n \nloss\n \n0.0002457649679854512\n\n\nepoch\n \n69\n,\n \nloss\n \n0.0002430236927466467\n\n\nepoch\n \n70\n,\n \nloss\n \n0.00024031475186347961\n\n\nepoch\n \n71\n,\n \nloss\n \n0.00023762597993481904\n\n\nepoch\n \n72\n,\n \nloss\n \n0.00023497406800743192\n\n\nepoch\n \n73\n,\n \nloss\n \n0.0002323519001947716\n\n\nepoch\n \n74\n,\n \nloss\n \n0.00022976362379267812\n\n\nepoch\n \n75\n,\n \nloss\n \n0.0002271933335578069\n\n\nepoch\n \n76\n,\n \nloss\n \n0.00022465786605607718\n\n\nepoch\n \n77\n,\n \nloss\n \n0.00022214400814846158\n\n\nepoch\n \n78\n,\n \nloss\n \n0.00021966728672850877\n\n\nepoch\n \n79\n,\n \nloss\n \n0.0002172116219298914\n\n\nepoch\n \n80\n,\n \nloss\n \n0.00021478648704942316\n\n\nepoch\n \n81\n,\n \nloss\n \n0.00021239375928416848\n\n\nepoch\n \n82\n,\n \nloss\n \n0.0002100227284245193\n\n\nepoch\n \n83\n,\n \nloss\n \n0.00020767028036061674\n\n\nepoch\n \n84\n,\n \nloss\n \n0.00020534756185952574\n\n\nepoch\n \n85\n,\n \nloss\n \n0.00020305956422816962\n\n\nepoch\n \n86\n,\n \nloss\n \n0.0002007894654525444\n\n\nepoch\n \n87\n,\n \nloss\n \n0.00019854879064951092\n\n\nepoch\n \n88\n,\n \nloss\n \n0.00019633043848443776\n\n\nepoch\n \n89\n,\n \nloss\n \n0.00019413618429098278\n\n\nepoch\n \n90\n,\n \nloss\n \n0.00019197272195015103\n\n\nepoch\n \n91\n,\n \nloss\n \n0.0001898303598864004\n\n\nepoch\n \n92\n,\n \nloss\n \n0.00018771187751553953\n\n\nepoch\n \n93\n,\n \nloss\n \n0.00018561164324637502\n\n\nepoch\n \n94\n,\n \nloss\n \n0.00018354636267758906\n\n\nepoch\n \n95\n,\n \nloss\n \n0.00018149390234611928\n\n\nepoch\n \n96\n,\n \nloss\n \n0.0001794644631445408\n\n\nepoch\n \n97\n,\n \nloss\n \n0.00017746571393217891\n\n\nepoch\n \n98\n,\n \nloss\n \n0.00017548113828524947\n\n\nepoch\n \n99\n,\n \nloss\n \n0.00017352371651213616\n\n\nepoch\n \n100\n,\n \nloss\n \n0.00017157981346827\n\n\n\n\n\n\n\nLooking at predicted values\n\n\n# Purely inference\n\n\npredicted\n \n=\n \nmodel\n(\ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n())\n.\ndata\n.\nnumpy\n()\n\n\npredicted\n\n\n\n\n\n\n\narray\n([[\n \n0.9756333\n],\n\n       \n[\n \n2.9791424\n],\n\n       \n[\n \n4.982651\n \n],\n\n       \n[\n \n6.9861603\n],\n\n       \n[\n \n8.98967\n  \n],\n\n       \n[\n10.993179\n \n],\n\n       \n[\n12.996688\n \n],\n\n       \n[\n15.000196\n \n],\n\n       \n[\n17.003706\n \n],\n\n       \n[\n19.007215\n \n],\n\n       \n[\n21.010725\n \n]],\n \ndtype\n=\nfloat32\n)\n\n\n\n\n\n\n\nLooking at training values\n\n\nThese are the true values, you can see how it's able to predict similar values.\n\n\n# y = 2x + 1 \n\n\ny_train\n\n\n\n\n\n\n\narray\n([[\n \n1.\n],\n\n       \n[\n \n3.\n],\n\n       \n[\n \n5.\n],\n\n       \n[\n \n7.\n],\n\n       \n[\n \n9.\n],\n\n       \n[\n11.\n],\n\n       \n[\n13.\n],\n\n       \n[\n15.\n],\n\n       \n[\n17.\n],\n\n       \n[\n19.\n],\n\n       \n[\n21.\n]],\n \ndtype\n=\nfloat32\n)\n\n\n\n\n\n\n\nPlot of predicted and actual values\n\n\n# Clear figure\n\n\nplt\n.\nclf\n()\n\n\n\n# Get predictions\n\n\npredicted\n \n=\n \nmodel\n(\ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n())\n.\ndata\n.\nnumpy\n()\n\n\n\n# Plot true data\n\n\nplt\n.\nplot\n(\nx_train\n,\n \ny_train\n,\n \n'go'\n,\n \nlabel\n=\n'True data'\n,\n \nalpha\n=\n0.5\n)\n\n\n\n# Plot predictions\n\n\nplt\n.\nplot\n(\nx_train\n,\n \npredicted\n,\n \n'--'\n,\n \nlabel\n=\n'Predictions'\n,\n \nalpha\n=\n0.5\n)\n\n\n\n# Legend and plot\n\n\nplt\n.\nlegend\n(\nloc\n=\n'best'\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\n\n\n\n\n\n\nSave Model\n\n\nsave_model\n \n=\n \nFalse\n\n\nif\n \nsave_model\n \nis\n \nTrue\n:\n\n    \n# Saves only parameters\n\n    \n# alpha & beta\n\n    \ntorch\n.\nsave\n(\nmodel\n.\nstate_dict\n(),\n \n'awesome_model.pkl'\n)\n\n\n\n\n\n\n\n\n\nLoad Model\n\n\nload_model\n \n=\n \nFalse\n\n\nif\n \nload_model\n \nis\n \nTrue\n:\n\n    \nmodel\n.\nload_state_dict\n(\ntorch\n.\nload\n(\n'awesome_model.pkl'\n))\n\n\n\n\n\n\n\nBuilding a Linear Regression Model with PyTorch (GPU)\n\u00b6\n\n\n\n\nCPU Summary\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\n\n'''\n\n\nSTEP 1: CREATE MODEL CLASS\n\n\n'''\n\n\nclass\n \nLinearRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLinearRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n  \n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n'''\n\n\nSTEP 2: INSTANTIATE MODEL CLASS\n\n\n'''\n\n\ninput_dim\n \n=\n \n1\n\n\noutput_dim\n \n=\n \n1\n\n\n\nmodel\n \n=\n \nLinearRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n'''\n\n\nSTEP 3: INSTANTIATE LOSS CLASS\n\n\n'''\n\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\n\n'''\n\n\nSTEP 4: INSTANTIATE OPTIMIZER CLASS\n\n\n'''\n\n\n\nlearning_rate\n \n=\n \n0.01\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n'''\n\n\nSTEP 5: TRAIN THE MODEL\n\n\n'''\n\n\nepochs\n \n=\n \n100\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n    \nepoch\n \n+=\n \n1\n\n    \n# Convert numpy array to torch Variable\n\n    \ninputs\n \n=\n \ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nrequires_grad_\n()\n\n    \nlabels\n \n=\n \ntorch\n.\nfrom_numpy\n(\ny_train\n)\n\n\n    \n# Clear gradients w.r.t. parameters\n\n    \noptimizer\n.\nzero_grad\n()\n \n\n    \n# Forward to get output\n\n    \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n\n    \n# Calculate Loss\n\n    \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n    \n# Getting gradients w.r.t. parameters\n\n    \nloss\n.\nbackward\n()\n\n\n    \n# Updating parameters\n\n    \noptimizer\n.\nstep\n()\n\n\n\n\n\n\n\n\n\nGPU Summary\n\n\n\n\nJust remember always 2 things must be on GPU\n\n\nmodel\n\n\ntensors with gradients\n\n\n\n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\n\n'''\n\n\nSTEP 1: CREATE MODEL CLASS\n\n\n'''\n\n\nclass\n \nLinearRegressionModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ninput_dim\n,\n \noutput_dim\n):\n\n        \nsuper\n(\nLinearRegressionModel\n,\n \nself\n)\n.\n__init__\n()\n\n        \nself\n.\nlinear\n \n=\n \nnn\n.\nLinear\n(\ninput_dim\n,\n \noutput_dim\n)\n  \n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nout\n \n=\n \nself\n.\nlinear\n(\nx\n)\n\n        \nreturn\n \nout\n\n\n\n'''\n\n\nSTEP 2: INSTANTIATE MODEL CLASS\n\n\n'''\n\n\ninput_dim\n \n=\n \n1\n\n\noutput_dim\n \n=\n \n1\n\n\n\nmodel\n \n=\n \nLinearRegressionModel\n(\ninput_dim\n,\n \noutput_dim\n)\n\n\n\n\n#######################\n\n\n#  USE GPU FOR MODEL  #\n\n\n#######################\n\n\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda:0\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\nmodel\n.\nto\n(\ndevice\n)\n\n\n\n'''\n\n\nSTEP 3: INSTANTIATE LOSS CLASS\n\n\n'''\n\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\n\n'''\n\n\nSTEP 4: INSTANTIATE OPTIMIZER CLASS\n\n\n'''\n\n\n\nlearning_rate\n \n=\n \n0.01\n\n\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\nlearning_rate\n)\n\n\n\n'''\n\n\nSTEP 5: TRAIN THE MODEL\n\n\n'''\n\n\nepochs\n \n=\n \n100\n\n\nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n    \nepoch\n \n+=\n \n1\n\n    \n# Convert numpy array to torch Variable\n\n\n    \n#######################\n\n    \n#  USE GPU FOR MODEL  #\n\n    \n#######################\n\n    \ninputs\n \n=\n \ntorch\n.\nfrom_numpy\n(\nx_train\n)\n.\nto\n(\ndevice\n)\n\n    \nlabels\n \n=\n \ntorch\n.\nfrom_numpy\n(\ny_train\n)\n.\nto\n(\ndevice\n)\n\n\n    \n# Clear gradients w.r.t. parameters\n\n    \noptimizer\n.\nzero_grad\n()\n \n\n    \n# Forward to get output\n\n    \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n\n    \n# Calculate Loss\n\n    \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n\n    \n# Getting gradients w.r.t. parameters\n\n    \nloss\n.\nbackward\n()\n\n\n    \n# Updating parameters\n\n    \noptimizer\n.\nstep\n()\n\n\n    \n# Logging\n\n    \nprint\n(\n'epoch {}, loss {}'\n.\nformat\n(\nepoch\n,\n \nloss\n.\nitem\n()))\n\n\n\n\n\n\n\nepoch\n \n1\n,\n \nloss\n \n336.0314025878906\n\n\nepoch\n \n2\n,\n \nloss\n \n27.67657470703125\n\n\nepoch\n \n3\n,\n \nloss\n \n2.5220539569854736\n\n\nepoch\n \n4\n,\n \nloss\n \n0.46732547879219055\n\n\nepoch\n \n5\n,\n \nloss\n \n0.2968060076236725\n\n\nepoch\n \n6\n,\n \nloss\n \n0.2800087630748749\n\n\nepoch\n \n7\n,\n \nloss\n \n0.27578213810920715\n\n\nepoch\n \n8\n,\n \nloss\n \n0.2726128399372101\n\n\nepoch\n \n9\n,\n \nloss\n \n0.269561231136322\n\n\nepoch\n \n10\n,\n \nloss\n \n0.2665504515171051\n\n\nepoch\n \n11\n,\n \nloss\n \n0.2635740041732788\n\n\nepoch\n \n12\n,\n \nloss\n \n0.26063060760498047\n\n\nepoch\n \n13\n,\n \nloss\n \n0.2577202618122101\n\n\nepoch\n \n14\n,\n \nloss\n \n0.2548423111438751\n\n\nepoch\n \n15\n,\n \nloss\n \n0.25199657678604126\n\n\nepoch\n \n16\n,\n \nloss\n \n0.24918246269226074\n\n\nepoch\n \n17\n,\n \nloss\n \n0.24639996886253357\n\n\nepoch\n \n18\n,\n \nloss\n \n0.24364829063415527\n\n\nepoch\n \n19\n,\n \nloss\n \n0.24092751741409302\n\n\nepoch\n \n20\n,\n \nloss\n \n0.2382371574640274\n\n\nepoch\n \n21\n,\n \nloss\n \n0.23557686805725098\n\n\nepoch\n \n22\n,\n \nloss\n \n0.2329462170600891\n\n\nepoch\n \n23\n,\n \nloss\n \n0.2303449958562851\n\n\nepoch\n \n24\n,\n \nloss\n \n0.22777271270751953\n\n\nepoch\n \n25\n,\n \nloss\n \n0.2252292037010193\n\n\nepoch\n \n26\n,\n \nloss\n \n0.22271405160427094\n\n\nepoch\n \n27\n,\n \nloss\n \n0.22022713720798492\n\n\nepoch\n \n28\n,\n \nloss\n \n0.21776780486106873\n\n\nepoch\n \n29\n,\n \nloss\n \n0.21533599495887756\n\n\nepoch\n \n30\n,\n \nloss\n \n0.21293145418167114\n\n\nepoch\n \n31\n,\n \nloss\n \n0.21055366098880768\n\n\nepoch\n \n32\n,\n \nloss\n \n0.20820240676403046\n\n\nepoch\n \n33\n,\n \nloss\n \n0.2058774083852768\n\n\nepoch\n \n34\n,\n \nloss\n \n0.20357847213745117\n\n\nepoch\n \n35\n,\n \nloss\n \n0.20130516588687897\n\n\nepoch\n \n36\n,\n \nloss\n \n0.1990572065114975\n\n\nepoch\n \n37\n,\n \nloss\n \n0.19683438539505005\n\n\nepoch\n \n38\n,\n \nloss\n \n0.19463638961315155\n\n\nepoch\n \n39\n,\n \nloss\n \n0.19246290624141693\n\n\nepoch\n \n40\n,\n \nloss\n \n0.1903136670589447\n\n\nepoch\n \n41\n,\n \nloss\n \n0.1881885528564453\n\n\nepoch\n \n42\n,\n \nloss\n \n0.18608702719211578\n\n\nepoch\n \n43\n,\n \nloss\n \n0.18400898575782776\n\n\nepoch\n \n44\n,\n \nloss\n \n0.18195408582687378\n\n\nepoch\n \n45\n,\n \nloss\n \n0.17992223799228668\n\n\nepoch\n \n46\n,\n \nloss\n \n0.17791320383548737\n\n\nepoch\n \n47\n,\n \nloss\n \n0.17592646181583405\n\n\nepoch\n \n48\n,\n \nloss\n \n0.17396186292171478\n\n\nepoch\n \n49\n,\n \nloss\n \n0.17201924324035645\n\n\nepoch\n \n50\n,\n \nloss\n \n0.17009828984737396\n\n\nepoch\n \n51\n,\n \nloss\n \n0.16819894313812256\n\n\nepoch\n \n52\n,\n \nloss\n \n0.16632060706615448\n\n\nepoch\n \n53\n,\n \nloss\n \n0.16446338593959808\n\n\nepoch\n \n54\n,\n \nloss\n \n0.16262666881084442\n\n\nepoch\n \n55\n,\n \nloss\n \n0.16081078350543976\n\n\nepoch\n \n56\n,\n \nloss\n \n0.15901507437229156\n\n\nepoch\n \n57\n,\n \nloss\n \n0.15723931789398193\n\n\nepoch\n \n58\n,\n \nloss\n \n0.15548335015773773\n\n\nepoch\n \n59\n,\n \nloss\n \n0.15374726057052612\n\n\nepoch\n \n60\n,\n \nloss\n \n0.1520303338766098\n\n\nepoch\n \n61\n,\n \nloss\n \n0.15033268928527832\n\n\nepoch\n \n62\n,\n \nloss\n \n0.14865389466285706\n\n\nepoch\n \n63\n,\n \nloss\n \n0.14699392020702362\n\n\nepoch\n \n64\n,\n \nloss\n \n0.14535246789455414\n\n\nepoch\n \n65\n,\n \nloss\n \n0.14372935891151428\n\n\nepoch\n \n66\n,\n \nloss\n \n0.14212435483932495\n\n\nepoch\n \n67\n,\n \nloss\n \n0.14053721725940704\n\n\nepoch\n \n68\n,\n \nloss\n \n0.13896773755550385\n\n\nepoch\n \n69\n,\n \nloss\n \n0.1374160647392273\n\n\nepoch\n \n70\n,\n \nloss\n \n0.1358814686536789\n\n\nepoch\n \n71\n,\n \nloss\n \n0.13436420261859894\n\n\nepoch\n \n72\n,\n \nloss\n \n0.13286370038986206\n\n\nepoch\n \n73\n,\n \nloss\n \n0.1313801407814026\n\n\nepoch\n \n74\n,\n \nloss\n \n0.12991292774677277\n\n\nepoch\n \n75\n,\n \nloss\n \n0.12846232950687408\n\n\nepoch\n \n76\n,\n \nloss\n \n0.1270277351140976\n\n\nepoch\n \n77\n,\n \nloss\n \n0.12560924887657166\n\n\nepoch\n \n78\n,\n \nloss\n \n0.12420656532049179\n\n\nepoch\n \n79\n,\n \nloss\n \n0.12281957268714905\n\n\nepoch\n \n80\n,\n \nloss\n \n0.1214480847120285\n\n\nepoch\n \n81\n,\n \nloss\n \n0.12009195983409882\n\n\nepoch\n \n82\n,\n \nloss\n \n0.1187509223818779\n\n\nepoch\n \n83\n,\n \nloss\n \n0.11742479354143143\n\n\nepoch\n \n84\n,\n \nloss\n \n0.11611353605985641\n\n\nepoch\n \n85\n,\n \nloss\n \n0.11481687426567078\n\n\nepoch\n \n86\n,\n \nloss\n \n0.11353478580713272\n\n\nepoch\n \n87\n,\n \nloss\n \n0.11226697266101837\n\n\nepoch\n \n88\n,\n \nloss\n \n0.11101329326629639\n\n\nepoch\n \n89\n,\n \nloss\n \n0.10977360606193542\n\n\nepoch\n \n90\n,\n \nloss\n \n0.10854770988225937\n\n\nepoch\n \n91\n,\n \nloss\n \n0.10733554512262344\n\n\nepoch\n \n92\n,\n \nloss\n \n0.10613703727722168\n\n\nepoch\n \n93\n,\n \nloss\n \n0.10495180636644363\n\n\nepoch\n \n94\n,\n \nloss\n \n0.10377981513738632\n\n\nepoch\n \n95\n,\n \nloss\n \n0.10262089222669601\n\n\nepoch\n \n96\n,\n \nloss\n \n0.10147502273321152\n\n\nepoch\n \n97\n,\n \nloss\n \n0.1003417894244194\n\n\nepoch\n \n98\n,\n \nloss\n \n0.09922132641077042\n\n\nepoch\n \n99\n,\n \nloss\n \n0.0981132984161377\n\n\nepoch\n \n100\n,\n \nloss\n \n0.09701769798994064\n\n\n\n\n\nSummary\n\u00b6\n\n\nWe've learnt to...\n\n\n\n\nSuccess\n\n\n\n\n Simple \nlinear regression basics\n\n\n \ny = Ax + B\ny = Ax + B\n\n\n \ny = 2x + 1\ny = 2x + 1\n\n\n\n\n\n\n \nExample\n of simple linear regression\n\n\n \nAim\n of linear regression\n\n\n Minimizing distance between the points and the line\n\n\n Calculate \"distance\" through \nMSE\n\n\n Calculate \ngradients\n\n\n Update parameters with \nparameters = parameters - learning_rate * gradients\n\n\n Slowly update parameters \nA\nA\n and \nB\nB\n model the linear relationship between \ny\ny\n and \nx\nx\n of the form \ny = 2x + 1\ny = 2x + 1\n\n\n\n\n\n\n\n\n\n\n Built a linear regression \nmodel\n in \nCPU and GPU\n\n\n Step 1: Create Model Class\n\n\n Step 2: Instantiate Model Class\n\n\n Step 3: Instantiate Loss Class\n\n\n Step 4: Instantiate Optimizer Class\n\n\n Step 5: Train Model\n\n\n\n\n\n\n Important things to be on \nGPU\n\n\n \nmodel\n\n\n \ntensors with gradients\n\n\n\n\n\n\n How to bring to \nGPU\n?\n\n\nmodel_name.to(device)\n\n\nvariable_name.to(device)",
            "title": "PyTorch Fundamentals - Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#linear-regression-with-pytorch",
            "text": "",
            "title": "Linear Regression with PyTorch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#about-linear-regression",
            "text": "",
            "title": "About Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#simple-linear-regression-basics",
            "text": "Allows us to understand  relationship  between two  continuous variables  Example  x: independent variable  weight    y: dependent variable  height      y = \\alpha x + \\beta y = \\alpha x + \\beta",
            "title": "Simple Linear Regression Basics"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#example-of-simple-linear-regression",
            "text": "Create plot for simple linear regression  Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists.  import   numpy   as   np  import   matplotlib.pyplot   as   plt  % matplotlib   inline  # Creates 50 random x and y numbers  np . random . seed ( 1 )  n   =   50  x   =   np . random . randn ( n )  y   =   x   *   np . random . randn ( n )  # Makes the dots colorful  colors   =   np . random . rand ( n )  # Plots best-fit line via polyfit  plt . plot ( np . unique ( x ),   np . poly1d ( np . polyfit ( x ,   y ,   1 ))( np . unique ( x )))  # Plots the random x and y data points we created  # Interestingly, alpha makes it more aesthetically pleasing  plt . scatter ( x ,   y ,   c = colors ,   alpha = 0.5 )  plt . show ()",
            "title": "Example of simple linear regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#aim-of-linear-regression",
            "text": "Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta )  Adjusting  Coefficient:  \\alpha \\alpha  Bias/intercept:  \\beta \\beta",
            "title": "Aim of Linear Regression"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch",
            "text": "",
            "title": "Building a Linear Regression Model with PyTorch"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#example",
            "text": "Coefficient:  \\alpha = 2 \\alpha = 2  Bias/intercept:  \\beta = 1 \\beta = 1  Equation:  y = 2x + 1 y = 2x + 1",
            "title": "Example"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-toy-dataset",
            "text": "Create a list of values from 0 to 11  x_values   =   [ i   for   i   in   range ( 11 )]   x_values    [ 0 ,   1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ]    Convert list of numbers to numpy array  # Convert to numpy  x_train   =   np . array ( x_values ,   dtype = np . float32 )  x_train . shape    ( 11 ,)    Convert to 2-dimensional array  If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required  x_train   =   x_train . reshape ( - 1 ,   1 )  x_train . shape    ( 11 ,   1 )    Create list of y values  We want y values for every x value we have above.   y = 2x + 1 y = 2x + 1  y_values   =   [ 2 * i   +   1   for   i   in   x_values ]   y_values    [ 1 ,   3 ,   5 ,   7 ,   9 ,   11 ,   13 ,   15 ,   17 ,   19 ,   21 ]    Alternative to create list of y values  If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators...  y_values   =   []  for   i   in   x_values : \n     result   =   2 * i   +   1 \n     y_values . append ( result )    y_values    [ 1 ,   3 ,   5 ,   7 ,   9 ,   11 ,   13 ,   15 ,   17 ,   19 ,   21 ]    Convert to numpy array  You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good.  y_train   =   np . array ( y_values ,   dtype = np . float32 )  y_train . shape    ( 11 ,)    Reshape y numpy array to 2-dimension  # IMPORTANT: 2D required  y_train   =   y_train . reshape ( - 1 ,   1 )  y_train . shape    ( 11 ,   1 )",
            "title": "Building a Toy Dataset"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-model",
            "text": "Critical Imports  import   torch  import   torch.nn   as   nn     Create Model   Linear model  True Equation:  y = 2x + 1 y = 2x + 1    Forward  Example  Input $x = 1 $  Output  \\hat y = ? \\hat y = ?       # Create class  class   LinearRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LinearRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim )   \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out     Instantiate Model Class   input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]   input_dim   =   1  output_dim   =   1  model   =   LinearRegressionModel ( input_dim ,   output_dim )     Instantiate Loss Class   MSE Loss: Mean Squared Error  MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)  \\hat y \\hat y : prediction  y y : true value     criterion   =   nn . MSELoss ()     Instantiate Optimizer Class   Simplified equation  $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $  \\theta \\theta : parameters (our variables)  \\eta \\eta : learning rate (how fast we want to learn)  \\nabla_\\theta \\nabla_\\theta : parameters' gradients      Even simplier equation  parameters = parameters - learning_rate * parameters_gradients  parameters:  \\alpha \\alpha  and  \\beta \\beta  in $ y = \\alpha x + \\beta$  desired parameters:  \\alpha = 2 \\alpha = 2  and  \\beta = 1 \\beta = 1  in $ y = 2x + 1$        learning_rate   =   0.01  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )     Train Model    1 epoch: going through the whole x_train data once   100 epochs:   100x mapping  x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]       Process    Convert inputs/labels to tensors with gradients  Clear gradient buffets  Get output given inputs   Get loss  Get gradients w.r.t. parameters  Update parameters using gradients  parameters = parameters - learning_rate * parameters_gradients    REPEAT     epochs   =   100   for   epoch   in   range ( epochs ): \n     epoch   +=   1 \n     # Convert numpy array to torch Variable \n     inputs   =   torch . from_numpy ( x_train ) . requires_grad_ () \n     labels   =   torch . from_numpy ( y_train ) \n\n     # Clear gradients w.r.t. parameters \n     optimizer . zero_grad ()  \n\n     # Forward to get output \n     outputs   =   model ( inputs ) \n\n     # Calculate Loss \n     loss   =   criterion ( outputs ,   labels ) \n\n     # Getting gradients w.r.t. parameters \n     loss . backward () \n\n     # Updating parameters \n     optimizer . step () \n\n     print ( 'epoch {}, loss {}' . format ( epoch ,   loss . item ()))    epoch   1 ,   loss   140.58143615722656  epoch   2 ,   loss   11.467253684997559  epoch   3 ,   loss   0.9358152747154236  epoch   4 ,   loss   0.07679400593042374  epoch   5 ,   loss   0.0067212567664682865  epoch   6 ,   loss   0.0010006226366385818  epoch   7 ,   loss   0.0005289533291943371  epoch   8 ,   loss   0.0004854927829001099  epoch   9 ,   loss   0.00047700389404781163  epoch   10 ,   loss   0.0004714332753792405  epoch   11 ,   loss   0.00046614606981165707  epoch   12 ,   loss   0.0004609318566508591  epoch   13 ,   loss   0.0004557870561257005  epoch   14 ,   loss   0.00045069155748933554  epoch   15 ,   loss   0.00044567222357727587  epoch   16 ,   loss   0.00044068993884138763  epoch   17 ,   loss   0.00043576463940553367  epoch   18 ,   loss   0.00043090470717288554  epoch   19 ,   loss   0.00042609183583408594  epoch   20 ,   loss   0.0004213254142086953  epoch   21 ,   loss   0.0004166301223449409  epoch   22 ,   loss   0.0004119801160413772  epoch   23 ,   loss   0.00040738462121225893  epoch   24 ,   loss   0.0004028224211651832  epoch   25 ,   loss   0.0003983367350883782  epoch   26 ,   loss   0.0003938761365134269  epoch   27 ,   loss   0.000389480876037851  epoch   28 ,   loss   0.00038514015614055097  epoch   29 ,   loss   0.000380824290914461  epoch   30 ,   loss   0.00037657516077160835  epoch   31 ,   loss   0.000372376263840124  epoch   32 ,   loss   0.0003682126116473228  epoch   33 ,   loss   0.0003640959912445396  epoch   34 ,   loss   0.00036003670538775623  epoch   35 ,   loss   0.00035601368290372193  epoch   36 ,   loss   0.00035203873994760215  epoch   37 ,   loss   0.00034810820943675935  epoch   38 ,   loss   0.000344215368386358  epoch   39 ,   loss   0.0003403784066904336  epoch   40 ,   loss   0.00033658024040050805  epoch   41 ,   loss   0.0003328165039420128  epoch   42 ,   loss   0.0003291067841928452  epoch   43 ,   loss   0.0003254293987993151  epoch   44 ,   loss   0.0003217888588551432  epoch   45 ,   loss   0.0003182037326041609  epoch   46 ,   loss   0.0003146533854305744  epoch   47 ,   loss   0.00031113551813177764  epoch   48 ,   loss   0.0003076607536058873  epoch   49 ,   loss   0.00030422292184084654  epoch   50 ,   loss   0.00030083119054324925  epoch   51 ,   loss   0.00029746422660537064  epoch   52 ,   loss   0.0002941471466328949  epoch   53 ,   loss   0.00029085995629429817  epoch   54 ,   loss   0.0002876132493838668  epoch   55 ,   loss   0.00028440452297218144  epoch   56 ,   loss   0.00028122696676291525  epoch   57 ,   loss   0.00027808290906250477  epoch   58 ,   loss   0.00027497278642840683  epoch   59 ,   loss   0.00027190230321139097  epoch   60 ,   loss   0.00026887087733484805  epoch   61 ,   loss   0.0002658693410921842  epoch   62 ,   loss   0.0002629039518069476  epoch   63 ,   loss   0.00025996880140155554  epoch   64 ,   loss   0.0002570618235040456  epoch   65 ,   loss   0.00025419273879379034  epoch   66 ,   loss   0.00025135406758636236  epoch   67 ,   loss   0.0002485490695107728  epoch   68 ,   loss   0.0002457649679854512  epoch   69 ,   loss   0.0002430236927466467  epoch   70 ,   loss   0.00024031475186347961  epoch   71 ,   loss   0.00023762597993481904  epoch   72 ,   loss   0.00023497406800743192  epoch   73 ,   loss   0.0002323519001947716  epoch   74 ,   loss   0.00022976362379267812  epoch   75 ,   loss   0.0002271933335578069  epoch   76 ,   loss   0.00022465786605607718  epoch   77 ,   loss   0.00022214400814846158  epoch   78 ,   loss   0.00021966728672850877  epoch   79 ,   loss   0.0002172116219298914  epoch   80 ,   loss   0.00021478648704942316  epoch   81 ,   loss   0.00021239375928416848  epoch   82 ,   loss   0.0002100227284245193  epoch   83 ,   loss   0.00020767028036061674  epoch   84 ,   loss   0.00020534756185952574  epoch   85 ,   loss   0.00020305956422816962  epoch   86 ,   loss   0.0002007894654525444  epoch   87 ,   loss   0.00019854879064951092  epoch   88 ,   loss   0.00019633043848443776  epoch   89 ,   loss   0.00019413618429098278  epoch   90 ,   loss   0.00019197272195015103  epoch   91 ,   loss   0.0001898303598864004  epoch   92 ,   loss   0.00018771187751553953  epoch   93 ,   loss   0.00018561164324637502  epoch   94 ,   loss   0.00018354636267758906  epoch   95 ,   loss   0.00018149390234611928  epoch   96 ,   loss   0.0001794644631445408  epoch   97 ,   loss   0.00017746571393217891  epoch   98 ,   loss   0.00017548113828524947  epoch   99 ,   loss   0.00017352371651213616  epoch   100 ,   loss   0.00017157981346827    Looking at predicted values  # Purely inference  predicted   =   model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy ()  predicted    array ([[   0.9756333 ], \n        [   2.9791424 ], \n        [   4.982651   ], \n        [   6.9861603 ], \n        [   8.98967    ], \n        [ 10.993179   ], \n        [ 12.996688   ], \n        [ 15.000196   ], \n        [ 17.003706   ], \n        [ 19.007215   ], \n        [ 21.010725   ]],   dtype = float32 )    Looking at training values  These are the true values, you can see how it's able to predict similar values.  # y = 2x + 1   y_train    array ([[   1. ], \n        [   3. ], \n        [   5. ], \n        [   7. ], \n        [   9. ], \n        [ 11. ], \n        [ 13. ], \n        [ 15. ], \n        [ 17. ], \n        [ 19. ], \n        [ 21. ]],   dtype = float32 )    Plot of predicted and actual values  # Clear figure  plt . clf ()  # Get predictions  predicted   =   model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy ()  # Plot true data  plt . plot ( x_train ,   y_train ,   'go' ,   label = 'True data' ,   alpha = 0.5 )  # Plot predictions  plt . plot ( x_train ,   predicted ,   '--' ,   label = 'Predictions' ,   alpha = 0.5 )  # Legend and plot  plt . legend ( loc = 'best' )  plt . show ()      Save Model  save_model   =   False  if   save_model   is   True : \n     # Saves only parameters \n     # alpha & beta \n     torch . save ( model . state_dict (),   'awesome_model.pkl' )     Load Model  load_model   =   False  if   load_model   is   True : \n     model . load_state_dict ( torch . load ( 'awesome_model.pkl' ))",
            "title": "Building Model"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch-gpu",
            "text": "CPU Summary  import   torch  import   torch.nn   as   nn  '''  STEP 1: CREATE MODEL CLASS  '''  class   LinearRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LinearRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim )   \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out  '''  STEP 2: INSTANTIATE MODEL CLASS  '''  input_dim   =   1  output_dim   =   1  model   =   LinearRegressionModel ( input_dim ,   output_dim )  '''  STEP 3: INSTANTIATE LOSS CLASS  '''  criterion   =   nn . MSELoss ()  '''  STEP 4: INSTANTIATE OPTIMIZER CLASS  '''  learning_rate   =   0.01  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )  '''  STEP 5: TRAIN THE MODEL  '''  epochs   =   100  for   epoch   in   range ( epochs ): \n     epoch   +=   1 \n     # Convert numpy array to torch Variable \n     inputs   =   torch . from_numpy ( x_train ) . requires_grad_ () \n     labels   =   torch . from_numpy ( y_train ) \n\n     # Clear gradients w.r.t. parameters \n     optimizer . zero_grad ()  \n\n     # Forward to get output \n     outputs   =   model ( inputs ) \n\n     # Calculate Loss \n     loss   =   criterion ( outputs ,   labels ) \n\n     # Getting gradients w.r.t. parameters \n     loss . backward () \n\n     # Updating parameters \n     optimizer . step ()     GPU Summary   Just remember always 2 things must be on GPU  model  tensors with gradients     import   torch  import   torch.nn   as   nn  import   numpy   as   np  '''  STEP 1: CREATE MODEL CLASS  '''  class   LinearRegressionModel ( nn . Module ): \n     def   __init__ ( self ,   input_dim ,   output_dim ): \n         super ( LinearRegressionModel ,   self ) . __init__ () \n         self . linear   =   nn . Linear ( input_dim ,   output_dim )   \n\n     def   forward ( self ,   x ): \n         out   =   self . linear ( x ) \n         return   out  '''  STEP 2: INSTANTIATE MODEL CLASS  '''  input_dim   =   1  output_dim   =   1  model   =   LinearRegressionModel ( input_dim ,   output_dim )  #######################  #  USE GPU FOR MODEL  #  #######################  device   =   torch . device ( \"cuda:0\"   if   torch . cuda . is_available ()   else   \"cpu\" )  model . to ( device )  '''  STEP 3: INSTANTIATE LOSS CLASS  '''  criterion   =   nn . MSELoss ()  '''  STEP 4: INSTANTIATE OPTIMIZER CLASS  '''  learning_rate   =   0.01  optimizer   =   torch . optim . SGD ( model . parameters (),   lr = learning_rate )  '''  STEP 5: TRAIN THE MODEL  '''  epochs   =   100  for   epoch   in   range ( epochs ): \n     epoch   +=   1 \n     # Convert numpy array to torch Variable \n\n     ####################### \n     #  USE GPU FOR MODEL  # \n     ####################### \n     inputs   =   torch . from_numpy ( x_train ) . to ( device ) \n     labels   =   torch . from_numpy ( y_train ) . to ( device ) \n\n     # Clear gradients w.r.t. parameters \n     optimizer . zero_grad ()  \n\n     # Forward to get output \n     outputs   =   model ( inputs ) \n\n     # Calculate Loss \n     loss   =   criterion ( outputs ,   labels ) \n\n     # Getting gradients w.r.t. parameters \n     loss . backward () \n\n     # Updating parameters \n     optimizer . step () \n\n     # Logging \n     print ( 'epoch {}, loss {}' . format ( epoch ,   loss . item ()))    epoch   1 ,   loss   336.0314025878906  epoch   2 ,   loss   27.67657470703125  epoch   3 ,   loss   2.5220539569854736  epoch   4 ,   loss   0.46732547879219055  epoch   5 ,   loss   0.2968060076236725  epoch   6 ,   loss   0.2800087630748749  epoch   7 ,   loss   0.27578213810920715  epoch   8 ,   loss   0.2726128399372101  epoch   9 ,   loss   0.269561231136322  epoch   10 ,   loss   0.2665504515171051  epoch   11 ,   loss   0.2635740041732788  epoch   12 ,   loss   0.26063060760498047  epoch   13 ,   loss   0.2577202618122101  epoch   14 ,   loss   0.2548423111438751  epoch   15 ,   loss   0.25199657678604126  epoch   16 ,   loss   0.24918246269226074  epoch   17 ,   loss   0.24639996886253357  epoch   18 ,   loss   0.24364829063415527  epoch   19 ,   loss   0.24092751741409302  epoch   20 ,   loss   0.2382371574640274  epoch   21 ,   loss   0.23557686805725098  epoch   22 ,   loss   0.2329462170600891  epoch   23 ,   loss   0.2303449958562851  epoch   24 ,   loss   0.22777271270751953  epoch   25 ,   loss   0.2252292037010193  epoch   26 ,   loss   0.22271405160427094  epoch   27 ,   loss   0.22022713720798492  epoch   28 ,   loss   0.21776780486106873  epoch   29 ,   loss   0.21533599495887756  epoch   30 ,   loss   0.21293145418167114  epoch   31 ,   loss   0.21055366098880768  epoch   32 ,   loss   0.20820240676403046  epoch   33 ,   loss   0.2058774083852768  epoch   34 ,   loss   0.20357847213745117  epoch   35 ,   loss   0.20130516588687897  epoch   36 ,   loss   0.1990572065114975  epoch   37 ,   loss   0.19683438539505005  epoch   38 ,   loss   0.19463638961315155  epoch   39 ,   loss   0.19246290624141693  epoch   40 ,   loss   0.1903136670589447  epoch   41 ,   loss   0.1881885528564453  epoch   42 ,   loss   0.18608702719211578  epoch   43 ,   loss   0.18400898575782776  epoch   44 ,   loss   0.18195408582687378  epoch   45 ,   loss   0.17992223799228668  epoch   46 ,   loss   0.17791320383548737  epoch   47 ,   loss   0.17592646181583405  epoch   48 ,   loss   0.17396186292171478  epoch   49 ,   loss   0.17201924324035645  epoch   50 ,   loss   0.17009828984737396  epoch   51 ,   loss   0.16819894313812256  epoch   52 ,   loss   0.16632060706615448  epoch   53 ,   loss   0.16446338593959808  epoch   54 ,   loss   0.16262666881084442  epoch   55 ,   loss   0.16081078350543976  epoch   56 ,   loss   0.15901507437229156  epoch   57 ,   loss   0.15723931789398193  epoch   58 ,   loss   0.15548335015773773  epoch   59 ,   loss   0.15374726057052612  epoch   60 ,   loss   0.1520303338766098  epoch   61 ,   loss   0.15033268928527832  epoch   62 ,   loss   0.14865389466285706  epoch   63 ,   loss   0.14699392020702362  epoch   64 ,   loss   0.14535246789455414  epoch   65 ,   loss   0.14372935891151428  epoch   66 ,   loss   0.14212435483932495  epoch   67 ,   loss   0.14053721725940704  epoch   68 ,   loss   0.13896773755550385  epoch   69 ,   loss   0.1374160647392273  epoch   70 ,   loss   0.1358814686536789  epoch   71 ,   loss   0.13436420261859894  epoch   72 ,   loss   0.13286370038986206  epoch   73 ,   loss   0.1313801407814026  epoch   74 ,   loss   0.12991292774677277  epoch   75 ,   loss   0.12846232950687408  epoch   76 ,   loss   0.1270277351140976  epoch   77 ,   loss   0.12560924887657166  epoch   78 ,   loss   0.12420656532049179  epoch   79 ,   loss   0.12281957268714905  epoch   80 ,   loss   0.1214480847120285  epoch   81 ,   loss   0.12009195983409882  epoch   82 ,   loss   0.1187509223818779  epoch   83 ,   loss   0.11742479354143143  epoch   84 ,   loss   0.11611353605985641  epoch   85 ,   loss   0.11481687426567078  epoch   86 ,   loss   0.11353478580713272  epoch   87 ,   loss   0.11226697266101837  epoch   88 ,   loss   0.11101329326629639  epoch   89 ,   loss   0.10977360606193542  epoch   90 ,   loss   0.10854770988225937  epoch   91 ,   loss   0.10733554512262344  epoch   92 ,   loss   0.10613703727722168  epoch   93 ,   loss   0.10495180636644363  epoch   94 ,   loss   0.10377981513738632  epoch   95 ,   loss   0.10262089222669601  epoch   96 ,   loss   0.10147502273321152  epoch   97 ,   loss   0.1003417894244194  epoch   98 ,   loss   0.09922132641077042  epoch   99 ,   loss   0.0981132984161377  epoch   100 ,   loss   0.09701769798994064",
            "title": "Building a Linear Regression Model with PyTorch (GPU)"
        },
        {
            "location": "/deep_learning/practical_pytorch/pytorch_linear_regression/#summary",
            "text": "We've learnt to...   Success    Simple  linear regression basics    y = Ax + B y = Ax + B    y = 2x + 1 y = 2x + 1      Example  of simple linear regression    Aim  of linear regression   Minimizing distance between the points and the line   Calculate \"distance\" through  MSE   Calculate  gradients   Update parameters with  parameters = parameters - learning_rate * gradients   Slowly update parameters  A A  and  B B  model the linear relationship between  y y  and  x x  of the form  y = 2x + 1 y = 2x + 1       Built a linear regression  model  in  CPU and GPU   Step 1: Create Model Class   Step 2: Instantiate Model Class   Step 3: Instantiate Loss Class   Step 4: Instantiate Optimizer Class   Step 5: Train Model     Important things to be on  GPU    model    tensors with gradients     How to bring to  GPU ?  model_name.to(device)  variable_name.to(device)",
            "title": "Summary"
        },
        {
            "location": "/news/news/",
            "text": "Welcome to our Blog\n\u00b6\n\n\nHere, we post news related to Deep Learning Wizard's releases, features and achievements \n\n\nNotable News\n\u00b6\n\n\n\n\n Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, July 2018\n\n\n Reached 2200+ students, 2018\n\n\n Featured on PyTorch Website, January 2018\n\n\n Reached 1000+ students, 2017\n\n\n Hosted NVIDIA Self-Driving Cars and Healthcare Talk, June 2017\n\n\n And more...",
            "title": "Welcome"
        },
        {
            "location": "/news/news/#welcome-to-our-blog",
            "text": "Here, we post news related to Deep Learning Wizard's releases, features and achievements",
            "title": "Welcome to our Blog"
        },
        {
            "location": "/news/news/#notable-news",
            "text": "Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, July 2018   Reached 2200+ students, 2018   Featured on PyTorch Website, January 2018   Reached 1000+ students, 2017   Hosted NVIDIA Self-Driving Cars and Healthcare Talk, June 2017   And more...",
            "title": "Notable News"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/",
            "text": "NVIDIA Workshop at NUS-MIT-NUHS Datathon\n\u00b6\n\n\nImage Recognition Workshop by Ritchie Ng\n\u00b6\n\n\nThe NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.\n\n\nIn \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.\n\n\nLink to the NUS-MIT-NUHS Datathon \nworkshop\n.",
            "title": "NUS-MIT-NUHS NVIDIA Image Recognition Workshop July 2018"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/#nvidia-workshop-at-nus-mit-nuhs-datathon",
            "text": "",
            "title": "NVIDIA Workshop at NUS-MIT-NUHS Datathon"
        },
        {
            "location": "/news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng",
            "text": "The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning.  In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance.  Link to the NUS-MIT-NUHS Datathon  workshop .",
            "title": "Image Recognition Workshop by Ritchie Ng"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/",
            "text": "Featured on PyTorch Website\n\u00b6\n\n\nPyTorch a Year Later\n\u00b6\n\n\nWe are featured on \nPyTorch website's post\n \n\n\nI used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.\n\n\nA year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!\n\n\nA big shoutout for \nAlfredo Canziani\n who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.\n\n\nTo more great years ahead for PyTorch \n\n\nCheers,\n\nRitchie Ng",
            "title": "Featured on PyTorch Website 2018"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website",
            "text": "",
            "title": "Featured on PyTorch Website"
        },
        {
            "location": "/news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later",
            "text": "We are featured on  PyTorch website's post    I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday.  A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned!  A big shoutout for  Alfredo Canziani  who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome.  To more great years ahead for PyTorch   Cheers, Ritchie Ng",
            "title": "PyTorch a Year Later"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/",
            "text": "NVIDIA Self-Driving Cars and Healthcare Workshop\n\u00b6\n\n\nHosted by Ritchie Ng\n\u00b6\n\n\nA talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.\n\n\nWe will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.\n\n\nDetails:\nWednesday, June 21\nst\n\n1:00 PM to 3:30 PM\n\n\nThe Hangar by NUS Enterprise\n21 Heng Mui Keng Terrace, Singapore 119613",
            "title": "NVIDIA Self Driving Cars & Healthcare Talk June 2017"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/#nvidia-self-driving-cars-and-healthcare-workshop",
            "text": "",
            "title": "NVIDIA Self-Driving Cars and Healthcare Workshop"
        },
        {
            "location": "/news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng",
            "text": "A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS.  We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA.  Details:\nWednesday, June 21 st \n1:00 PM to 3:30 PM  The Hangar by NUS Enterprise\n21 Heng Mui Keng Terrace, Singapore 119613",
            "title": "Hosted by Ritchie Ng"
        }
    ]
}