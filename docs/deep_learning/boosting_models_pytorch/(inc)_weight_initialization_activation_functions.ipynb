{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Weight Initializations & Activation Functions\n",
    "\n",
    "## Recap of Logistic Regression\n",
    "<img src=\"./images/cross_entropy_final_4.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
    "\n",
    "## Recap of Feedforward Neural Network Activation Function\n",
    "<img src=\"./images/logistic_regression_comparison_nn5.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
    "\n",
    "#### Sigmoid (Logistic)\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Input number $\\rightarrow$ [0, 1]\n",
    "    - Large negative number $\\rightarrow$ 0\n",
    "    - Large positive number $\\rightarrow$ 1\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - Solution: Have to carefully initialize weights to prevent this\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10d46dad0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEECAYAAADJSpQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYU9EFAX+PyQqAGwvdYiPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d368110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+np.exp(-item)))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(x,sig, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "- $\\tanh(x) = 2 \\sigma(2x) -1$\n",
    "    - A scaled sigmoid function\n",
    "- Input number $\\rightarrow$ [-1, 1]\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - **Solution**: Have to carefully initialize weights to prevent this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10d578b50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEECAYAAADK0VhyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhg4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d368990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10., 10., 0.2)\n",
    "tanh = np.dot(2, sigmoid(np.dot(2, x))) - 1\n",
    "\n",
    "plt.plot(x,tanh, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLUs\n",
    "- $f(x) = \\max(0, x)$\n",
    "- Pros:\n",
    "    1. Accelerates convergence $\\rightarrow$ **train faster**\n",
    "    2. **Less computationally expensive operation** compared to Sigmoid/Tanh exponentials\n",
    "- Cons:\n",
    "    1. Many ReLU units \"die\" $\\rightarrow$ **gradients = 0** forever\n",
    "        - **Solution**: careful learning rate and weight initialization choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10c33de10>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEECAYAAAAifS8cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AEgoAh4AAgoAh4AAoqAB4CA+n/3S1R4Mtk9ygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d6e1690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10., 10., 0.2)\n",
    "relu = np.maximum(x, 0)\n",
    "\n",
    "plt.plot(x,relu, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need weight initializations or new activation functions?\n",
    "- **To prevent vanishing/exploding gradients**\n",
    "\n",
    "### Case 1: Sigmoid/Tanh\n",
    "- **Problem**\n",
    "    - If variance of input too large: gradients = 0 (vanishing gradients)\n",
    "    - If variance of input too small: linear $\\rightarrow$ gradients = constant value\n",
    "- **Solutions**\n",
    "    - Want a constant variance of input to achieve non-linearity $\\rightarrow$ unique gradients for unique updates\n",
    "        - Xavier Initialization (good constant variance for Sigmoid/Tanh)\n",
    "        - ReLU or Leaky ReLU\n",
    "\n",
    "### Case 2: ReLU\n",
    "- **Solution to Case 1**\n",
    "    - Regardless of variance of input: gradients = 0 or 1 \n",
    "- **Problem**\n",
    "    - But those with 0: no updates (\"dead ReLU units\") \n",
    "    - Has unlimited output size with input > 0 (explodes gradients subsequently)\n",
    "- **Solutions**\n",
    "    - He Initialization (good constant variance)\n",
    "    - Leaky ReLU\n",
    "\n",
    "### Case 3: Leaky ReLU\n",
    "- **Solution to Case 2**\n",
    "    - Solves the 0 signal issue when input < 0 \n",
    "![](./images/leaky_relu_compare2.png)\n",
    "- **Problem**\n",
    "    - Has unlimited output size with input > 0 (explodes)\n",
    "- **Solution**\n",
    "    - He Initialization (good constant variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of weight initialization solutions to activations\n",
    "- Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization\n",
    "    - Good range of constant variance\n",
    "- ReLU/Leaky ReLU exploding gradients can be solved with He initialization\n",
    "    - Good range of constant variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Types of weight intializations\n",
    "\n",
    "#### Zero Initialization: set all weights to 0 \n",
    "- Every neuron in the network computes the same output $\\rightarrow$ computes the same gradient $\\rightarrow$ same parameter updates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Initialization: set all weights to random small numbers\n",
    "- Every neuron in the network computes different output $\\rightarrow$ computes different gradient $\\rightarrow$ different parameter updates \n",
    "- \"Symmetry breaking\" \n",
    "- Problem: variance that grows with the number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'#### Xavier Initialization: normalize variance\n",
    "- Solves growing variance with the number of inputs $\\rightarrow$ constant variance \n",
    "- Look at a simple feedforward neural network\n",
    "![](./images/nn2.png)\n",
    "\n",
    "**Some equations to explain**\n",
    "- $Y = AX + B$\n",
    "- $y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b$\n",
    "- $Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b)$\n",
    "- $Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i)$\n",
    "    - General term, you might be more familiar with the following\n",
    "        - $Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y)$\n",
    "    - $E(x_i)$: expectation/mean of $x_i$\n",
    "    - $E(a_i)$: expectation/mean of $a_i$\n",
    "- Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0\n",
    "    - $E(x_i) = E(a_i) = 0$\n",
    "    - $Var(a_i x_i) = Var(a_i)Var(x_i)$\n",
    "- $Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) $\n",
    "    - Since the bias, b, is a constant, $Var(b) = 0$\n",
    "- Since i.i.d.\n",
    "    - $Var(y) = n \\times Var(a_i)Var(x_i) $\n",
    "- Since we want constant variance where $ Var(y) = Var(x_i) $\n",
    "    - $1 = nVar(a_i)$\n",
    "    - $Var(a_i) = \\frac{1}{n}$\n",
    "- This is essentially a simplified Xavier initialization\n",
    "    - We draw our weights i.i.d. with mean=0 and variance = $\\frac{1}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### He Initialization: normalize variance\n",
    "- Modification of Xavier for ReLU activations\n",
    "- Xavier variance = $\\frac{1}{n}$\n",
    "- He variance = $\\frac{2}{n}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of  weight initializations\n",
    "- Normal Distribution\n",
    "- Xavier Normal Distribution\n",
    "- He Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initializations with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Initialization: Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.29728397727012634. Accuracy: 88.35\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.29919782280921936. Accuracy: 90.0\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.4477359652519226. Accuracy: 91.02\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.4226881265640259. Accuracy: 91.6\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.09856002777814865. Accuracy: 92.42\n",
      "Iteration: 3000. Loss: 0.28250059485435486. Accuracy: 92.54\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.normal(self.fc1.weight, mean=0, std=1)\n",
    "        # Non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.normal(self.fc2.weight, mean=0, std=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.tanh(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization: Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.08207494765520096. Accuracy: 95.67\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.1031789779663086. Accuracy: 96.58\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.1701303869485855. Accuracy: 97.19\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.05285242572426796. Accuracy: 97.28\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.013970551081001759. Accuracy: 97.52\n",
      "Iteration: 3000. Loss: 0.06025965139269829. Accuracy: 97.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.xavier_normal(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.xavier_normal(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.tanh(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization: ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.06129903346300125. Accuracy: 96.04\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.11424191296100616. Accuracy: 96.76\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.2197684794664383. Accuracy: 97.32\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.05090019851922989. Accuracy: 97.39\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.009756524115800858. Accuracy: 97.45\n",
      "Iteration: 3000. Loss: 0.023999467492103577. Accuracy: 97.59\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.xavier_normal(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.xavier_normal(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization: ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.06682520359754562. Accuracy: 96.26\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.09697430580854416. Accuracy: 96.91\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.2139369249343872. Accuracy: 97.34\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.04469038546085358. Accuracy: 97.21\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.008070570416748524. Accuracy: 97.48\n",
      "Iteration: 3000. Loss: 0.02822907269001007. Accuracy: 97.35\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.kaiming_normal(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.kaiming_normal(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Performance\n",
    "\n",
    "| Initialization: Activation        | Test Accuracy           | \n",
    "| :-------------: |:-------------:| \n",
    "| Normal: Tanh| 92.54 | \n",
    "| Xavier: Tanh    | 97.52     |\n",
    "| Xavier: ReLU | 97.59    |  \n",
    "| He: ReLU | 97.48     |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Recap of LG\n",
    "- Recap of FNN\n",
    "- Recap of Activation Functions\n",
    "    - Sigmoid (Logistic)\n",
    "    - Tanh\n",
    "    - ReLU\n",
    "- Need for Weight Initializations\n",
    "    - Sigmoid/Tanh: vanishing gradients\n",
    "        - Constant Variance initialization with Xavier \n",
    "    - ReLU: exploding gradients with dead units\n",
    "        - He Initialization\n",
    "    - Leaky ReLU: exploding gradients only\n",
    "        - He Initialization\n",
    "- Types of weight initialisations\n",
    "    - Zero\n",
    "    - Normal: growing weight variance\n",
    "    - Xavier: constant variance\n",
    "    - Kaiming He: constant variance for ReLU activations\n",
    "- PyTorch implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
