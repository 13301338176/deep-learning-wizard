{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "- Let's go back to our simple FNN to put things in perspective\n",
    "    - Let us ignore non-linearities for now to keep it simpler, but it's just a tiny change subsequently\n",
    "    - Given a linear transformation on our input (for simplicity instead of an affine transformation that includes a bias): $\\hat y = \\theta x$\n",
    "        - $\\theta$ is our parameters\n",
    "        - $x$ is our input\n",
    "        - $\\hat y$ is our prediction\n",
    "    - Then we have our loss function $L = \\frac{1}{2} (\\hat y - y)$\n",
    "- We need to calculate our partial derivatives of our loss w.r.t. our parameters to update our parameters: $\\nabla_{\\theta} = \\frac{dL}{d\\theta}$\n",
    "    - With chain rule we have $\\frac{dL}{d \\theta} = \\frac{dL}{dy} \\frac{dy}{d \\theta}$\n",
    "        - $\\frac{dL}{dy} = (\\hat y -  y)$\n",
    "        - $\\frac{dy}{d \\theta}$ is our partial derivatives of y w.r.t. our parameters (our gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
    "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
    "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale units\n",
    "X_max, _ = torch.max(X, 0)\n",
    "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
    "\n",
    "X = torch.div(X, X_max)\n",
    "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
    "y = y / 100  # max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "        \n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.15593862533569336\n",
      "#1 Loss: 0.09614866971969604\n",
      "#2 Loss: 0.06381534039974213\n",
      "#3 Loss: 0.045552074909210205\n",
      "#4 Loss: 0.03445689007639885\n",
      "#5 Loss: 0.027250075712800026\n",
      "#6 Loss: 0.022305255755782127\n",
      "#7 Loss: 0.018760118633508682\n",
      "#8 Loss: 0.016126910224556923\n",
      "#9 Loss: 0.01411380898207426\n",
      "#10 Loss: 0.012537665665149689\n",
      "#11 Loss: 0.011278732679784298\n",
      "#12 Loss: 0.010256008245050907\n",
      "#13 Loss: 0.00941301230341196\n",
      "#14 Loss: 0.008709365501999855\n",
      "#15 Loss: 0.008115552365779877\n",
      "#16 Loss: 0.007609547581523657\n",
      "#17 Loss: 0.007174663711339235\n",
      "#18 Loss: 0.0067980121821165085\n",
      "#19 Loss: 0.006469555199146271\n",
      "#20 Loss: 0.006181338336318731\n",
      "#21 Loss: 0.00592699833214283\n",
      "#22 Loss: 0.005701401736587286\n",
      "#23 Loss: 0.005500353407114744\n",
      "#24 Loss: 0.005320416763424873\n",
      "#25 Loss: 0.005158723331987858\n",
      "#26 Loss: 0.00501289451494813\n",
      "#27 Loss: 0.004880925640463829\n",
      "#28 Loss: 0.004761120770126581\n",
      "#29 Loss: 0.00465202983468771\n",
      "#30 Loss: 0.0045524281449615955\n",
      "#31 Loss: 0.00446124654263258\n",
      "#32 Loss: 0.004377579316496849\n",
      "#33 Loss: 0.004300621338188648\n",
      "#34 Loss: 0.004229693673551083\n",
      "#35 Loss: 0.00416417745873332\n",
      "#36 Loss: 0.004103557672351599\n",
      "#37 Loss: 0.004047350957989693\n",
      "#38 Loss: 0.003995157778263092\n",
      "#39 Loss: 0.003946609329432249\n",
      "#40 Loss: 0.0039013742934912443\n",
      "#41 Loss: 0.0038591676857322454\n",
      "#42 Loss: 0.003819728037342429\n",
      "#43 Loss: 0.0037828341592103243\n",
      "#44 Loss: 0.0037482576444745064\n",
      "#45 Loss: 0.0037158250343054533\n",
      "#46 Loss: 0.0036853712517768145\n",
      "#47 Loss: 0.003656734712421894\n",
      "#48 Loss: 0.003629772225394845\n",
      "#49 Loss: 0.0036043694708496332\n",
      "#50 Loss: 0.003580396296456456\n",
      "#51 Loss: 0.003557766554877162\n",
      "#52 Loss: 0.0035363773349672556\n",
      "#53 Loss: 0.0035161443520337343\n",
      "#54 Loss: 0.003496983088552952\n",
      "#55 Loss: 0.0034788220655173063\n",
      "#56 Loss: 0.003461591899394989\n",
      "#57 Loss: 0.0034452404361218214\n",
      "#58 Loss: 0.0034297024831175804\n",
      "#59 Loss: 0.0034149326384067535\n",
      "#60 Loss: 0.003400870831683278\n",
      "#61 Loss: 0.003387484000995755\n",
      "#62 Loss: 0.0033747293055057526\n",
      "#63 Loss: 0.0033625606447458267\n",
      "#64 Loss: 0.0033509458880871534\n",
      "#65 Loss: 0.0033398575615137815\n",
      "#66 Loss: 0.0033292651642113924\n",
      "#67 Loss: 0.003319135634228587\n",
      "#68 Loss: 0.00330943800508976\n",
      "#69 Loss: 0.0033001499250531197\n",
      "#70 Loss: 0.0032912585884332657\n",
      "#71 Loss: 0.0032827339600771666\n",
      "#72 Loss: 0.0032745616044849157\n",
      "#73 Loss: 0.0032667135819792747\n",
      "#74 Loss: 0.003259173361584544\n",
      "#75 Loss: 0.003251938149333\n",
      "#76 Loss: 0.003244977444410324\n",
      "#77 Loss: 0.0032382828649133444\n",
      "#78 Loss: 0.0032318371813744307\n",
      "#79 Loss: 0.003225628286600113\n",
      "#80 Loss: 0.0032196503598243\n",
      "#81 Loss: 0.003213886870071292\n",
      "#82 Loss: 0.0032083287369459867\n",
      "#83 Loss: 0.003202967345714569\n",
      "#84 Loss: 0.003197782440111041\n",
      "#85 Loss: 0.0031927796080708504\n",
      "#86 Loss: 0.003187948139384389\n",
      "#87 Loss: 0.003183267777785659\n",
      "#88 Loss: 0.003178747370839119\n",
      "#89 Loss: 0.0031743727158755064\n",
      "#90 Loss: 0.0031701279804110527\n",
      "#91 Loss: 0.0031660220120102167\n",
      "#92 Loss: 0.0031620366498827934\n",
      "#93 Loss: 0.0031581807415932417\n",
      "#94 Loss: 0.0031544361263513565\n",
      "#95 Loss: 0.003150796750560403\n",
      "#96 Loss: 0.003147259121760726\n",
      "#97 Loss: 0.003143833950161934\n",
      "#98 Loss: 0.003140498651191592\n",
      "#99 Loss: 0.003137253923341632\n",
      "#100 Loss: 0.00313409510999918\n",
      "#101 Loss: 0.0031310273334383965\n",
      "#102 Loss: 0.003128036158159375\n",
      "#103 Loss: 0.0031251199543476105\n",
      "#104 Loss: 0.003122275695204735\n",
      "#105 Loss: 0.00311950477771461\n",
      "#106 Loss: 0.003116806736215949\n",
      "#107 Loss: 0.003114165971055627\n",
      "#108 Loss: 0.003111593658104539\n",
      "#109 Loss: 0.0031090788543224335\n",
      "#110 Loss: 0.003106616670265794\n",
      "#111 Loss: 0.003104214556515217\n",
      "#112 Loss: 0.0031018622685223818\n",
      "#113 Loss: 0.003099562833085656\n",
      "#114 Loss: 0.00309731368906796\n",
      "#115 Loss: 0.0030951073858886957\n",
      "#116 Loss: 0.003092946484684944\n",
      "#117 Loss: 0.0030908279586583376\n",
      "#118 Loss: 0.0030887543689459562\n",
      "#119 Loss: 0.0030867133755236864\n",
      "#120 Loss: 0.0030847161542624235\n",
      "#121 Loss: 0.0030827538575977087\n",
      "#122 Loss: 0.0030808299779891968\n",
      "#123 Loss: 0.0030789391603320837\n",
      "#124 Loss: 0.003077073721215129\n",
      "#125 Loss: 0.0030752469319850206\n",
      "#126 Loss: 0.003073448082432151\n",
      "#127 Loss: 0.0030716818291693926\n",
      "#128 Loss: 0.0030699362978339195\n",
      "#129 Loss: 0.003068219870328903\n",
      "#130 Loss: 0.003066535107791424\n",
      "#131 Loss: 0.0030648689717054367\n",
      "#132 Loss: 0.0030632326379418373\n",
      "#133 Loss: 0.003061616560444236\n",
      "#134 Loss: 0.0030600198078900576\n",
      "#135 Loss: 0.0030584472697228193\n",
      "#136 Loss: 0.0030568940564990044\n",
      "#137 Loss: 0.0030553629621863365\n",
      "#138 Loss: 0.003053847700357437\n",
      "#139 Loss: 0.0030523475725203753\n",
      "#140 Loss: 0.0030508737545460463\n",
      "#141 Loss: 0.003049408784136176\n",
      "#142 Loss: 0.0030479691922664642\n",
      "#143 Loss: 0.0030465414747595787\n",
      "#144 Loss: 0.003045126562938094\n",
      "#145 Loss: 0.003043730743229389\n",
      "#146 Loss: 0.0030423456337302923\n",
      "#147 Loss: 0.003040972165763378\n",
      "#148 Loss: 0.0030396180227398872\n",
      "#149 Loss: 0.0030382724944502115\n",
      "#150 Loss: 0.0030369365122169256\n",
      "#151 Loss: 0.0030356196220964193\n",
      "#152 Loss: 0.0030343111138790846\n",
      "#153 Loss: 0.0030330114532262087\n",
      "#154 Loss: 0.003031722968444228\n",
      "#155 Loss: 0.003030449151992798\n",
      "#156 Loss: 0.0030291806906461716\n",
      "#157 Loss: 0.003027923172339797\n",
      "#158 Loss: 0.003026670543476939\n",
      "#159 Loss: 0.0030254304874688387\n",
      "#160 Loss: 0.0030242002103477716\n",
      "#161 Loss: 0.003022972261533141\n",
      "#162 Loss: 0.0030217620078474283\n",
      "#163 Loss: 0.0030205529183149338\n",
      "#164 Loss: 0.003019350813701749\n",
      "#165 Loss: 0.0030181556940078735\n",
      "#166 Loss: 0.003016972215846181\n",
      "#167 Loss: 0.003015793627128005\n",
      "#168 Loss: 0.0030146182980388403\n",
      "#169 Loss: 0.0030134497210383415\n",
      "#170 Loss: 0.0030122885946184397\n",
      "#171 Loss: 0.003011136082932353\n",
      "#172 Loss: 0.003009981708601117\n",
      "#173 Loss: 0.003008835716173053\n",
      "#174 Loss: 0.0030076950788497925\n",
      "#175 Loss: 0.0030065590981394053\n",
      "#176 Loss: 0.0030054282397031784\n",
      "#177 Loss: 0.0030043041333556175\n",
      "#178 Loss: 0.003003183752298355\n",
      "#179 Loss: 0.0030020661652088165\n",
      "#180 Loss: 0.003000951372087002\n",
      "#181 Loss: 0.002999840071424842\n",
      "#182 Loss: 0.002998732030391693\n",
      "#183 Loss: 0.0029976293444633484\n",
      "#184 Loss: 0.0029965301509946585\n",
      "#185 Loss: 0.0029954344499856234\n",
      "#186 Loss: 0.0029943399131298065\n",
      "#187 Loss: 0.0029932481702417135\n",
      "#188 Loss: 0.0029921589884907007\n",
      "#189 Loss: 0.002991076558828354\n",
      "#190 Loss: 0.0029899931978434324\n",
      "#191 Loss: 0.002988911932334304\n",
      "#192 Loss: 0.002987835556268692\n",
      "#193 Loss: 0.00298676616512239\n",
      "#194 Loss: 0.002985691651701927\n",
      "#195 Loss: 0.002984620863571763\n",
      "#196 Loss: 0.002983556129038334\n",
      "#197 Loss: 0.00298248790204525\n",
      "#198 Loss: 0.002981429221108556\n",
      "#199 Loss: 0.0029803619254380465\n",
      "#200 Loss: 0.0029793072026222944\n",
      "#201 Loss: 0.002978244563564658\n",
      "#202 Loss: 0.002977187978103757\n",
      "#203 Loss: 0.002976135117933154\n",
      "#204 Loss: 0.0029750813264399767\n",
      "#205 Loss: 0.0029740259051322937\n",
      "#206 Loss: 0.0029729753732681274\n",
      "#207 Loss: 0.0029719241429120302\n",
      "#208 Loss: 0.002970878966152668\n",
      "#209 Loss: 0.002969828201457858\n",
      "#210 Loss: 0.0029687825590372086\n",
      "#211 Loss: 0.0029677387792617083\n",
      "#212 Loss: 0.002966691739857197\n",
      "#213 Loss: 0.002965647494420409\n",
      "#214 Loss: 0.002964603016152978\n",
      "#215 Loss: 0.0029635606333613396\n",
      "#216 Loss: 0.0029625233728438616\n",
      "#217 Loss: 0.0029614779632538557\n",
      "#218 Loss: 0.002960437908768654\n",
      "#219 Loss: 0.0029594001825898886\n",
      "#220 Loss: 0.0029583608265966177\n",
      "#221 Loss: 0.002957323333248496\n",
      "#222 Loss: 0.0029562849085778\n",
      "#223 Loss: 0.0029552497435361147\n",
      "#224 Loss: 0.0029542117845267057\n",
      "#225 Loss: 0.0029531766194850206\n",
      "#226 Loss: 0.0029521360993385315\n",
      "#227 Loss: 0.0029511048924177885\n",
      "#228 Loss: 0.0029500704258680344\n",
      "#229 Loss: 0.0029490359593182802\n",
      "#230 Loss: 0.002947999397292733\n",
      "#231 Loss: 0.0029469591099768877\n",
      "#232 Loss: 0.002945926273241639\n",
      "#233 Loss: 0.0029448920395225286\n",
      "#234 Loss: 0.0029438568744808435\n",
      "#235 Loss: 0.0029428235720843077\n",
      "#236 Loss: 0.002941789338365197\n",
      "#237 Loss: 0.0029407490510493517\n",
      "#238 Loss: 0.0029397213365882635\n",
      "#239 Loss: 0.0029386819805949926\n",
      "#240 Loss: 0.0029376475140452385\n",
      "#241 Loss: 0.002936615841463208\n",
      "#242 Loss: 0.00293557601980865\n",
      "#243 Loss: 0.0029345443472266197\n",
      "#244 Loss: 0.00293350568972528\n",
      "#245 Loss: 0.0029324758797883987\n",
      "#246 Loss: 0.0029314367566257715\n",
      "#247 Loss: 0.0029303983319550753\n",
      "#248 Loss: 0.002929368056356907\n",
      "#249 Loss: 0.002928328700363636\n",
      "#250 Loss: 0.0029272919055074453\n",
      "#251 Loss: 0.0029262546449899673\n",
      "#252 Loss: 0.002925219014286995\n",
      "#253 Loss: 0.0029241808224469423\n",
      "#254 Loss: 0.002923138439655304\n",
      "#255 Loss: 0.002922109328210354\n",
      "#256 Loss: 0.0029210641514509916\n",
      "#257 Loss: 0.0029200275894254446\n",
      "#258 Loss: 0.0029189896304160357\n",
      "#259 Loss: 0.002917950041592121\n",
      "#260 Loss: 0.0029169078916311264\n",
      "#261 Loss: 0.0029158678371459246\n",
      "#262 Loss: 0.0029148266185075045\n",
      "#263 Loss: 0.002913791686296463\n",
      "#264 Loss: 0.0029127460438758135\n",
      "#265 Loss: 0.002911699004471302\n",
      "#266 Loss: 0.0029106608126312494\n",
      "#267 Loss: 0.002909620525315404\n",
      "#268 Loss: 0.0029085762798786163\n",
      "#269 Loss: 0.0029075269121676683\n",
      "#270 Loss: 0.0029064828995615244\n",
      "#271 Loss: 0.0029054442420601845\n",
      "#272 Loss: 0.0029043937101960182\n",
      "#273 Loss: 0.002903349930420518\n",
      "#274 Loss: 0.0029023010283708572\n",
      "#275 Loss: 0.002901257248595357\n",
      "#276 Loss: 0.002900207182392478\n",
      "#277 Loss: 0.0028991561848670244\n",
      "#278 Loss: 0.0028981072828173637\n",
      "#279 Loss: 0.0028970639687031507\n",
      "#280 Loss: 0.0028960073832422495\n",
      "#281 Loss: 0.0028949601110070944\n",
      "#282 Loss: 0.0028939053881913424\n",
      "#283 Loss: 0.0028928574174642563\n",
      "#284 Loss: 0.0028918019961565733\n",
      "#285 Loss: 0.0028907526284456253\n",
      "#286 Loss: 0.0028896990697830915\n",
      "#287 Loss: 0.002888643881306052\n",
      "#288 Loss: 0.002887589856982231\n",
      "#289 Loss: 0.002886534435674548\n",
      "#290 Loss: 0.0028854801785200834\n",
      "#291 Loss: 0.0028844207990914583\n",
      "#292 Loss: 0.0028833651449531317\n",
      "#293 Loss: 0.0028823043685406446\n",
      "#294 Loss: 0.0028812449891120195\n",
      "#295 Loss: 0.0028801856096833944\n",
      "#296 Loss: 0.002879129955545068\n",
      "#297 Loss: 0.002878063591197133\n",
      "#298 Loss: 0.002876998856663704\n",
      "#299 Loss: 0.0028759429696947336\n",
      "#300 Loss: 0.0028748793993145227\n",
      "#301 Loss: 0.0028738200198858976\n",
      "#302 Loss: 0.002872750861570239\n",
      "#303 Loss: 0.0028716863598674536\n",
      "#304 Loss: 0.0028706190641969442\n",
      "#305 Loss: 0.0028695554938167334\n",
      "#306 Loss: 0.002868489595130086\n",
      "#307 Loss: 0.0028674155473709106\n",
      "#308 Loss: 0.0028663529083132744\n",
      "#309 Loss: 0.0028652839828282595\n",
      "#310 Loss: 0.0028642145916819572\n",
      "#311 Loss: 0.0028631407767534256\n",
      "#312 Loss: 0.002862066961824894\n",
      "#313 Loss: 0.002861000830307603\n",
      "#314 Loss: 0.002859926549717784\n",
      "#315 Loss: 0.0028588564600795507\n",
      "#316 Loss: 0.0028577789198607206\n",
      "#317 Loss: 0.002856702543795109\n",
      "#318 Loss: 0.002855627564713359\n",
      "#319 Loss: 0.0028545521199703217\n",
      "#320 Loss: 0.0028534773737192154\n",
      "#321 Loss: 0.0028523963410407305\n",
      "#322 Loss: 0.002851324388757348\n",
      "#323 Loss: 0.002850243588909507\n",
      "#324 Loss: 0.0028491623234003782\n",
      "#325 Loss: 0.002848081523552537\n",
      "#326 Loss: 0.0028469953685998917\n",
      "#327 Loss: 0.002845920855179429\n",
      "#328 Loss: 0.0028448335360735655\n",
      "#329 Loss: 0.002843747613951564\n",
      "#330 Loss: 0.0028426656499505043\n",
      "#331 Loss: 0.0028415813576430082\n",
      "#332 Loss: 0.002840495901182294\n",
      "#333 Loss: 0.0028394076507538557\n",
      "#334 Loss: 0.0028383247554302216\n",
      "#335 Loss: 0.0028372344095259905\n",
      "#336 Loss: 0.0028361480217427015\n",
      "#337 Loss: 0.0028350604698061943\n",
      "#338 Loss: 0.002833969658240676\n",
      "#339 Loss: 0.002832879312336445\n",
      "#340 Loss: 0.0028317857068032026\n",
      "#341 Loss: 0.002830692334100604\n",
      "#342 Loss: 0.0028295989613980055\n",
      "#343 Loss: 0.0028285107109695673\n",
      "#344 Loss: 0.0028274133801460266\n",
      "#345 Loss: 0.0028263202402740717\n",
      "#346 Loss: 0.0028252238407731056\n",
      "#347 Loss: 0.002824121154844761\n",
      "#348 Loss: 0.002823028014972806\n",
      "#349 Loss: 0.002821926726028323\n",
      "#350 Loss: 0.002820829162374139\n",
      "#351 Loss: 0.0028197302017360926\n",
      "#352 Loss: 0.0028186326380819082\n",
      "#353 Loss: 0.002817530417814851\n",
      "#354 Loss: 0.002816423075273633\n",
      "#355 Loss: 0.0028153269086033106\n",
      "#356 Loss: 0.0028142218943685293\n",
      "#357 Loss: 0.002813115483149886\n",
      "#358 Loss: 0.0028120093047618866\n",
      "#359 Loss: 0.0028109047561883926\n",
      "#360 Loss: 0.0028098004404455423\n",
      "#361 Loss: 0.0028086937963962555\n",
      "#362 Loss: 0.0028075820300728083\n",
      "#363 Loss: 0.002806475618854165\n",
      "#364 Loss: 0.0028053659480065107\n",
      "#365 Loss: 0.002804261865094304\n",
      "#366 Loss: 0.0028031517285853624\n",
      "#367 Loss: 0.0028020385652780533\n",
      "#368 Loss: 0.002800925401970744\n",
      "#369 Loss: 0.002799810143187642\n",
      "#370 Loss: 0.002798696979880333\n",
      "#371 Loss: 0.002797584282234311\n",
      "#372 Loss: 0.0027964673936367035\n",
      "#373 Loss: 0.0027953526005148888\n",
      "#374 Loss: 0.002794235246255994\n",
      "#375 Loss: 0.0027931195218116045\n",
      "#376 Loss: 0.002792000537738204\n",
      "#377 Loss: 0.00279088388197124\n",
      "#378 Loss: 0.0027897648978978395\n",
      "#379 Loss: 0.00278864405117929\n",
      "#380 Loss: 0.002787519246339798\n",
      "#381 Loss: 0.0027864014264196157\n",
      "#382 Loss: 0.0027852754574269056\n",
      "#383 Loss: 0.002784150652587414\n",
      "#384 Loss: 0.002783033298328519\n",
      "#385 Loss: 0.0027819043025374413\n",
      "#386 Loss: 0.002780781826004386\n",
      "#387 Loss: 0.0027796567883342505\n",
      "#388 Loss: 0.0027785308193415403\n",
      "#389 Loss: 0.0027774020563811064\n",
      "#390 Loss: 0.0027762732934206724\n",
      "#391 Loss: 0.0027751466259360313\n",
      "#392 Loss: 0.0027740143705159426\n",
      "#393 Loss: 0.0027728863060474396\n",
      "#394 Loss: 0.0027717549819499254\n",
      "#395 Loss: 0.002770621096715331\n",
      "#396 Loss: 0.0027694886084645987\n",
      "#397 Loss: 0.002768358215689659\n",
      "#398 Loss: 0.002767225494608283\n",
      "#399 Loss: 0.0027660869527608156\n",
      "#400 Loss: 0.0027649514377117157\n",
      "#401 Loss: 0.002763819880783558\n",
      "#402 Loss: 0.0027626834344118834\n",
      "#403 Loss: 0.002761542098596692\n",
      "#404 Loss: 0.0027604035567492247\n",
      "#405 Loss: 0.0027592647820711136\n",
      "#406 Loss: 0.00275812647305429\n",
      "#407 Loss: 0.002756985602900386\n",
      "#408 Loss: 0.002755846129730344\n",
      "#409 Loss: 0.002754700370132923\n",
      "#410 Loss: 0.002753564389422536\n",
      "#411 Loss: 0.00275241956114769\n",
      "#412 Loss: 0.002751278458163142\n",
      "#413 Loss: 0.0027501333970576525\n",
      "#414 Loss: 0.0027489911299198866\n",
      "#415 Loss: 0.0027478386182338\n",
      "#416 Loss: 0.002746694954112172\n",
      "#417 Loss: 0.0027455491945147514\n",
      "#418 Loss: 0.0027443980798125267\n",
      "#419 Loss: 0.0027432541828602552\n",
      "#420 Loss: 0.002742103533819318\n",
      "#421 Loss: 0.002740953117609024\n",
      "#422 Loss: 0.0027397971134632826\n",
      "#423 Loss: 0.0027386473957449198\n",
      "#424 Loss: 0.002737500471994281\n",
      "#425 Loss: 0.002736353315412998\n",
      "#426 Loss: 0.0027351966127753258\n",
      "#427 Loss: 0.0027340457309037447\n",
      "#428 Loss: 0.002732887864112854\n",
      "#429 Loss: 0.002731733024120331\n",
      "#430 Loss: 0.0027305760886520147\n",
      "#431 Loss: 0.002729421481490135\n",
      "#432 Loss: 0.002728261984884739\n",
      "#433 Loss: 0.002727105049416423\n",
      "#434 Loss: 0.00272594322450459\n",
      "#435 Loss: 0.002724789083003998\n",
      "#436 Loss: 0.0027236274909228086\n",
      "#437 Loss: 0.002722466131672263\n",
      "#438 Loss: 0.0027213043067604303\n",
      "#439 Loss: 0.0027201417833566666\n",
      "#440 Loss: 0.0027189813554286957\n",
      "#441 Loss: 0.002717820694670081\n",
      "#442 Loss: 0.0027166528161615133\n",
      "#443 Loss: 0.0027154877316206694\n",
      "#444 Loss: 0.002714321715757251\n",
      "#445 Loss: 0.002713157096877694\n",
      "#446 Loss: 0.0027119945734739304\n",
      "#447 Loss: 0.0027108322829008102\n",
      "#448 Loss: 0.002709662541747093\n",
      "#449 Loss: 0.002708492800593376\n",
      "#450 Loss: 0.002707322360947728\n",
      "#451 Loss: 0.0027061530854552984\n",
      "#452 Loss: 0.0027049866039305925\n",
      "#453 Loss: 0.002703814534470439\n",
      "#454 Loss: 0.0027026410680264235\n",
      "#455 Loss: 0.002701466903090477\n",
      "#456 Loss: 0.002700292505323887\n",
      "#457 Loss: 0.00269912532530725\n",
      "#458 Loss: 0.002697947435081005\n",
      "#459 Loss: 0.002696774899959564\n",
      "#460 Loss: 0.0026955970097333193\n",
      "#461 Loss: 0.002694426802918315\n",
      "#462 Loss: 0.002693249611184001\n",
      "#463 Loss: 0.0026920705568045378\n",
      "#464 Loss: 0.002690892666578293\n",
      "#465 Loss: 0.0026897117495536804\n",
      "#466 Loss: 0.0026885352563112974\n",
      "#467 Loss: 0.0026873562019318342\n",
      "#468 Loss: 0.0026861794758588076\n",
      "#469 Loss: 0.002684996696189046\n",
      "#470 Loss: 0.0026838157791644335\n",
      "#471 Loss: 0.002682632999494672\n",
      "#472 Loss: 0.0026814518496394157\n",
      "#473 Loss: 0.0026802706997841597\n",
      "#474 Loss: 0.0026790883857756853\n",
      "#475 Loss: 0.0026779004838317633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#476 Loss: 0.0026767172385007143\n",
      "#477 Loss: 0.0026755270082503557\n",
      "#478 Loss: 0.002674344927072525\n",
      "#479 Loss: 0.0026731558609753847\n",
      "#480 Loss: 0.0026719728484749794\n",
      "#481 Loss: 0.0026707814540714025\n",
      "#482 Loss: 0.002669595880433917\n",
      "#483 Loss: 0.0026684061158448458\n",
      "#484 Loss: 0.0026672144886106253\n",
      "#485 Loss: 0.002666029380634427\n",
      "#486 Loss: 0.002664836822077632\n",
      "#487 Loss: 0.0026636403053998947\n",
      "#488 Loss: 0.002662449376657605\n",
      "#489 Loss: 0.002661255421116948\n",
      "#490 Loss: 0.0026600633282214403\n",
      "#491 Loss: 0.0026588747277855873\n",
      "#492 Loss: 0.0026576800737529993\n",
      "#493 Loss: 0.0026564865838736296\n",
      "#494 Loss: 0.0026552907656878233\n",
      "#495 Loss: 0.0026540961116552353\n",
      "#496 Loss: 0.0026528991293162107\n",
      "#497 Loss: 0.002651705639436841\n",
      "#498 Loss: 0.0026505060959607363\n",
      "#499 Loss: 0.002649306319653988\n",
      "#500 Loss: 0.0026481058448553085\n",
      "#501 Loss: 0.002646910259500146\n",
      "#502 Loss: 0.0026457123458385468\n",
      "#503 Loss: 0.002644512103870511\n",
      "#504 Loss: 0.0026433076709508896\n",
      "#505 Loss: 0.0026421097572892904\n",
      "#506 Loss: 0.0026409064885228872\n",
      "#507 Loss: 0.0026397062465548515\n",
      "#508 Loss: 0.0026385022792965174\n",
      "#509 Loss: 0.0026372962165623903\n",
      "#510 Loss: 0.002636092947795987\n",
      "#511 Loss: 0.002634892240166664\n",
      "#512 Loss: 0.0026336880400776863\n",
      "#513 Loss: 0.0026324826758354902\n",
      "#514 Loss: 0.002631276147440076\n",
      "#515 Loss: 0.002630070550367236\n",
      "#516 Loss: 0.0026288647204637527\n",
      "#517 Loss: 0.002627655165269971\n",
      "#518 Loss: 0.0026264467742294073\n",
      "#519 Loss: 0.002625240245833993\n",
      "#520 Loss: 0.002624029293656349\n",
      "#521 Loss: 0.0026228167116642\n",
      "#522 Loss: 0.00262160855345428\n",
      "#523 Loss: 0.002620399696752429\n",
      "#524 Loss: 0.0026191917713731527\n",
      "#525 Loss: 0.0026179756969213486\n",
      "#526 Loss: 0.0026167610194534063\n",
      "#527 Loss: 0.002615552395582199\n",
      "#528 Loss: 0.0026143391150981188\n",
      "#529 Loss: 0.0026131283957511187\n",
      "#530 Loss: 0.0026119130197912455\n",
      "#531 Loss: 0.0026107013691216707\n",
      "#532 Loss: 0.0026094838976860046\n",
      "#533 Loss: 0.002608271548524499\n",
      "#534 Loss: 0.002607058035209775\n",
      "#535 Loss: 0.0026058368384838104\n",
      "#536 Loss: 0.0026046205312013626\n",
      "#537 Loss: 0.0026034004986286163\n",
      "#538 Loss: 0.0026021848898380995\n",
      "#539 Loss: 0.0026009727735072374\n",
      "#540 Loss: 0.002599752740934491\n",
      "#541 Loss: 0.002598529914394021\n",
      "#542 Loss: 0.0025973089504987\n",
      "#543 Loss: 0.002596090314909816\n",
      "#544 Loss: 0.002594874007627368\n",
      "#545 Loss: 0.0025936507154256105\n",
      "#546 Loss: 0.002592428121715784\n",
      "#547 Loss: 0.002591207390651107\n",
      "#548 Loss: 0.002589983167126775\n",
      "#549 Loss: 0.002588763600215316\n",
      "#550 Loss: 0.0025875407736748457\n",
      "#551 Loss: 0.0025863167829811573\n",
      "#552 Loss: 0.0025850916281342506\n",
      "#553 Loss: 0.0025838706642389297\n",
      "#554 Loss: 0.0025826452765613794\n",
      "#555 Loss: 0.0025814208202064037\n",
      "#556 Loss: 0.0025801940355449915\n",
      "#557 Loss: 0.0025789684150367975\n",
      "#558 Loss: 0.002577744424343109\n",
      "#559 Loss: 0.002576514845713973\n",
      "#560 Loss: 0.0025752857327461243\n",
      "#561 Loss: 0.002574061742052436\n",
      "#562 Loss: 0.002572834026068449\n",
      "#563 Loss: 0.00257160235196352\n",
      "#564 Loss: 0.00257036997936666\n",
      "#565 Loss: 0.0025691408663988113\n",
      "#566 Loss: 0.0025679143145680428\n",
      "#567 Loss: 0.0025666847359389067\n",
      "#568 Loss: 0.0025654551573097706\n",
      "#569 Loss: 0.002564219990745187\n",
      "#570 Loss: 0.0025629920419305563\n",
      "#571 Loss: 0.0025617594365030527\n",
      "#572 Loss: 0.0025605249684304\n",
      "#573 Loss: 0.0025592935271561146\n",
      "#574 Loss: 0.002558064414188266\n",
      "#575 Loss: 0.0025568308774381876\n",
      "#576 Loss: 0.002555598272010684\n",
      "#577 Loss: 0.002554362639784813\n",
      "#578 Loss: 0.002553130965679884\n",
      "#579 Loss: 0.002551896031945944\n",
      "#580 Loss: 0.0025506566744297743\n",
      "#581 Loss: 0.0025494247674942017\n",
      "#582 Loss: 0.002548186806961894\n",
      "#583 Loss: 0.0025469546671956778\n",
      "#584 Loss: 0.00254571414552629\n",
      "#585 Loss: 0.0025444799102842808\n",
      "#586 Loss: 0.002543241949751973\n",
      "#587 Loss: 0.0025420028250664473\n",
      "#588 Loss: 0.002540766028687358\n",
      "#589 Loss: 0.002539528300985694\n",
      "#590 Loss: 0.002538292668759823\n",
      "#591 Loss: 0.002537052147090435\n",
      "#592 Loss: 0.002535811625421047\n",
      "#593 Loss: 0.002534571336582303\n",
      "#594 Loss: 0.0025333298835903406\n",
      "#595 Loss: 0.0025320968125015497\n",
      "#596 Loss: 0.0025308511685580015\n",
      "#597 Loss: 0.002529611112549901\n",
      "#598 Loss: 0.0025283657014369965\n",
      "#599 Loss: 0.002527124946936965\n",
      "#600 Loss: 0.0025258890818804502\n",
      "#601 Loss: 0.0025246439035981894\n",
      "#602 Loss: 0.0025234005879610777\n",
      "#603 Loss: 0.0025221577379852533\n",
      "#604 Loss: 0.002520916284993291\n",
      "#605 Loss: 0.0025196727365255356\n",
      "#606 Loss: 0.002518427325412631\n",
      "#607 Loss: 0.002517188200727105\n",
      "#608 Loss: 0.0025159395299851894\n",
      "#609 Loss: 0.0025146929547190666\n",
      "#610 Loss: 0.0025134491734206676\n",
      "#611 Loss: 0.0025122067891061306\n",
      "#612 Loss: 0.002510959515348077\n",
      "#613 Loss: 0.0025097152683883905\n",
      "#614 Loss: 0.002508467296138406\n",
      "#615 Loss: 0.002507224213331938\n",
      "#616 Loss: 0.0025059778708964586\n",
      "#617 Loss: 0.0025047294329851866\n",
      "#618 Loss: 0.0025034851860255003\n",
      "#619 Loss: 0.00250223814509809\n",
      "#620 Loss: 0.0025009927339851856\n",
      "#621 Loss: 0.0024997417349368334\n",
      "#622 Loss: 0.002498490735888481\n",
      "#623 Loss: 0.0024972413666546345\n",
      "#624 Loss: 0.0024959987495094538\n",
      "#625 Loss: 0.002494748681783676\n",
      "#626 Loss: 0.0024934951215982437\n",
      "#627 Loss: 0.0024922459851950407\n",
      "#628 Loss: 0.0024910024367272854\n",
      "#629 Loss: 0.0024897458497434855\n",
      "#630 Loss: 0.002488495083525777\n",
      "#631 Loss: 0.0024872501380741596\n",
      "#632 Loss: 0.0024859951809048653\n",
      "#633 Loss: 0.0024847458116710186\n",
      "#634 Loss: 0.002483497839421034\n",
      "#635 Loss: 0.0024822461418807507\n",
      "#636 Loss: 0.002480993280187249\n",
      "#637 Loss: 0.0024797413498163223\n",
      "#638 Loss: 0.0024784861598163843\n",
      "#639 Loss: 0.0024772349279373884\n",
      "#640 Loss: 0.0024759869556874037\n",
      "#641 Loss: 0.0024747333955019712\n",
      "#642 Loss: 0.0024734779726713896\n",
      "#643 Loss: 0.0024722295347601175\n",
      "#644 Loss: 0.0024709722492843866\n",
      "#645 Loss: 0.0024697205517441034\n",
      "#646 Loss: 0.0024684665258973837\n",
      "#647 Loss: 0.0024672129657119513\n",
      "#648 Loss: 0.002465961268171668\n",
      "#649 Loss: 0.00246470351703465\n",
      "#650 Loss: 0.002463450888171792\n",
      "#651 Loss: 0.0024621945340186357\n",
      "#652 Loss: 0.002460941905155778\n",
      "#653 Loss: 0.0024596902076154947\n",
      "#654 Loss: 0.0024584324564784765\n",
      "#655 Loss: 0.0024571754038333893\n",
      "#656 Loss: 0.0024559220764786005\n",
      "#657 Loss: 0.002454663859680295\n",
      "#658 Loss: 0.0024534135591238737\n",
      "#659 Loss: 0.0024521530140191317\n",
      "#660 Loss: 0.0024508957285434008\n",
      "#661 Loss: 0.002449639840051532\n",
      "#662 Loss: 0.0024483876768499613\n",
      "#663 Loss: 0.0024471294600516558\n",
      "#664 Loss: 0.002445873571559787\n",
      "#665 Loss: 0.002444617450237274\n",
      "#666 Loss: 0.0024433571379631758\n",
      "#667 Loss: 0.002442101715132594\n",
      "#668 Loss: 0.0024408476892858744\n",
      "#669 Loss: 0.002439588075503707\n",
      "#670 Loss: 0.0024383272975683212\n",
      "#671 Loss: 0.002437071641907096\n",
      "#672 Loss: 0.002435817616060376\n",
      "#673 Loss: 0.0024345580022782087\n",
      "#674 Loss: 0.0024333023466169834\n",
      "#675 Loss: 0.002432039938867092\n",
      "#676 Loss: 0.0024307845160365105\n",
      "#677 Loss: 0.00242952280677855\n",
      "#678 Loss: 0.002428267849609256\n",
      "#679 Loss: 0.0024270075373351574\n",
      "#680 Loss: 0.0024257509503513575\n",
      "#681 Loss: 0.0024244908709079027\n",
      "#682 Loss: 0.002423232654109597\n",
      "#683 Loss: 0.002421972109004855\n",
      "#684 Loss: 0.0024207152891904116\n",
      "#685 Loss: 0.002419453812763095\n",
      "#686 Loss: 0.002418196527287364\n",
      "#687 Loss: 0.002416933886706829\n",
      "#688 Loss: 0.0024156770668923855\n",
      "#689 Loss: 0.002414418151602149\n",
      "#690 Loss: 0.002413156908005476\n",
      "#691 Loss: 0.0024118993896991014\n",
      "#692 Loss: 0.002410640474408865\n",
      "#693 Loss: 0.0024093815591186285\n",
      "#694 Loss: 0.0024081191513687372\n",
      "#695 Loss: 0.002406858839094639\n",
      "#696 Loss: 0.0024056017864495516\n",
      "#697 Loss: 0.0024043426383286715\n",
      "#698 Loss: 0.00240308023057878\n",
      "#699 Loss: 0.0024018210824579\n",
      "#700 Loss: 0.0024005600716918707\n",
      "#701 Loss: 0.0023992995265871286\n",
      "#702 Loss: 0.002398039447143674\n",
      "#703 Loss: 0.0023967816960066557\n",
      "#704 Loss: 0.0023955178912729025\n",
      "#705 Loss: 0.002394262934103608\n",
      "#706 Loss: 0.0023929974995553493\n",
      "#707 Loss: 0.0023917383514344692\n",
      "#708 Loss: 0.002390475943684578\n",
      "#709 Loss: 0.00238922075368464\n",
      "#710 Loss: 0.002387960674241185\n",
      "#711 Loss: 0.002386694774031639\n",
      "#712 Loss: 0.0023854358587414026\n",
      "#713 Loss: 0.0023841799702495337\n",
      "#714 Loss: 0.002382915699854493\n",
      "#715 Loss: 0.002381657948717475\n",
      "#716 Loss: 0.002380398102104664\n",
      "#717 Loss: 0.002379137324169278\n",
      "#718 Loss: 0.002377873519435525\n",
      "#719 Loss: 0.0023766132071614265\n",
      "#720 Loss: 0.0023753561545163393\n",
      "#721 Loss: 0.0023740939795970917\n",
      "#722 Loss: 0.002372835762798786\n",
      "#723 Loss: 0.0023715749848634005\n",
      "#724 Loss: 0.002370312577113509\n",
      "#725 Loss: 0.002369053428992629\n",
      "#726 Loss: 0.002367792185395956\n",
      "#727 Loss: 0.0023665311746299267\n",
      "#728 Loss: 0.002365273190662265\n",
      "#729 Loss: 0.002364011248573661\n",
      "#730 Loss: 0.0023627502378076315\n",
      "#731 Loss: 0.002361491322517395\n",
      "#732 Loss: 0.0023602310102432966\n",
      "#733 Loss: 0.0023589737247675657\n",
      "#734 Loss: 0.0023577127140015364\n",
      "#735 Loss: 0.0023564521688967943\n",
      "#736 Loss: 0.002355191158130765\n",
      "#737 Loss: 0.002353931777179241\n",
      "#738 Loss: 0.00235266936942935\n",
      "#739 Loss: 0.00235141278244555\n",
      "#740 Loss: 0.002350150840356946\n",
      "#741 Loss: 0.002348893089219928\n",
      "#742 Loss: 0.002347635803744197\n",
      "#743 Loss: 0.0023463687393814325\n",
      "#744 Loss: 0.0023451123852282763\n",
      "#745 Loss: 0.0023438576608896255\n",
      "#746 Loss: 0.0023425950203090906\n",
      "#747 Loss: 0.002341336337849498\n",
      "#748 Loss: 0.002340079518035054\n",
      "#749 Loss: 0.0023388222325593233\n",
      "#750 Loss: 0.0023375595919787884\n",
      "#751 Loss: 0.002336302539333701\n",
      "#752 Loss: 0.0023350429255515337\n",
      "#753 Loss: 0.002333782846108079\n",
      "#754 Loss: 0.0023325246293097734\n",
      "#755 Loss: 0.0023312659468501806\n",
      "#756 Loss: 0.0023300109896808863\n",
      "#757 Loss: 0.0023287504445761442\n",
      "#758 Loss: 0.0023274924606084824\n",
      "#759 Loss: 0.0023262365721166134\n",
      "#760 Loss: 0.002324973465874791\n",
      "#761 Loss: 0.0023237226996570826\n",
      "#762 Loss: 0.0023224621545523405\n",
      "#763 Loss: 0.0023212034720927477\n",
      "#764 Loss: 0.0023199457209557295\n",
      "#765 Loss: 0.002318690065294504\n",
      "#766 Loss: 0.0023174320813268423\n",
      "#767 Loss: 0.0023161740973591805\n",
      "#768 Loss: 0.0023149200715124607\n",
      "#769 Loss: 0.002313664648681879\n",
      "#770 Loss: 0.002312406664714217\n",
      "#771 Loss: 0.0023111498448997736\n",
      "#772 Loss: 0.002309896517544985\n",
      "#773 Loss: 0.002308635273948312\n",
      "#774 Loss: 0.0023073817137628794\n",
      "#775 Loss: 0.002306124893948436\n",
      "#776 Loss: 0.002304874127730727\n",
      "#777 Loss: 0.002303613582625985\n",
      "#778 Loss: 0.0023023600224405527\n",
      "#779 Loss: 0.002301107859238982\n",
      "#780 Loss: 0.002299848711118102\n",
      "#781 Loss: 0.0022985946852713823\n",
      "#782 Loss: 0.0022973373997956514\n",
      "#783 Loss: 0.0022960829082876444\n",
      "#784 Loss: 0.0022948356345295906\n",
      "#785 Loss: 0.002293577417731285\n",
      "#786 Loss: 0.00229232432320714\n",
      "#787 Loss: 0.00229106517508626\n",
      "#788 Loss: 0.002289817202836275\n",
      "#789 Loss: 0.0022885652724653482\n",
      "#790 Loss: 0.002287309616804123\n",
      "#791 Loss: 0.002286055823788047\n",
      "#792 Loss: 0.0022848069202154875\n",
      "#793 Loss: 0.002283554757013917\n",
      "#794 Loss: 0.002282301662489772\n",
      "#795 Loss: 0.0022810508962720633\n",
      "#796 Loss: 0.0022797940764576197\n",
      "#797 Loss: 0.002278547501191497\n",
      "#798 Loss: 0.0022772972006350756\n",
      "#799 Loss: 0.0022760413121432066\n",
      "#800 Loss: 0.0022747910115867853\n",
      "#801 Loss: 0.002273544669151306\n",
      "#802 Loss: 0.0022722892463207245\n",
      "#803 Loss: 0.0022710440680384636\n",
      "#804 Loss: 0.0022697956301271915\n",
      "#805 Loss: 0.002268544863909483\n",
      "#806 Loss: 0.002267288975417614\n",
      "#807 Loss: 0.0022660440299659967\n",
      "#808 Loss: 0.002264791401103139\n",
      "#809 Loss: 0.0022635450586676598\n",
      "#810 Loss: 0.0022623019758611917\n",
      "#811 Loss: 0.002261047950014472\n",
      "#812 Loss: 0.0022597995121032\n",
      "#813 Loss: 0.00225855503231287\n",
      "#814 Loss: 0.002257309155538678\n",
      "#815 Loss: 0.0022560590878129005\n",
      "#816 Loss: 0.0022548113483935595\n",
      "#817 Loss: 0.002253563841804862\n",
      "#818 Loss: 0.0022523198276758194\n",
      "#819 Loss: 0.00225107092410326\n",
      "#820 Loss: 0.0022498287726193666\n",
      "#821 Loss: 0.002248582663014531\n",
      "#822 Loss: 0.0022473365534096956\n",
      "#823 Loss: 0.0022460906766355038\n",
      "#824 Loss: 0.0022448443342000246\n",
      "#825 Loss: 0.002243602415546775\n",
      "#826 Loss: 0.0022423602640628815\n",
      "#827 Loss: 0.0022411132231354713\n",
      "#828 Loss: 0.0022398694418370724\n",
      "#829 Loss: 0.0022386277560144663\n",
      "#830 Loss: 0.002237382112070918\n",
      "#831 Loss: 0.0022361392620950937\n",
      "#832 Loss: 0.0022348985075950623\n",
      "#833 Loss: 0.0022336572874337435\n",
      "#834 Loss: 0.00223241257481277\n",
      "#835 Loss: 0.0022311753127723932\n",
      "#836 Loss: 0.002229932928457856\n",
      "#837 Loss: 0.0022286931052803993\n",
      "#838 Loss: 0.0022274525836110115\n",
      "#839 Loss: 0.00222621182911098\n",
      "#840 Loss: 0.0022249750327318907\n",
      "#841 Loss: 0.0022237328812479973\n",
      "#842 Loss: 0.002222490031272173\n",
      "#843 Loss: 0.002221254166215658\n",
      "#844 Loss: 0.002220016671344638\n",
      "#845 Loss: 0.0022187780123203993\n",
      "#846 Loss: 0.0022175421472638845\n",
      "#847 Loss: 0.0022163025569170713\n",
      "#848 Loss: 0.0022150676231831312\n",
      "#849 Loss: 0.0022138271015137434\n",
      "#850 Loss: 0.002212596358731389\n",
      "#851 Loss: 0.0022113583981990814\n",
      "#852 Loss: 0.0022101199720054865\n",
      "#853 Loss: 0.002208885969594121\n",
      "#854 Loss: 0.0022076501045376062\n",
      "#855 Loss: 0.0022064161021262407\n",
      "#856 Loss: 0.0022051844280213118\n",
      "#857 Loss: 0.002203947864472866\n",
      "#858 Loss: 0.0022027159575372934\n",
      "#859 Loss: 0.0022014810238033533\n",
      "#860 Loss: 0.0022002507466822863\n",
      "#861 Loss: 0.002199014415964484\n",
      "#862 Loss: 0.002197781577706337\n",
      "#863 Loss: 0.0021965522319078445\n",
      "#864 Loss: 0.0021953219547867775\n",
      "#865 Loss: 0.002194088650867343\n",
      "#866 Loss: 0.0021928586065769196\n",
      "#867 Loss: 0.0021916290279477835\n",
      "#868 Loss: 0.002190402941778302\n",
      "#869 Loss: 0.002189172198995948\n",
      "#870 Loss: 0.002187945181503892\n",
      "#871 Loss: 0.0021867130417376757\n",
      "#872 Loss: 0.002185484627261758\n",
      "#873 Loss: 0.0021842592395842075\n",
      "#874 Loss: 0.002183031989261508\n",
      "#875 Loss: 0.0021818059030920267\n",
      "#876 Loss: 0.0021805763244628906\n",
      "#877 Loss: 0.002179353730753064\n",
      "#878 Loss: 0.0021781239192932844\n",
      "#879 Loss: 0.0021769015584141016\n",
      "#880 Loss: 0.002175676403567195\n",
      "#881 Loss: 0.0021744512487202883\n",
      "#882 Loss: 0.0021732270251959562\n",
      "#883 Loss: 0.0021720051299780607\n",
      "#884 Loss: 0.0021707837004214525\n",
      "#885 Loss: 0.002169561106711626\n",
      "#886 Loss: 0.002168339677155018\n",
      "#887 Loss: 0.002167114056646824\n",
      "#888 Loss: 0.002165893791243434\n",
      "#889 Loss: 0.0021646746899932623\n",
      "#890 Loss: 0.002163451863452792\n",
      "#891 Loss: 0.0021622339263558388\n",
      "#892 Loss: 0.0021610104013234377\n",
      "#893 Loss: 0.0021597922313958406\n",
      "#894 Loss: 0.0021585740614682436\n",
      "#895 Loss: 0.0021573547273874283\n",
      "#896 Loss: 0.0021561391185969114\n",
      "#897 Loss: 0.0021549228113144636\n",
      "#898 Loss: 0.002153706969693303\n",
      "#899 Loss: 0.002152484143152833\n",
      "#900 Loss: 0.0021512736566364765\n",
      "#901 Loss: 0.0021500596776604652\n",
      "#902 Loss: 0.002148841740563512\n",
      "#903 Loss: 0.0021476270630955696\n",
      "#904 Loss: 0.0021464137826114893\n",
      "#905 Loss: 0.002145199105143547\n",
      "#906 Loss: 0.00214399048127234\n",
      "#907 Loss: 0.0021427746396511793\n",
      "#908 Loss: 0.002141561359167099\n",
      "#909 Loss: 0.0021403534337878227\n",
      "#910 Loss: 0.0021391380578279495\n",
      "#911 Loss: 0.002137927571311593\n",
      "#912 Loss: 0.0021367173176258802\n",
      "#913 Loss: 0.002135507995262742\n",
      "#914 Loss: 0.002134300535544753\n",
      "#915 Loss: 0.0021330888848751783\n",
      "#916 Loss: 0.0021318865474313498\n",
      "#917 Loss: 0.002130672335624695\n",
      "#918 Loss: 0.0021294692996889353\n",
      "#919 Loss: 0.002128262771293521\n",
      "#920 Loss: 0.0021270543802529573\n",
      "#921 Loss: 0.002125851809978485\n",
      "#922 Loss: 0.002124642953276634\n",
      "#923 Loss: 0.002123442245647311\n",
      "#924 Loss: 0.002122236415743828\n",
      "#925 Loss: 0.0021210312843322754\n",
      "#926 Loss: 0.0021198310423642397\n",
      "#927 Loss: 0.002118626143783331\n",
      "#928 Loss: 0.00211742683313787\n",
      "#929 Loss: 0.002116224728524685\n",
      "#930 Loss: 0.002115024020895362\n",
      "#931 Loss: 0.0021138209849596024\n",
      "#932 Loss: 0.002112623071298003\n",
      "#933 Loss: 0.002111425856128335\n",
      "#934 Loss: 0.002110227243974805\n",
      "#935 Loss: 0.0021090249065309763\n",
      "#936 Loss: 0.0021078300196677446\n",
      "#937 Loss: 0.0021066281478852034\n",
      "#938 Loss: 0.00210543442517519\n",
      "#939 Loss: 0.00210423837415874\n",
      "#940 Loss: 0.002103045117110014\n",
      "#941 Loss: 0.002101848367601633\n",
      "#942 Loss: 0.0021006513852626085\n",
      "#943 Loss: 0.0020994592923671007\n",
      "#944 Loss: 0.002098262310028076\n",
      "#945 Loss: 0.002097073011100292\n",
      "#946 Loss: 0.002095881151035428\n",
      "#947 Loss: 0.002094685798510909\n",
      "#948 Loss: 0.0020934988278895617\n",
      "#949 Loss: 0.0020923062693327665\n",
      "#950 Loss: 0.0020911165047436953\n",
      "#951 Loss: 0.0020899290684610605\n",
      "#952 Loss: 0.00208873744122684\n",
      "#953 Loss: 0.002087552333250642\n",
      "#954 Loss: 0.002086358843371272\n",
      "#955 Loss: 0.002085175598040223\n",
      "#956 Loss: 0.0020839888602495193\n",
      "#957 Loss: 0.002082802588120103\n",
      "#958 Loss: 0.002081613289192319\n",
      "#959 Loss: 0.0020804337691515684\n",
      "#960 Loss: 0.0020792472641915083\n",
      "#961 Loss: 0.0020780637860298157\n",
      "#962 Loss: 0.002076879609376192\n",
      "#963 Loss: 0.0020756989251822233\n",
      "#964 Loss: 0.002074514515697956\n",
      "#965 Loss: 0.0020733338315039873\n",
      "#966 Loss: 0.002072148723527789\n",
      "#967 Loss: 0.0020709719974547625\n",
      "#968 Loss: 0.0020697929430752993\n",
      "#969 Loss: 0.0020686148200184107\n",
      "#970 Loss: 0.0020674371626228094\n",
      "#971 Loss: 0.002066258108243346\n",
      "#972 Loss: 0.002065081615000963\n",
      "#973 Loss: 0.002063901163637638\n",
      "#974 Loss: 0.002062726067379117\n",
      "#975 Loss: 0.002061551669612527\n",
      "#976 Loss: 0.002060375874862075\n",
      "#977 Loss: 0.002059199148789048\n",
      "#978 Loss: 0.002058029407635331\n",
      "#979 Loss: 0.0020568573381751776\n",
      "#980 Loss: 0.002055683871731162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#981 Loss: 0.002054517390206456\n",
      "#982 Loss: 0.002053341595456004\n",
      "#983 Loss: 0.002052172552794218\n",
      "#984 Loss: 0.002050998853519559\n",
      "#985 Loss: 0.002049830975010991\n",
      "#986 Loss: 0.002048664027824998\n",
      "#987 Loss: 0.0020474963821470737\n",
      "#988 Loss: 0.002046327805146575\n",
      "#989 Loss: 0.0020451629534363747\n",
      "#990 Loss: 0.0020439934451133013\n",
      "#991 Loss: 0.002042829291895032\n",
      "#992 Loss: 0.0020416623447090387\n",
      "#993 Loss: 0.0020404972601681948\n",
      "#994 Loss: 0.0020393340382725\n",
      "#995 Loss: 0.002038170350715518\n",
      "#996 Loss: 0.0020370108541101217\n",
      "#997 Loss: 0.002035850891843438\n",
      "#998 Loss: 0.0020346862729638815\n",
      "#999 Loss: 0.002033526310697198\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "loss_lst = []\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    loss_lst.append(torch.mean((y - NN(X))**2).detach().item())\n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f583f17fda0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "pd.Series(loss_lst).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
