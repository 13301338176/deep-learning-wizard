{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced Learning Rate Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What types of hyperparameters are there?\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Weight decay\n",
    "- Dropout\n",
    "- And more!\n",
    "\n",
    "## Why the concentration on learning rate?\n",
    "- It's one of the most important hyperparameter that, if properly tuned gives 2 main benefits:\n",
    "    - Better generalization (higher validation and test accuracy)\n",
    "    - Faster convergence (less time spent on training)\n",
    "    \n",
    "## We've learnt 2 basic ways to optimize learning rate\n",
    "1. Step-wise Decay \n",
    "2. Reduce on Loss Plateau Decay\n",
    "\n",
    "## 2 new advanced learning rate optimization\n",
    "- SGD restarts with snapshots\n",
    "- SGD hypergradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SGD Restarts with warm restarts\n",
    "- **Problem**\n",
    "    - With a larger model capacity (more layers and more complex), more number of local minima with different generalization ability\n",
    "    - Taking a snapshot of the model at different local minima and combining them gives us an ensemble of models for free  \n",
    "- **Benefits**\n",
    "    - Warm restart in optimization allows us to improve generalization\n",
    "        - Allows us to escape bad local minima\n",
    "        - Allows us to explore wider loss surface\n",
    "- **How**\n",
    "    - Let the learning rate decay then restarting it to a high learning rate when it converges to a local minima\n",
    "        - Using cosine annealing to decay our learning rate\n",
    "        - $\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\\cos(\\frac{T_{cur}}{T_{max}}\\pi))$\n",
    "            - $\\eta_{min}$: Minimum learning rate\n",
    "            - $\\eta_{max}$: Maximum learning rate\n",
    "            - $T_{max}$: Maximum number of epochs\n",
    "            - $T_{cur}$: Current epoch\n",
    "    - Before each reset, snapshot (save) the model's parameters via a checkpoint\n",
    "    - When we've cycled through our snapshots, we can average the models' softmax outputs to obtain a final averaged categorical distribution (probability distribution over N possible outcomes)\n",
    "        - Given:\n",
    "            - $x$ be in the input\n",
    "            - $m$ is the last m model's softmax outputs\n",
    "            - $h_i (x)$ is the softmax output of snapshot $i$\n",
    "        - Then the ensemble's output is $h_{ensemble} = \\frac{1}{m} \\sum^{m-1}_0 h_{M - i} (x)$ which is simple the average of the last $m$ snapshots where we have to choose $m$ \n",
    "    - Instead of just choosing the last $m$ models, we can choose m best snapshots which in practical experience, gives better results than just the last $m$ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDR(_LRScheduler):\n",
    "    \"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
    "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
    "\n",
    "    When last_epoch=-1, sets initial lr as lr.    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        T_mult (float): Increase T_max by a factor of T_mult after every restart to improve performance. Default: 1.\n",
    "        model (Model): The model to save.\n",
    "        save_dir (str): Directory to save snapshots. Default: '/'.\n",
    "        save_model (bool): Saves the model after every restart. Default: True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, model, eta_min=0, last_epoch=-1, T_mult=1, save_dir='/', save_model=True):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.Te = self.T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.current_epoch = last_epoch\n",
    "        \n",
    "        self.model = model\n",
    "        self.save_dir = save_dir\n",
    "        self.take_snapshot = take_snapshot\n",
    "        \n",
    "        self.lr_history = []\n",
    "        \n",
    "        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    # Default function given by PyTorch\n",
    "    def get_lr(self):\n",
    "        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "        \n",
    "        # Append learning rates to tracker so we can print the behavior\n",
    "        self.lr_history.append(new_lrs)\n",
    "        return new_lrs\n",
    "    \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "        \n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        ## restart\n",
    "        if self.current_epoch == self.Te:\n",
    "            print(\"restart at epoch {:03d}\".format(self.last_epoch + 1))\n",
    "            \n",
    "            if self.take_snapshot:\n",
    "                torch.save({\n",
    "                    'epoch': self.T_max,\n",
    "                    'state_dict': self.model.state_dict()\n",
    "                }, self.save_dir + \"/\" + 'snapshot_e_{:03d}.pth.tar'.format(self.T_max))\n",
    "            \n",
    "            ## reset epochs since the last reset\n",
    "            self.current_epoch = 0\n",
    "            \n",
    "            ## reset the next goal\n",
    "            self.Te = int(self.Te * self.T_mult)\n",
    "            self.T_max = self.T_max + self.Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
