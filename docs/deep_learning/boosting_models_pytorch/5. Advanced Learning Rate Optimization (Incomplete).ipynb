{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Learning Rate Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What types of hyperparameters are there?\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Weight decay\n",
    "- Dropout\n",
    "- And more!\n",
    "\n",
    "## Why the concentration on learning rate?\n",
    "- It's one of the most important hyperparameter that, if properly tuned gives 2 main benefits:\n",
    "    - Better generalization (higher validation and test accuracy)\n",
    "    - Faster convergence (less time spent on training)\n",
    "    \n",
    "## We've learnt 2 basic ways to optimize learning rate\n",
    "1. Step-wise Decay \n",
    "2. Reduce on Loss Plateau Decay\n",
    "\n",
    "## 2 new advanced learning rate optimization\n",
    "- SGD restarts with snapshots\n",
    "    - This ultimately is heuristic-based\n",
    "- SGD hypergradient descent\n",
    "    - This is very diferrent from the basic ways and the advanced SGD restarts with snapshots where they all rely on heuristics\n",
    "    - It deploys a disciplined approach by using gradient descent to optimize both parameters and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SGD Restarts with warm restarts\n",
    "- **Problem**\n",
    "    - With a larger model capacity (more layers and more complex), more number of local minima with different generalization ability\n",
    "    - Taking a snapshot of the model at different local minima and combining them gives us an ensemble of models for free  \n",
    "- **Benefits**\n",
    "    - Warm restart in optimization allows us to improve generalization\n",
    "        - Allows us to escape bad local minima\n",
    "        - Allows us to explore wider loss surface\n",
    "- **How**\n",
    "    - Let the learning rate decay then restarting it to a high learning rate when it converges to a local minima\n",
    "        - Using cosine annealing to decay our learning rate\n",
    "        - $\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\\cos(\\frac{T_{cur}}{T_{max}}\\pi))$\n",
    "            - $\\eta_{min}$: Minimum learning rate\n",
    "            - $\\eta_{max}$: Maximum learning rate\n",
    "            - $T_{max}$: Maximum number of epochs\n",
    "            - $T_{cur}$: Current epoch\n",
    "    - Before each reset, snapshot (save) the model's parameters via a checkpoint\n",
    "    - When we've cycled through our snapshots, we can average the models' softmax outputs to obtain a final averaged categorical distribution (probability distribution over N possible outcomes)\n",
    "        - Given:\n",
    "            - $x$ be in the input\n",
    "            - $m$ is the last m model's softmax outputs\n",
    "            - $h_i (x)$ is the softmax output of snapshot $i$\n",
    "        - Then the ensemble's output is $h_{ensemble} = \\frac{1}{m} \\sum^{m-1}_0 h_{M - i} (x)$ which is simple the average of the last $m$ snapshots where we have to choose $m$ \n",
    "    - Instead of just choosing the last $m$ models, we can choose m best snapshots which in practical experience, gives better results than just the last $m$ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDR(_LRScheduler):\n",
    "    \"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
    "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
    "\n",
    "    When last_epoch=-1, sets initial lr as lr.    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "        T_mult (float): Increase T_max by a factor of T_mult after every restart to improve performance. Default: 1.\n",
    "        model (Model): The model to save.\n",
    "        save_dir (str): Directory to save snapshots. Default: '/'.\n",
    "        save_model (bool): Saves the model after every restart. Default: True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, model, eta_min=0, last_epoch=-1, T_mult=1, save_dir='/', save_model=True):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.Te = self.T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.current_epoch = last_epoch\n",
    "        \n",
    "        self.model = model\n",
    "        self.save_dir = save_dir\n",
    "        self.take_snapshot = take_snapshot\n",
    "        \n",
    "        self.lr_history = []\n",
    "        \n",
    "        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    # Default function given by PyTorch\n",
    "    def get_lr(self):\n",
    "        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "        \n",
    "        # Append learning rates to tracker so we can print the behavior\n",
    "        self.lr_history.append(new_lrs)\n",
    "        return new_lrs\n",
    "    \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "        \n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        ## restart\n",
    "        if self.current_epoch == self.Te:\n",
    "            print(\"restart at epoch {:03d}\".format(self.last_epoch + 1))\n",
    "            \n",
    "            if self.take_snapshot:\n",
    "                torch.save({\n",
    "                    'epoch': self.T_max,\n",
    "                    'state_dict': self.model.state_dict()\n",
    "                }, self.save_dir + \"/\" + 'snapshot_e_{:03d}.pth.tar'.format(self.T_max))\n",
    "            \n",
    "            ## reset epochs since the last reset\n",
    "            self.current_epoch = 0\n",
    "            \n",
    "            ## reset the next goal\n",
    "            self.Te = int(self.Te * self.T_mult)\n",
    "            self.T_max = self.T_max + self.Te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SGD Hypergradient Descent (SGDHD)\n",
    "\n",
    "### Why do we need SGDHD?\n",
    "- Typically Snapshots with SGD Restarts would give one of the best results already.\n",
    "- However, this technique is important because it leverages on the concept of using gradient descent on whatever we want to optimize\n",
    "    - Be it our parameters or hyperparameters, a non-heuristic approach is in using gradient descent to optimize for everything versus relying on heuristics like (1) step-wise learning rate decay, (3) reduce on plateau loss decay or (3) snapshots with SGD starts\n",
    "    \n",
    "### How it works in detail\n",
    "- Regular update rule: gradient of the lost function w.r.t. parameters\n",
    "    - We update our old parameters with the loss w.r.t. parameters\n",
    "    - $\\theta_t = \\theta_{t-1} - \\alpha \\nabla f (\\theta_{t-1})$\n",
    "        - Parameters $\\theta_{t-1}$\n",
    "        - Objective function $f$\n",
    "        - Gradient $\\nabla f (\\theta_{t-1})$\n",
    "        - Learning rate $\\alpha$\n",
    "- Additional update rule for learning rate: partial derivative of the lost function w.r.t. learning rate\n",
    "    - $\\alpha_t = \\alpha_{t-1} - \\beta \\frac {\\partial f(\\theta_{t-1})}{\\partial \\alpha}$\n",
    "        - Hyper learning rate $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import reduce\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "\n",
    "class SGDHD(Optimizer):\n",
    "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
    "    Nesterov momentum is based on the formula from\n",
    "    `On the importance of initialization and momentum in deep learning`__.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        momentum (float, optional): momentum factor (default: 0)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        dampening (float, optional): dampening for momentum (default: 0)\n",
    "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
    "        hypergrad_lr (float, optional): hypergradient learning rate for the online\n",
    "        tuning of the learning rate, introduced in the paper\n",
    "        `Online Learning Rate Adaptation with Hypergradient Descent`_\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
    "    .. _Online Learning Rate Adaptation with Hypergradient Descent:\n",
    "        https://openreview.net/forum?id=BkrsAzWAb\n",
    "    .. note::\n",
    "        The implementation of SGD with Momentum/Nesterov subtly differs from\n",
    "        Sutskever et. al. and implementations in some other frameworks.\n",
    "        Considering the specific case of Momentum, the update can be written as\n",
    "        .. math::\n",
    "                  v = \\rho * v + g \\\\\n",
    "                  p = p - lr * v\n",
    "        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n",
    "        velocity, and momentum respectively.\n",
    "        This is in contrast to Sutskever et. al. and\n",
    "        other frameworks which employ an update of the form\n",
    "        .. math::\n",
    "             v = \\rho * v + lr * g \\\\\n",
    "             p = p - v\n",
    "        The Nesterov version is analogously modified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, hypergrad_lr=1e-6):\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, hypergrad_lr=hypergrad_lr)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGDHD, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError(\"SGDHD doesn't support per-parameter options (parameter groups)\")\n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "        self._params_numel = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
    "\n",
    "    def _gather_flat_grad_with_weight_decay(self, weight_decay=0):\n",
    "        views = []\n",
    "        for p in self._params:\n",
    "            if p.grad is None:\n",
    "                view = torch.zeros_like(p.data)\n",
    "            elif p.grad.data.is_sparse:\n",
    "                view = p.grad.data.to_dense().view(-1)\n",
    "            else:\n",
    "                view = p.grad.data.view(-1)\n",
    "            if weight_decay != 0:\n",
    "                view.add_(weight_decay, p.data.view(-1))\n",
    "            views.append(view)\n",
    "        return torch.cat(views, 0)\n",
    "\n",
    "    def _add_grad(self, step_size, update):\n",
    "        offset = 0\n",
    "        for p in self._params:\n",
    "            numel = p.numel()\n",
    "            # view as to avoid deprecated pointwise semantics\n",
    "            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n",
    "            offset += numel\n",
    "        assert offset == self._params_numel\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        assert len(self.param_groups) == 1\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        weight_decay = group['weight_decay']\n",
    "        momentum = group['momentum']\n",
    "        dampening = group['dampening']\n",
    "        nesterov = group['nesterov']\n",
    "\n",
    "        grad = self._gather_flat_grad_with_weight_decay(weight_decay)\n",
    "\n",
    "        # NOTE: SGDHD has only global state, but we register it as state for\n",
    "        # the first param, because this helps with casting in load_state_dict\n",
    "        state = self.state[self._params[0]]\n",
    "        # State initialization\n",
    "        if len(state) == 0:\n",
    "            state['grad_prev'] = torch.zeros_like(grad)\n",
    "\n",
    "        grad_prev = state['grad_prev']\n",
    "        # Hypergradient for SGD\n",
    "        h = torch.dot(grad, grad_prev)\n",
    "        # Hypergradient descent of the learning rate:\n",
    "        group['lr'] += group['hypergrad_lr'] * h\n",
    "\n",
    "        if momentum != 0:\n",
    "            if 'momentum_buffer' not in state:\n",
    "                buf = state['momentum_buffer'] = torch.zeros_like(grad)\n",
    "                buf.mul_(momentum).add_(grad)\n",
    "            else:\n",
    "                buf = state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(1 - dampening, grad)\n",
    "            if nesterov:\n",
    "                grad.add_(momentum, buf)\n",
    "            else:\n",
    "                grad = buf\n",
    "\n",
    "        state['grad_prev'] = grad\n",
    "\n",
    "        self._add_grad(-group['lr'], grad)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
