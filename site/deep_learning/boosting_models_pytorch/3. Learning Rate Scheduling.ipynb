{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deeplearningwizard_web_logo.png\" alt=\"deeplearningwizard\" style=\"width: 100px;\"/>\n",
    "\n",
    "# 3. Learning Rate Scheduling\n",
    "\n",
    "## Optimization Algorithm 3: Mini-batch Gradient Descent (SGD)\n",
    "- Combination of batch gradient descent & stochastic gradient descent\n",
    "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
    "- Characteristics\n",
    "    - Compute the gradient of the lost function w.r.t. parameters for **n sets of training sample (n input and n label)**, $\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
    "    - Use this to update our parameters at every iteration\n",
    "    \n",
    "## Learning Intuition Recap\n",
    "- Learning process\n",
    "    - Original parameters $\\rightarrow$ given input, get output $\\rightarrow$ compare with labels $\\rightarrow$ get loss with comparison of input/output $\\rightarrow$ get gradients of loss w.r.t parameters $\\rightarrow$ **update parameters so model can churn output closer to labels** $\\rightarrow$ repeat\n",
    "\n",
    "\n",
    "## Learning Rate Pointers\n",
    "![](./images/lr1.png)\n",
    "- **Update parameters so model can churn output closer to labels, lower loss**\n",
    "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
    "- If we set $\\eta$ to be a **large value** $\\rightarrow$ learn too much (rapid learning)\n",
    "    - Unable to converge to a good local minima (unable to effectively gradually decrease your loss, overshoot the local lowest value)\n",
    "- If we set $\\eta$ to be a **small value** $\\rightarrow$ learn too little (slow learning)\n",
    "    - May take too long or unable to convert to a good local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Benefits\n",
    "    - Converge faster\n",
    "    - Higher accuracy\n",
    "![](./images/lr2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Step-wise Decay \n",
    "2. Reduce on Loss Plateau Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step-wise Decay: Every Epoch\n",
    "- At every epoch,\n",
    "    - $ \\eta_t = \\eta_{t-1}\\gamma $\n",
    "    - $\\gamma = 0.1$\n",
    "- Optimization Algorithm 4: SGD Nesterov\n",
    "    - Modification of SGD Momentum \n",
    "        - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
    "        - $\\theta = \\theta - v_t$\n",
    "- Practical example\n",
    "    - Given $ \\eta_t = 0.1 $ and $ \\gamma = 0.01$\n",
    "    - Epoch 0: $ \\eta_t = 0.1 $\n",
    "    - Epoch 1: $ \\eta_{t+1} = 0.1 (0.1) =  0.01$\n",
    "    - Epoch 2: $ \\eta_{t+2} = 0.1 (0.1)^2 =  0.001$\n",
    "    - Epoch n: $ \\eta_{t+n} = 0.1 (0.1)^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.25232651829719543. Accuracy: 95.66\n",
      "Epoch: 1 LR: [0.010000000000000002]\n",
      "Iteration: 1000. Loss: 0.07556439191102982. Accuracy: 97.1\n",
      "Epoch: 2 LR: [0.0010000000000000002]\n",
      "Iteration: 1500. Loss: 0.08089657127857208. Accuracy: 97.14\n",
      "Epoch: 3 LR: [0.00010000000000000003]\n",
      "Iteration: 2000. Loss: 0.03777354583144188. Accuracy: 97.16\n",
      "Epoch: 4 LR: [1.0000000000000003e-05]\n",
      "Iteration: 2500. Loss: 0.039101023226976395. Accuracy: 97.16\n",
      "Iteration: 3000. Loss: 0.0747142806649208. Accuracy: 97.16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 1 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step-wise Decay: Every 2 Epochs\n",
    "- At every 2 epoch,\n",
    "    - $ \\eta_t = \\eta_{t-1}\\gamma $\n",
    "    - $\\gamma = 0.1$\n",
    "- Optimization Algorithm 4: SGD Nesterov\n",
    "    - Modification of SGD Momentum \n",
    "        - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
    "        - $\\theta = \\theta - v_t$\n",
    "- Practical example\n",
    "    - Given $ \\eta_t = 0.1 $ and $ \\gamma = 0.01$\n",
    "    - Epoch 0: $ \\eta_t = 0.1 $\n",
    "    - Epoch 1: $ \\eta_{t+1} = 0.1$\n",
    "    - Epoch 2: $ \\eta_{t+2} = 0.1 (0.1) =  0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.25232651829719543. Accuracy: 95.66\n",
      "Epoch: 1 LR: [0.1]\n",
      "Iteration: 1000. Loss: 0.07672126591205597. Accuracy: 96.47\n",
      "Epoch: 2 LR: [0.010000000000000002]\n",
      "Iteration: 1500. Loss: 0.07323123514652252. Accuracy: 97.5\n",
      "Epoch: 3 LR: [0.010000000000000002]\n",
      "Iteration: 2000. Loss: 0.045940183103084564. Accuracy: 97.62\n",
      "Epoch: 4 LR: [0.0010000000000000002]\n",
      "Iteration: 2500. Loss: 0.029797477647662163. Accuracy: 97.56\n",
      "Iteration: 3000. Loss: 0.034976325929164886. Accuracy: 97.58\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step-wise Decay: Every Epoch, Larger Gamma\n",
    "- At every epoch,\n",
    "    - $ \\eta_t = \\eta_{t-1}\\gamma $\n",
    "    - $\\gamma = 0.96$\n",
    "- Optimization Algorithm 4: SGD Nesterov\n",
    "    - Modification of SGD Momentum \n",
    "        - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
    "        - $\\theta = \\theta - v_t$\n",
    "- Practical example\n",
    "    - Given $ \\eta_t = 0.1 $ and $ \\gamma = 0.96$\n",
    "    - Epoch 1: $ \\eta_t = 0.1 $\n",
    "    - Epoch 2: $ \\eta_{t+1} = 0.1 (0.96) =  0.096$\n",
    "    - Epoch 3: $ \\eta_{t+2} = 0.1 (0.96)^2 =  0.092$\n",
    "    - Epoch n: $ \\eta_{t+n} = 0.1 (0.96)^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.25232651829719543. Accuracy: 95.66\n",
      "Epoch: 1 LR: [0.1]\n",
      "Iteration: 1000. Loss: 0.07672126591205597. Accuracy: 96.47\n",
      "Epoch: 2 LR: [0.096]\n",
      "Iteration: 1500. Loss: 0.10065296292304993. Accuracy: 97.07\n",
      "Epoch: 3 LR: [0.096]\n",
      "Iteration: 2000. Loss: 0.05262482166290283. Accuracy: 97.62\n",
      "Epoch: 4 LR: [0.09216]\n",
      "Iteration: 2500. Loss: 0.021723033860325813. Accuracy: 97.54\n",
      "Iteration: 3000. Loss: 0.0373525433242321. Accuracy: 97.71\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointers on Step-wise Decay\n",
    "- You would want to decay your LR gradually when you're training more epochs\n",
    "    - Converge too fast, to a crappy loss/accuracy, if you decay rapidly\n",
    "- To decay slower\n",
    "    - Larger $\\gamma$\n",
    "    - Larger interval of decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reduce on Loss Plateau Decay, Patience=1, Factor=0.1\n",
    "- Reduce learning rate whenever loss plateaus\n",
    "    - Patience: number of epochs with no improvement after which learning rate will be reduced\n",
    "        - Patience = 1 \n",
    "    - Factor: multiplier to decrease learning rate, lr = lr*factor $\\rightarrow \\gamma$\n",
    "        - Factor=0.1\n",
    "- Optimization Algorithm 4: SGD Nesterov\n",
    "    - Modification of SGD Momentum \n",
    "        - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
    "        - $\\theta = \\theta - v_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Loss: 0.06099593639373779. Accuracy: 95.66\n",
      "--------------------\n",
      "Epoch 1 completed\n",
      "Loss: 0.17056599259376526. Accuracy: 96.47\n",
      "--------------------\n",
      "Epoch 2 completed\n",
      "Loss: 0.08728326857089996. Accuracy: 96.82\n",
      "--------------------\n",
      "Epoch 3 completed\n",
      "Loss: 0.056053031235933304. Accuracy: 97.56\n",
      "--------------------\n",
      "Epoch 4 completed\n",
      "Loss: 0.050669003278017044. Accuracy: 97.58\n",
      "--------------------\n",
      "Epoch 5 completed\n",
      "Loss: 0.04032094031572342. Accuracy: 97.45\n",
      "--------------------\n",
      "Epoch     5: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 6 completed\n",
      "Loss: 0.0028860687743872404. Accuracy: 97.97\n",
      "--------------------\n",
      "Epoch 7 completed\n",
      "Loss: 0.0026359560433775187. Accuracy: 98.01\n",
      "--------------------\n",
      "Epoch 8 completed\n",
      "Loss: 0.0034447305370122194. Accuracy: 98.04\n",
      "--------------------\n",
      "Epoch 9 completed\n",
      "Loss: 0.026919852942228317. Accuracy: 97.98\n",
      "--------------------\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# lr = lr * factor \n",
    "# mode='max': look for the maximum validation accuracy to track\n",
    "# patience: number of epochs - 1 where loss plateaus before decreasing LR\n",
    "        # patience = 0, after 1 bad epoch, reduce LR\n",
    "# factor = decaying factor\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=0, verbose=True)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n",
    "        \n",
    "    # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n",
    "    print('Epoch {} completed'.format(epoch))\n",
    "    print('Loss: {}. Accuracy: {}'.format(loss.data[0], accuracy))\n",
    "    print('-'*20)\n",
    "    scheduler.step(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reduce on Loss Plateau Decay, Patience=1, Factor=0.5\n",
    "- Reduce learning rate whenever loss plateaus\n",
    "    - Patience: number of epochs with no improvement after which learning rate will be reduced\n",
    "        - Patience = 1 \n",
    "    - Factor: multiplier to decrease learning rate, lr = lr*factor\n",
    "        - Factor=0.5\n",
    "- Optimization Algorithm 4: SGD Nesterov\n",
    "    - Modification of SGD Momentum \n",
    "        - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
    "        - $\\theta = \\theta - v_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Loss: 0.06099593639373779. Accuracy: 95.66\n",
      "--------------------\n",
      "Epoch 1 completed\n",
      "Loss: 0.17056599259376526. Accuracy: 96.47\n",
      "--------------------\n",
      "Epoch 2 completed\n",
      "Loss: 0.08728326857089996. Accuracy: 96.82\n",
      "--------------------\n",
      "Epoch 3 completed\n",
      "Loss: 0.056053031235933304. Accuracy: 97.56\n",
      "--------------------\n",
      "Epoch 4 completed\n",
      "Loss: 0.050669003278017044. Accuracy: 97.58\n",
      "--------------------\n",
      "Epoch 5 completed\n",
      "Loss: 0.04032094031572342. Accuracy: 97.45\n",
      "--------------------\n",
      "Epoch     5: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch 6 completed\n",
      "Loss: 0.0020523075945675373. Accuracy: 98.04\n",
      "--------------------\n",
      "Epoch 7 completed\n",
      "Loss: 0.0019354888936504722. Accuracy: 97.92\n",
      "--------------------\n",
      "Epoch     7: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch 8 completed\n",
      "Loss: 0.003255807561799884. Accuracy: 98.02\n",
      "--------------------\n",
      "Epoch     8: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Epoch 9 completed\n",
      "Loss: 0.019222687929868698. Accuracy: 98.05\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Where to add a new import\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# lr = lr * factor \n",
    "# mode='max': look for the maximum validation accuracy to track\n",
    "# patience: number of epochs - 1 where loss plateaus before decreasing LR\n",
    "        # patience = 0, after 1 bad epoch, reduce LR\n",
    "# factor = decaying factor\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=0, verbose=True)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            # print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))\n",
    "        \n",
    "    # Decay Learning Rate, pass validation accuracy for tracking at every epoch\n",
    "    print('Epoch {} completed'.format(epoch))\n",
    "    print('Loss: {}. Accuracy: {}'.format(loss.data[0], accuracy))\n",
    "    print('-'*20)\n",
    "    scheduler.step(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointers on Reduce on Loss Pleateau Decay\n",
    "- In these examples, we used patience=1 because we are running few epochs\n",
    "    - You should look at a larger patience such as 5 if for example you ran 500 epochs. \n",
    "- You should experiment with 2 properties \n",
    "    - Patience\n",
    "    - Decay factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Learning Rate Intuition\n",
    "    - Update parameters so model can churn output closer to labels\n",
    "    - Gradual parameter updates\n",
    "- Learning Rate Pointers\n",
    "    - If we set $\\eta$ to be a **large value** $\\rightarrow$ learn too much (rapid learning)\n",
    "    - If we set $\\eta$ to be a **small value** $\\rightarrow$ learn too little (slow learning)\n",
    "- Learning Rate Schedules\n",
    "    - Step-wise Decay\n",
    "    - Reduce on Loss Plateau Decay\n",
    "- Step-wise Decay\n",
    "    - Every 1 epoch\n",
    "    - Every 2 epoch\n",
    "    - Every 1 epoch, larger gamma\n",
    "- Step-wise Decay Pointers\n",
    "    - Decay LR gradually\n",
    "        - Larger $\\gamma$\n",
    "        - Larger interval of decay (increase epoch)\n",
    "- Reduce on Loss Plateau Decay\n",
    "    - Patience=1, Factor=1\n",
    "    - Patience =1, Factor=0.5\n",
    "- Pointers on Reduce on Loss Plateau Decay\n",
    "    - Larger patience with more epochs\n",
    "    - 2 properties to experiment\n",
    "        - Patience\n",
    "        - Decay factor\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
