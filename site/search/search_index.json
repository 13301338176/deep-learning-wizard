{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Us \u00b6 We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our video course and join over 2300 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. PyTorch as our Preferred Deep Learning Library \u00b6 We chose PyTorch because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook. # It is this easy! import torch # Create a variable of value 1 each. a = torch . Tensor ([ 1 ]) b = torch . Tensor ([ 1 ]) # Add the 2 variables to give you 2, it's that simple! c = a + b Made for Visual and Book Lovers \u00b6 We are visual creatures, that is why we offer detailed video courses on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. For book lovers, you will be happy to know Deep Learning Wizard's wikipedia will always be updated first prior to our release of video courses. Experienced Research and Applied Team \u00b6 Ritchie Ng Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA. My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial. I was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI. Check out my profile link at ritchieng.com Jie Fu I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal. I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low. I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner. Check out my profile link at bigaidream.github.io","title":"Home"},{"location":"#about-us","text":"We deploy a top-down approach that enables you to grasp deep learning theories and code easily and quickly. We have open-sourced all our materials through our Deep Learning Wizard Wikipedia. For visual learners, feel free to sign up for our video course and join over 2300 deep learning wizards. To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world.","title":"About Us"},{"location":"#pytorch-as-our-preferred-deep-learning-library","text":"We chose PyTorch because it integrates with Python well with similar syntax that allows you to quickly pick it up and implement your projects and research papers on GPU and CPU. It is also actively maintained by Facebook. # It is this easy! import torch # Create a variable of value 1 each. a = torch . Tensor ([ 1 ]) b = torch . Tensor ([ 1 ]) # Add the 2 variables to give you 2, it's that simple! c = a + b","title":"PyTorch as our Preferred Deep Learning Library"},{"location":"#made-for-visual-and-book-lovers","text":"We are visual creatures, that is why we offer detailed video courses on Udemy that have shown to accelerate learning and boost knowledge retention. Our courses are updated regularly to ensure our codes are compatible with the latest version of PyTorch. For book lovers, you will be happy to know Deep Learning Wizard's wikipedia will always be updated first prior to our release of video courses.","title":"Made for Visual and Book Lovers"},{"location":"#experienced-research-and-applied-team","text":"Ritchie Ng Currently I am leading artificial intelligence with my colleagues in ensemblecap.ai, an AI hedge fund based in Singapore. I am also an NVIDIA Deep Learning Institute instructor enabling developers, data scientists, and researchers leverage on deep learning to solve the most challenging problems. Also, I\u2019m into deep learning research with researchers based in NExT++ (NUS) and MILA. My passion for enabling anyone to leverage on deep learning has led to the creation of Deep Learning Wizard where I have taught and still continue to teach more than 2000 students in over 60 countries around the world. The course is recognized by Soumith Chintala, Facebook AI Research, and Alfredo Canziani, Post-Doctoral Associate under Yann Lecun, as the first comprehensive PyTorch Tutorial. I was previously conducting research in deep learning, computer vision and natural language processing in NExT Search Centre led by Professor Tat-Seng Chua that is jointly setup between National University of Singapore (NUS) and Tsinghua University and is part of NUS Smart Systems Institute. During my time there, I managed to publish in top-tier conferences and workshops like ICML and IJCAI. Check out my profile link at ritchieng.com Jie Fu I am undergoing my postdoc journey at Montreal Institute for Learning Algorithms (MILA) to prepare for the coming AI winter, as Eddard Stark said \"He won't be a boy forever and winter is coming\" -- Game of Thrones. I am privileged to work with Christopher Pal. I earned my PhD degree from National University of Singapore (NUS), and was fortunately under the supervision of Tat-Seng Chua and Huan Xu, also closely working with Jiashi Feng and Kian Hsiang Low. I am interested in machine learning. More specifically, my research is focused on deep learning, probabilistic reasoning, reinforcement learning and neural abstract machines. I am especially excited about reducing the gap between theoretical and practical algorithms in a principled and efficient manner. Check out my profile link at bigaidream.github.io","title":"Experienced Research and Applied Team"},{"location":"review/","text":"Reviews \u00b6 To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors. Roberto Trevi\u00f1o Cervantes Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year. Muktabh Mayank This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch. Charles Neiswender I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing. Ian Lipton This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math. And check out hundreds of more reviews for our video course !","title":"Reviews"},{"location":"review/#reviews","text":"To this date, we have taught thousands of students across more than 120+ countries from students as young as 15 to postgraduates and professionals in leading MNCs and research institutions around the world. These are just some of the hundreds of reviews we've had and will be updated when we have more time or contributors. Roberto Trevi\u00f1o Cervantes Congratulations! You have achieved the unachievable, teaching a 15 year old boy how to get a machine to estimate a function, just what he had been trying to do for almost a year. Muktabh Mayank This course helped me understand idiomatic pytorch and avoiding translating theano-to-torch. Charles Neiswender I really feel like I can take the techniques I learned and apply them to deep learning projects of my choosing. Ian Lipton This was a good overview of the different types of neural networks. I like that the instructor started with linear and logistic regression before diving into the neural networks. That background helped make sense of the neural network functionality. I like that this gave enough detail on the mathematics so that you can understand the basic functioning of the models but didn't overwhelm you with the nitty gritty math. And check out hundreds of more reviews for our video course !","title":"Reviews"},{"location":"supporters/","text":"Supporters \u00b6 More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings. Individuals \u00b6 Alfredo Canziani Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch. He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun. Do check out his latest mini-course on PyTorch that was held in Princeton University. Marek Bardonski Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months. NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation. Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal . Corporations \u00b6 NVIDIA (NVIDIA Inception Partner) Facebook Amazon Research Institutions \u00b6 Montreal Institute of Learning Algorithms (MILA), Montreal, Canada Imperial College London, UK Massachusetts Institute of Technology (MIT), USA National University of Singapore (NUS), Singapore Nanyang Technological University (NTU), Singapore NVIDIA Inception Partner \u00b6 \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.","title":"Supporters"},{"location":"supporters/#supporters","text":"More than 2500 students in over 120 countries around the world from students as young as 15 to postgraduates and professionals in leading corporations and research institutions have taken our course with awesome ratings.","title":"Supporters"},{"location":"supporters/#individuals","text":"Alfredo Canziani Alfredo is the main person who inspired Ritchie to open-source Deep Learning Wizard's materials. We are very grateful for his advice particular in the space of open-source projects, deep learning and PyTorch. He is currently working as a Post-Doctoral Deep Learning Research Scientist and Lecturer at Courant Institute of Mathematical Sciences, under the supervision of professors KyungHyun Cho and Yann LeCun. Do check out his latest mini-course on PyTorch that was held in Princeton University. Marek Bardonski Since graduation, Marek has been working on state-of-the-art research. He was involved in inventing the system that diagnoses problems with Ubuntu packages using Machine Learning and in improving the detection accuracy of breast cancer using Deep Learning, each time leading 40 highly skilled Machine Learning/Deep Learning engineers for over 6 months. NASA noticed his unique skills and asked him to manage a project aiming to improve the International Space Station rotation algorithm that would allow them to collect enough power from solar panels while preventing them from overheating. The project was successful and generated a huge ROI for NASA. Marek has been honored by multiple Harvard professors and the Director of Advanced Exploration Systems at NASA, who signed an official Letter of Recommendation. Since his first summer of university coursework, Marek has participated in four internships at NVIDIA HQ and Microsoft HQ. He also spent one year in NVIDIA Switzerland optimizing Deep Learning inference for self-driving cars and, by implementing very efficient Deep Learning algorithms in the GPU assembly language, improving the speed of Tesla\u2019s cars self-driving engine inference. He's currently the Head of AI at Sigmoidal .","title":"Individuals"},{"location":"supporters/#corporations","text":"NVIDIA (NVIDIA Inception Partner) Facebook Amazon","title":"Corporations"},{"location":"supporters/#research-institutions","text":"Montreal Institute of Learning Algorithms (MILA), Montreal, Canada Imperial College London, UK Massachusetts Institute of Technology (MIT), USA National University of Singapore (NUS), Singapore Nanyang Technological University (NTU), Singapore","title":"Research Institutions"},{"location":"supporters/#nvidia-inception-partner","text":"\"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems.","title":"NVIDIA Inception Partner"},{"location":"deep_learning/intro/","text":"Deep Learning Theory and Programming Tutorials \u00b6 Our main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our video course . Work-in-progress This open-source portion is still a work in progress, it is very sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page. Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier experience. All of our code allows you to run in a notebook for this deep learning section. Please use a jupyter notebook and run the examples from the start of the page to the end.","title":"Introduction"},{"location":"deep_learning/intro/#deep-learning-theory-and-programming-tutorials","text":"Our main open-source programming languages and libraries are Python, PyTorch and C++. If you would like a more visual and guided experience, feel free to take our video course . Work-in-progress This open-source portion is still a work in progress, it is very sparse in explanation as traditionally all our explanation are done via video. Stay tuned while we gradually upload our tutorials and notes. Feel free to contact Ritchie Ng if you would like to contribute via our Facebook page. Also take note that these notes are best used as a referral. This is because we have yet to expand it comprehensively to be a stand-alone guide. Go head and take our video course that provides a much easier experience. All of our code allows you to run in a notebook for this deep learning section. Please use a jupyter notebook and run the examples from the start of the page to the end.","title":"Deep Learning Theory and Programming Tutorials"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/","text":"Feedforward Neural Network with PyTorch \u00b6 About Feedforward Neural Network \u00b6 Logistic Regression Transition to Neural Networks \u00b6 Logistic Regression Review \u00b6 Define logistic regression model Import our relevant torch modules. import torch import torch.nn as nn Define our model class. class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate the logistic regression model. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). print ( model ) LogisticRegressionModel ( ( linear ): Linear ( in_features = 784 , out_features = 10 , bias = True ) ) Logistic Regression Problems \u00b6 Can represent linear functions well y = 2x + 3 y = 2x + 3 y = x_1 + x_2 y = x_1 + x_2 y = x_1 + 3x_2 + 4x_3 y = x_1 + 3x_2 + 4x_3 Cannot represent non-linear functions y = 4x_1 + 2x_2^2 +3x_3^3 y = 4x_1 + 2x_2^2 +3x_3^3 y = x_1x_2 y = x_1x_2 Introducing a Non-linear Function \u00b6 Non-linear Function In-Depth \u00b6 Function: takes a number & perform mathematical operation Common Types of Non-linearity ReLUs (Rectified Linear Units) Sigmoid Tanh Sigmoid (Logistic) \u00b6 \\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this Outputs not centered around 0 If output always positive \\rightarrow \\rightarrow gradients always positive or negative \\rightarrow \\rightarrow bad for gradient updates Tanh \u00b6 \\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this ReLUs \u00b6 f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate choice Building a Feedforward Neural Network with PyTorch \u00b6 Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) Step 2: Make Dataset Iterable \u00b6 Batch sizes and iterations Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently. There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would: (1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. (2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests! If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. 60000 / 100 600.0 Epochs An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations. If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). 600 * 5 3000.0 Bringing batch size, iterations and epochs together As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100. Because we want 5 epochs, we need a total of 3000 iterations. batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Step 3: Create Model Class \u00b6 Creating our feedforward neural network Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. This translates to just 4 more lines of code! class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . sigmoid = nn . Sigmoid () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function # LINEAR out = self . fc1 ( x ) # Non-linearity # NON-LINEAR out = self . sigmoid ( out ) # Linear function (readout) # LINEAR out = self . fc2 ( out ) return out Step 4: Instantiate Model Class \u00b6 Input dimension: 784 Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Hidden dimension: 100 Can be any number Similar term Number of neurons Number of non-linear activation functions Instantiating our model class Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28). Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10. Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results. On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Feedforward Neural Network: Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Loss class This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels. criterion = nn . CrossEntropyLoss () Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation capabilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer class Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities. Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section. learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth \u00b6 Linear layers' parameters In a simple linear layer it's Y = AX + B Y = AX + B , and our parameters are A A and bias B B . Hence, each linear layer would have 2 groups of parameters A A and B B . It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on Y Y , the output of our linear layers. This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. print ( model . parameters ()) Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters A A and b b . print ( len ( list ( model . parameters ()))) Our first linear layer parameters, A_1 A_1 , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) Our first linear layer bias parameters, B_1 B_1 , would be of size 100 which is our hidden size. # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) Our second linear layer is our readout layer, where the parameters A_2 A_2 would be of size 10 x 100. This is because our output size is 10 and hidden size is 100. # FC 2 Parameters print ( list ( model . parameters ())[ 2 ] . size ()) Likewise our readout layer's bias B_1 B_1 would just be 10, the size of our output. # FC 2 Bias Parameters print ( list ( model . parameters ())[ 3 ] . size ()) The diagram below shows the interaction amongst our input X X and our linear layers' parameters A_1 A_1 , B_1 B_1 , A_2 A_2 , and B_2 B_2 to reach to the final size of 10 x 1. If you're still unfamiliar with dot product, go ahead and review the previous quick lesson where we covered it in logistic regression . < generator object Module . parameters at 0x7f1d530fa678 > 4 torch . Size ([ 100 , 784 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ]) Step 7: Train Model \u00b6 Process Convert inputs to tensors with gradient accumulation capabilities Clear gradient buffers Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT 7-step training process iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.6457265615463257 . Accuracy : 85 Iteration : 1000. Loss : 0.39627206325531006 . Accuracy : 89 Iteration : 1500. Loss : 0.2831554412841797 . Accuracy : 90 Iteration : 2000. Loss : 0.4409525394439697 . Accuracy : 91 Iteration : 2500. Loss : 0.2397005707025528 . Accuracy : 91 Iteration : 3000. Loss : 0.3160165846347809 . Accuracy : 91 Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with Tanh Activation The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.4128190577030182 . Accuracy : 91 Iteration : 1000. Loss : 0.14497484266757965 . Accuracy : 92 Iteration : 1500. Loss : 0.272532194852829 . Accuracy : 93 Iteration : 2000. Loss : 0.2758277952671051 . Accuracy : 94 Iteration : 2500. Loss : 0.1603182554244995 . Accuracy : 94 Iteration : 3000. Loss : 0.08848697692155838 . Accuracy : 95 Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with ReLU Activation The only difference again is in using ReLU activation and it affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3179700970649719 . Accuracy : 91 Iteration : 1000. Loss : 0.17288273572921753 . Accuracy : 93 Iteration : 1500. Loss : 0.16829034686088562 . Accuracy : 94 Iteration : 2000. Loss : 0.25494423508644104 . Accuracy : 94 Iteration : 2500. Loss : 0.16818439960479736 . Accuracy : 95 Iteration : 3000. Loss : 0.11110792309045792 . Accuracy : 95 Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2-layer FNN with ReLU Activation This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3 (readout): 100 --> 10 self . fc3 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 3 (readout) out = self . fc3 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.2995373010635376 . Accuracy : 91 Iteration : 1000. Loss : 0.3924565613269806 . Accuracy : 93 Iteration : 1500. Loss : 0.1283276081085205 . Accuracy : 94 Iteration : 2000. Loss : 0.10905527323484421 . Accuracy : 95 Iteration : 2500. Loss : 0.11943754553794861 . Accuracy : 96 Iteration : 3000. Loss : 0.15632082521915436 . Accuracy : 96 Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation) \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality? import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.33234935998916626 . Accuracy : 89 Iteration : 1000. Loss : 0.3098006248474121 . Accuracy : 94 Iteration : 1500. Loss : 0.12461677193641663 . Accuracy : 95 Iteration : 2000. Loss : 0.14346086978912354 . Accuracy : 96 Iteration : 2500. Loss : 0.03763459622859955 . Accuracy : 96 Iteration : 3000. Loss : 0.1397182047367096 . Accuracy : 97 General Comments on FNNs \u00b6 2 ways to expand a neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy 3. Building a Feedforward Neural Network with PyTorch (GPU) \u00b6 GPU: 2 things must be on GPU - model - tensors with gradient accumulation capabilities Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation on GPU Only step 4 and 7 of the CPU code will be affected and it's a simple change. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3877025246620178 . Accuracy : 90 Iteration : 1000. Loss : 0.1337055265903473 . Accuracy : 93 Iteration : 1500. Loss : 0.2038637101650238 . Accuracy : 95 Iteration : 2000. Loss : 0.17892278730869293 . Accuracy : 95 Iteration : 2500. Loss : 0.14455552399158478 . Accuracy : 96 Iteration : 3000. Loss : 0.024540524929761887 . Accuracy : 96 Summary \u00b6 We've learnt to... Success Logistic Regression Problems for Non-Linear Functions Representation Cannot represent non-linear functions $ y = 4x_1 + 2x_2^2 +3x_3^3 $ $ y = x_1x_2$ Introduced Non-Linearity to Logistic Regression to form a Neural Network Types of Non-Linearity Sigmoid Tanh ReLU Feedforward Neural Network Models Model A: 1 hidden layer ( sigmoid activation) Model B: 1 hidden layer ( tanh activation) Model C: 1 hidden layer ( ReLU activation) Model D: 2 hidden layers (ReLU activation) Model E: 3 hidden layers (ReLU activation) Models Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation capabilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"PyTorch Fundamentals - Feedforward Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#feedforward-neural-network-with-pytorch","text":"","title":"Feedforward Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#about-feedforward-neural-network","text":"","title":"About Feedforward Neural Network"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-transition-to-neural-networks","text":"","title":"Logistic Regression Transition to Neural Networks"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-review","text":"Define logistic regression model Import our relevant torch modules. import torch import torch.nn as nn Define our model class. class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate the logistic regression model. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) When we inspect the model, we would have an input size of 784 (derived from 28 x 28) and output size of 10 (which is the number of classes we are classifying from 0 to 9). print ( model ) LogisticRegressionModel ( ( linear ): Linear ( in_features = 784 , out_features = 10 , bias = True ) )","title":"Logistic Regression Review"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#logistic-regression-problems","text":"Can represent linear functions well y = 2x + 3 y = 2x + 3 y = x_1 + x_2 y = x_1 + x_2 y = x_1 + 3x_2 + 4x_3 y = x_1 + 3x_2 + 4x_3 Cannot represent non-linear functions y = 4x_1 + 2x_2^2 +3x_3^3 y = 4x_1 + 2x_2^2 +3x_3^3 y = x_1x_2 y = x_1x_2","title":"Logistic Regression Problems"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#introducing-a-non-linear-function","text":"","title":"Introducing a Non-linear Function"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#non-linear-function-in-depth","text":"Function: takes a number & perform mathematical operation Common Types of Non-linearity ReLUs (Rectified Linear Units) Sigmoid Tanh","title":"Non-linear Function In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#sigmoid-logistic","text":"\\sigma(x) = \\frac{1}{1 + e^{-x}} \\sigma(x) = \\frac{1}{1 + e^{-x}} Input number \\rightarrow \\rightarrow [0, 1] Large negative number \\rightarrow \\rightarrow 0 Large positive number \\rightarrow \\rightarrow 1 Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution: Have to carefully initialize weights to prevent this Outputs not centered around 0 If output always positive \\rightarrow \\rightarrow gradients always positive or negative \\rightarrow \\rightarrow bad for gradient updates","title":"Sigmoid (Logistic)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#tanh","text":"\\tanh(x) = 2 \\sigma(2x) -1 \\tanh(x) = 2 \\sigma(2x) -1 A scaled sigmoid function Input number \\rightarrow \\rightarrow [-1, 1] Cons: Activation saturates at 0 or 1 with gradients \\approx \\approx 0 No signal to update weights \\rightarrow \\rightarrow cannot learn Solution : Have to carefully initialize weights to prevent this","title":"Tanh"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#relus","text":"f(x) = \\max(0, x) f(x) = \\max(0, x) Pros: Accelerates convergence \\rightarrow \\rightarrow train faster Less computationally expensive operation compared to Sigmoid/Tanh exponentials Cons: Many ReLU units \"die\" \\rightarrow \\rightarrow gradients = 0 forever Solution : careful learning rate choice","title":"ReLUs"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#building-a-feedforward-neural-network-with-pytorch","text":"","title":"Building a Feedforward Neural Network with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-a-1-hidden-layer-feedforward-neural-network-sigmoid-activation","text":"","title":"Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-1-loading-mnist-train-dataset","text":"Images from 1 to 9 Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ())","title":"Step 1: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-2-make-dataset-iterable","text":"Batch sizes and iterations Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently. There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would: (1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors. (2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests! If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions. 60000 / 100 600.0 Epochs An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations. If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5). 600 * 5 3000.0 Bringing batch size, iterations and epochs together As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100. Because we want 5 epochs, we need a total of 3000 iterations. batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-3-create-model-class","text":"Creating our feedforward neural network Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer. This translates to just 4 more lines of code! class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . sigmoid = nn . Sigmoid () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function # LINEAR out = self . fc1 ( x ) # Non-linearity # NON-LINEAR out = self . sigmoid ( out ) # Linear function (readout) # LINEAR out = self . fc2 ( out ) return out","title":"Step 3: Create Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-4-instantiate-model-class","text":"Input dimension: 784 Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Hidden dimension: 100 Can be any number Similar term Number of neurons Number of non-linear activation functions Instantiating our model class Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28). Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10. Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results. On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power. input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-5-instantiate-loss-class","text":"Feedforward Neural Network: Cross Entropy Loss Logistic Regression : Cross Entropy Loss Linear Regression : MSE Loss class This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels. criterion = nn . CrossEntropyLoss ()","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our tensors with gradient accumulation capabilities) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Optimizer class Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities. Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong. A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm. A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently. We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section. learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#parameters-in-depth","text":"Linear layers' parameters In a simple linear layer it's Y = AX + B Y = AX + B , and our parameters are A A and bias B B . Hence, each linear layer would have 2 groups of parameters A A and B B . It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on Y Y , the output of our linear layers. This would return a Python generator object, so you need to call list on the generator object to access anything meaningful. print ( model . parameters ()) Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters A A and b b . print ( len ( list ( model . parameters ()))) Our first linear layer parameters, A_1 A_1 , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100. # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) Our first linear layer bias parameters, B_1 B_1 , would be of size 100 which is our hidden size. # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) Our second linear layer is our readout layer, where the parameters A_2 A_2 would be of size 10 x 100. This is because our output size is 10 and hidden size is 100. # FC 2 Parameters print ( list ( model . parameters ())[ 2 ] . size ()) Likewise our readout layer's bias B_1 B_1 would just be 10, the size of our output. # FC 2 Bias Parameters print ( list ( model . parameters ())[ 3 ] . size ()) The diagram below shows the interaction amongst our input X X and our linear layers' parameters A_1 A_1 , B_1 B_1 , A_2 A_2 , and B_2 B_2 to reach to the final size of 10 x 1. If you're still unfamiliar with dot product, go ahead and review the previous quick lesson where we covered it in logistic regression . < generator object Module . parameters at 0x7f1d530fa678 > 4 torch . Size ([ 100 , 784 ]) torch . Size ([ 100 ]) torch . Size ([ 10 , 100 ]) torch . Size ([ 10 ])","title":"Parameters In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#step-7-train-model","text":"Process Convert inputs to tensors with gradient accumulation capabilities Clear gradient buffers Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT 7-step training process iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.6457265615463257 . Accuracy : 85 Iteration : 1000. Loss : 0.39627206325531006 . Accuracy : 89 Iteration : 1500. Loss : 0.2831554412841797 . Accuracy : 90 Iteration : 2000. Loss : 0.4409525394439697 . Accuracy : 91 Iteration : 2500. Loss : 0.2397005707025528 . Accuracy : 91 Iteration : 3000. Loss : 0.3160165846347809 . Accuracy : 91","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-b-1-hidden-layer-feedforward-neural-network-tanh-activation","text":"","title":"Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_1","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with Tanh Activation The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . tanh = nn . Tanh () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . tanh ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.4128190577030182 . Accuracy : 91 Iteration : 1000. Loss : 0.14497484266757965 . Accuracy : 92 Iteration : 1500. Loss : 0.272532194852829 . Accuracy : 93 Iteration : 2000. Loss : 0.2758277952671051 . Accuracy : 94 Iteration : 2500. Loss : 0.1603182554244995 . Accuracy : 94 Iteration : 3000. Loss : 0.08848697692155838 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-c-1-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_2","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 1-layer FNN with ReLU Activation The only difference again is in using ReLU activation and it affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity self . relu = nn . ReLU () # Linear function (readout) self . fc2 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function out = self . fc1 ( x ) # Non-linearity out = self . relu ( out ) # Linear function (readout) out = self . fc2 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3179700970649719 . Accuracy : 91 Iteration : 1000. Loss : 0.17288273572921753 . Accuracy : 93 Iteration : 1500. Loss : 0.16829034686088562 . Accuracy : 94 Iteration : 2000. Loss : 0.25494423508644104 . Accuracy : 94 Iteration : 2500. Loss : 0.16818439960479736 . Accuracy : 95 Iteration : 3000. Loss : 0.11110792309045792 . Accuracy : 95","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-d-2-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_3","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 2-layer FNN with ReLU Activation This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3 (readout): 100 --> 10 self . fc3 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 3 (readout) out = self . fc3 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.2995373010635376 . Accuracy : 91 Iteration : 1000. Loss : 0.3924565613269806 . Accuracy : 93 Iteration : 1500. Loss : 0.1283276081085205 . Accuracy : 94 Iteration : 2000. Loss : 0.10905527323484421 . Accuracy : 95 Iteration : 2500. Loss : 0.11943754553794861 . Accuracy : 96 Iteration : 3000. Loss : 0.15632082521915436 . Accuracy : 96","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#model-e-3-hidden-layer-feedforward-neural-network-relu-activation","text":"","title":"Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_4","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality? import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images with gradient accumulation capabilities images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.33234935998916626 . Accuracy : 89 Iteration : 1000. Loss : 0.3098006248474121 . Accuracy : 94 Iteration : 1500. Loss : 0.12461677193641663 . Accuracy : 95 Iteration : 2000. Loss : 0.14346086978912354 . Accuracy : 96 Iteration : 2500. Loss : 0.03763459622859955 . Accuracy : 96 Iteration : 3000. Loss : 0.1397182047367096 . Accuracy : 97","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#general-comments-on-fnns","text":"2 ways to expand a neural network More non-linear activation units (neurons) More hidden layers Cons Need a larger dataset Curse of dimensionality Does not necessarily mean higher accuracy","title":"General Comments on FNNs"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#3-building-a-feedforward-neural-network-with-pytorch-gpu","text":"GPU: 2 things must be on GPU - model - tensors with gradient accumulation capabilities","title":"3. Building a Feedforward Neural Network with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#steps_5","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model 3-layer FNN with ReLU Activation on GPU Only step 4 and 7 of the CPU code will be affected and it's a simple change. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class FeedforwardNeuralNetModel ( nn . Module ): def __init__ ( self , input_dim , hidden_dim , output_dim ): super ( FeedforwardNeuralNetModel , self ) . __init__ () # Linear function 1: 784 --> 100 self . fc1 = nn . Linear ( input_dim , hidden_dim ) # Non-linearity 1 self . relu1 = nn . ReLU () # Linear function 2: 100 --> 100 self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 2 self . relu2 = nn . ReLU () # Linear function 3: 100 --> 100 self . fc3 = nn . Linear ( hidden_dim , hidden_dim ) # Non-linearity 3 self . relu3 = nn . ReLU () # Linear function 4 (readout): 100 --> 10 self . fc4 = nn . Linear ( hidden_dim , output_dim ) def forward ( self , x ): # Linear function 1 out = self . fc1 ( x ) # Non-linearity 1 out = self . relu1 ( out ) # Linear function 2 out = self . fc2 ( out ) # Non-linearity 2 out = self . relu2 ( out ) # Linear function 2 out = self . fc3 ( out ) # Non-linearity 2 out = self . relu3 ( out ) # Linear function 4 (readout) out = self . fc4 ( out ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = FeedforwardNeuralNetModel ( input_dim , hidden_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.1 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 0.3877025246620178 . Accuracy : 90 Iteration : 1000. Loss : 0.1337055265903473 . Accuracy : 93 Iteration : 1500. Loss : 0.2038637101650238 . Accuracy : 95 Iteration : 2000. Loss : 0.17892278730869293 . Accuracy : 95 Iteration : 2500. Loss : 0.14455552399158478 . Accuracy : 96 Iteration : 3000. Loss : 0.024540524929761887 . Accuracy : 96","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/#summary","text":"We've learnt to... Success Logistic Regression Problems for Non-Linear Functions Representation Cannot represent non-linear functions $ y = 4x_1 + 2x_2^2 +3x_3^3 $ $ y = x_1x_2$ Introduced Non-Linearity to Logistic Regression to form a Neural Network Types of Non-Linearity Sigmoid Tanh ReLU Feedforward Neural Network Models Model A: 1 hidden layer ( sigmoid activation) Model B: 1 hidden layer ( tanh activation) Model C: 1 hidden layer ( ReLU activation) Model D: 2 hidden layers (ReLU activation) Model E: 3 hidden layers (ReLU activation) Models Variation in Code Modifying only step 3 Ways to Expand Model\u2019s Capacity More non-linear activation units ( neurons ) More hidden layers Cons of Expanding Capacity Need more data Does not necessarily mean higher accuracy GPU Code 2 things on GPU model tensors with gradient accumulation capabilities Modifying only Step 4 & Step 7 7 Step Model Building Recap Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/","text":"PyTorch Fundamentals - Gradients \u00b6 Tensors with Gradients \u00b6 Creating Tensors with Gradients \u00b6 Allows accumulation of gradients Method 1: Create tensor with gradients It is very similar to creating a tensor, all you need to do is to add an additional argument. import torch a = torch . ones (( 2 , 2 ), requires_grad = True ) a tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Check if tensor requires gradients This should return True otherwise you've not done it right. a . requires_grad True Method 2: Create tensor with gradients This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients. # Normal way of creating gradients a = torch . ones (( 2 , 2 )) # Requires gradient a . requires_grad_ () # Check if requires gradient a . requires_grad True A tensor without gradients just for comparison If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable no_gradient = torch . ones ( 2 , 2 ) no_gradient . requires_grad False Tensor with gradients addition operation # Behaves similarly to tensors b = torch . ones (( 2 , 2 ), requires_grad = True ) print ( a + b ) print ( torch . add ( a , b )) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) Tensor with gradients multiplication operation As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a * b ) print ( torch . mul ( a , b )) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Manually and Automatically Calculating Gradients \u00b6 What exactly is requires_grad ? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Create tensor of size 2x1 filled with 1's that requires gradient x = torch . ones ( 2 , requires_grad = True ) x tensor ([ 1. , 1. ]) Simple linear equation with x tensor created y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 We should get a value of 20 by replicating this simple equation y = 5 * ( x + 1 ) ** 2 y tensor ([ 20. , 20. ]) Simple equation with y tensor Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable Let's reduce y to a scalar then... o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i As you can see above, we've a tensor filled with 20's, so average them would return 20 o = ( 1 / 2 ) * torch . sum ( y ) o tensor ( 20. ) Calculating first derivative Recap y equation : y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Recap o equation : o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i Substitute y into o equation : o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 We should expect to get 10, and it's so simple to do this with PyTorch with the following line... Get first derivative: o . backward () Print out first derivative: x . grad tensor ([ 10. , 10. ]) If x requires gradient and you create new objects with it, you get all gradients print ( x . requires_grad ) print ( y . requires_grad ) print ( o . requires_grad ) True True True Summary \u00b6 We've learnt to... Success Tensor with Gradients Wraps a tensor for gradient accumulation Gradients Define original equation Substitute equation with x values Reduce to scalar output, o through mean Calculate gradients with o.backward() Then access gradients of the x tensor with requires_grad through x.grad","title":"PyTorch Fundamentals - Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#pytorch-fundamentals-gradients","text":"","title":"PyTorch Fundamentals - Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#tensors-with-gradients","text":"","title":"Tensors with Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#creating-tensors-with-gradients","text":"Allows accumulation of gradients Method 1: Create tensor with gradients It is very similar to creating a tensor, all you need to do is to add an additional argument. import torch a = torch . ones (( 2 , 2 ), requires_grad = True ) a tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Check if tensor requires gradients This should return True otherwise you've not done it right. a . requires_grad True Method 2: Create tensor with gradients This allows you to create a tensor as usual then an additional line to allow it to accumulate gradients. # Normal way of creating gradients a = torch . ones (( 2 , 2 )) # Requires gradient a . requires_grad_ () # Check if requires gradient a . requires_grad True A tensor without gradients just for comparison If you do not do either of the methods above, you'll realize you will get False for checking for gradients. # Not a variable no_gradient = torch . ones ( 2 , 2 ) no_gradient . requires_grad False Tensor with gradients addition operation # Behaves similarly to tensors b = torch . ones (( 2 , 2 ), requires_grad = True ) print ( a + b ) print ( torch . add ( a , b )) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) tensor ([[ 2. , 2. ], [ 2. , 2. ]]) Tensor with gradients multiplication operation As usual, the operations we learnt previously for tensors apply for tensors with gradients. Feel free to try divisions, mean or standard deviation! print ( a * b ) print ( torch . mul ( a , b )) tensor ([[ 1. , 1. ], [ 1. , 1. ]]) tensor ([[ 1. , 1. ], [ 1. , 1. ]])","title":"Creating Tensors with Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#manually-and-automatically-calculating-gradients","text":"What exactly is requires_grad ? - Allows calculation of gradients w.r.t. the tensor that all allows gradients accumulation y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Create tensor of size 2x1 filled with 1's that requires gradient x = torch . ones ( 2 , requires_grad = True ) x tensor ([ 1. , 1. ]) Simple linear equation with x tensor created y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 y_i\\bigr\\rvert_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20 We should get a value of 20 by replicating this simple equation y = 5 * ( x + 1 ) ** 2 y tensor ([ 20. , 20. ]) Simple equation with y tensor Backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable Let's reduce y to a scalar then... o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i As you can see above, we've a tensor filled with 20's, so average them would return 20 o = ( 1 / 2 ) * torch . sum ( y ) o tensor ( 20. ) Calculating first derivative Recap y equation : y_i = 5(x_i+1)^2 y_i = 5(x_i+1)^2 Recap o equation : o = \\frac{1}{2}\\sum_i y_i o = \\frac{1}{2}\\sum_i y_i Substitute y into o equation : o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 o = \\frac{1}{2} \\sum_i 5(x_i+1)^2 \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i+1)] \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10}{2}(2) = 10 We should expect to get 10, and it's so simple to do this with PyTorch with the following line... Get first derivative: o . backward () Print out first derivative: x . grad tensor ([ 10. , 10. ]) If x requires gradient and you create new objects with it, you get all gradients print ( x . requires_grad ) print ( y . requires_grad ) print ( o . requires_grad ) True True True","title":"Manually and Automatically Calculating Gradients"},{"location":"deep_learning/practical_pytorch/pytorch_gradients/#summary","text":"We've learnt to... Success Tensor with Gradients Wraps a tensor for gradient accumulation Gradients Define original equation Substitute equation with x values Reduce to scalar output, o through mean Calculate gradients with o.backward() Then access gradients of the x tensor with requires_grad through x.grad","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/","text":"Linear Regression with PyTorch \u00b6 About Linear Regression \u00b6 Simple Linear Regression Basics \u00b6 Allows us to understand relationship between two continuous variables Example x: independent variable weight y: dependent variable height y = \\alpha x + \\beta y = \\alpha x + \\beta Example of simple linear regression \u00b6 Create plot for simple linear regression Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists. import numpy as np import matplotlib.pyplot as plt % matplotlib inline # Creates 50 random x and y numbers np . random . seed ( 1 ) n = 50 x = np . random . randn ( n ) y = x * np . random . randn ( n ) # Makes the dots colorful colors = np . random . rand ( n ) # Plots best-fit line via polyfit plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) # Plots the random x and y data points we created # Interestingly, alpha makes it more aesthetically pleasing plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Aim of Linear Regression \u00b6 Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta ) Adjusting Coefficient: \\alpha \\alpha Bias/intercept: \\beta \\beta Building a Linear Regression Model with PyTorch \u00b6 Example \u00b6 Coefficient: \\alpha = 2 \\alpha = 2 Bias/intercept: \\beta = 1 \\beta = 1 Equation: y = 2x + 1 y = 2x + 1 Building a Toy Dataset \u00b6 Create a list of values from 0 to 11 x_values = [ i for i in range ( 11 )] x_values [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Convert list of numbers to numpy array # Convert to numpy x_train = np . array ( x_values , dtype = np . float32 ) x_train . shape ( 11 ,) Convert to 2-dimensional array If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required x_train = x_train . reshape ( - 1 , 1 ) x_train . shape ( 11 , 1 ) Create list of y values We want y values for every x value we have above. y = 2x + 1 y = 2x + 1 y_values = [ 2 * i + 1 for i in x_values ] y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Alternative to create list of y values If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators... y_values = [] for i in x_values : result = 2 * i + 1 y_values . append ( result ) y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Convert to numpy array You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good. y_train = np . array ( y_values , dtype = np . float32 ) y_train . shape ( 11 ,) Reshape y numpy array to 2-dimension # IMPORTANT: 2D required y_train = y_train . reshape ( - 1 , 1 ) y_train . shape ( 11 , 1 ) Building Model \u00b6 Critical Imports import torch import torch.nn as nn Create Model Linear model True Equation: y = 2x + 1 y = 2x + 1 Forward Example Input x = 1 x = 1 Output \\hat y = ? \\hat y = ? # Create class class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate Model Class input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21] input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) Instantiate Loss Class MSE Loss: Mean Squared Error MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) \\hat y \\hat y : prediction y y : true value criterion = nn . MSELoss () Instantiate Optimizer Class Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients parameters: \\alpha \\alpha and \\beta \\beta in y = \\alpha x + \\beta y = \\alpha x + \\beta desired parameters: \\alpha = 2 \\alpha = 2 and \\beta = 1 \\beta = 1 in $ y = 2x + 1$ learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Train Model 1 epoch: going through the whole x_train data once 100 epochs: 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 140.58143615722656 epoch 2 , loss 11.467253684997559 epoch 3 , loss 0.9358152747154236 epoch 4 , loss 0.07679400593042374 epoch 5 , loss 0.0067212567664682865 epoch 6 , loss 0.0010006226366385818 epoch 7 , loss 0.0005289533291943371 epoch 8 , loss 0.0004854927829001099 epoch 9 , loss 0.00047700389404781163 epoch 10 , loss 0.0004714332753792405 epoch 11 , loss 0.00046614606981165707 epoch 12 , loss 0.0004609318566508591 epoch 13 , loss 0.0004557870561257005 epoch 14 , loss 0.00045069155748933554 epoch 15 , loss 0.00044567222357727587 epoch 16 , loss 0.00044068993884138763 epoch 17 , loss 0.00043576463940553367 epoch 18 , loss 0.00043090470717288554 epoch 19 , loss 0.00042609183583408594 epoch 20 , loss 0.0004213254142086953 epoch 21 , loss 0.0004166301223449409 epoch 22 , loss 0.0004119801160413772 epoch 23 , loss 0.00040738462121225893 epoch 24 , loss 0.0004028224211651832 epoch 25 , loss 0.0003983367350883782 epoch 26 , loss 0.0003938761365134269 epoch 27 , loss 0.000389480876037851 epoch 28 , loss 0.00038514015614055097 epoch 29 , loss 0.000380824290914461 epoch 30 , loss 0.00037657516077160835 epoch 31 , loss 0.000372376263840124 epoch 32 , loss 0.0003682126116473228 epoch 33 , loss 0.0003640959912445396 epoch 34 , loss 0.00036003670538775623 epoch 35 , loss 0.00035601368290372193 epoch 36 , loss 0.00035203873994760215 epoch 37 , loss 0.00034810820943675935 epoch 38 , loss 0.000344215368386358 epoch 39 , loss 0.0003403784066904336 epoch 40 , loss 0.00033658024040050805 epoch 41 , loss 0.0003328165039420128 epoch 42 , loss 0.0003291067841928452 epoch 43 , loss 0.0003254293987993151 epoch 44 , loss 0.0003217888588551432 epoch 45 , loss 0.0003182037326041609 epoch 46 , loss 0.0003146533854305744 epoch 47 , loss 0.00031113551813177764 epoch 48 , loss 0.0003076607536058873 epoch 49 , loss 0.00030422292184084654 epoch 50 , loss 0.00030083119054324925 epoch 51 , loss 0.00029746422660537064 epoch 52 , loss 0.0002941471466328949 epoch 53 , loss 0.00029085995629429817 epoch 54 , loss 0.0002876132493838668 epoch 55 , loss 0.00028440452297218144 epoch 56 , loss 0.00028122696676291525 epoch 57 , loss 0.00027808290906250477 epoch 58 , loss 0.00027497278642840683 epoch 59 , loss 0.00027190230321139097 epoch 60 , loss 0.00026887087733484805 epoch 61 , loss 0.0002658693410921842 epoch 62 , loss 0.0002629039518069476 epoch 63 , loss 0.00025996880140155554 epoch 64 , loss 0.0002570618235040456 epoch 65 , loss 0.00025419273879379034 epoch 66 , loss 0.00025135406758636236 epoch 67 , loss 0.0002485490695107728 epoch 68 , loss 0.0002457649679854512 epoch 69 , loss 0.0002430236927466467 epoch 70 , loss 0.00024031475186347961 epoch 71 , loss 0.00023762597993481904 epoch 72 , loss 0.00023497406800743192 epoch 73 , loss 0.0002323519001947716 epoch 74 , loss 0.00022976362379267812 epoch 75 , loss 0.0002271933335578069 epoch 76 , loss 0.00022465786605607718 epoch 77 , loss 0.00022214400814846158 epoch 78 , loss 0.00021966728672850877 epoch 79 , loss 0.0002172116219298914 epoch 80 , loss 0.00021478648704942316 epoch 81 , loss 0.00021239375928416848 epoch 82 , loss 0.0002100227284245193 epoch 83 , loss 0.00020767028036061674 epoch 84 , loss 0.00020534756185952574 epoch 85 , loss 0.00020305956422816962 epoch 86 , loss 0.0002007894654525444 epoch 87 , loss 0.00019854879064951092 epoch 88 , loss 0.00019633043848443776 epoch 89 , loss 0.00019413618429098278 epoch 90 , loss 0.00019197272195015103 epoch 91 , loss 0.0001898303598864004 epoch 92 , loss 0.00018771187751553953 epoch 93 , loss 0.00018561164324637502 epoch 94 , loss 0.00018354636267758906 epoch 95 , loss 0.00018149390234611928 epoch 96 , loss 0.0001794644631445408 epoch 97 , loss 0.00017746571393217891 epoch 98 , loss 0.00017548113828524947 epoch 99 , loss 0.00017352371651213616 epoch 100 , loss 0.00017157981346827 Looking at predicted values # Purely inference predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () predicted array ([[ 0.9756333 ], [ 2.9791424 ], [ 4.982651 ], [ 6.9861603 ], [ 8.98967 ], [ 10.993179 ], [ 12.996688 ], [ 15.000196 ], [ 17.003706 ], [ 19.007215 ], [ 21.010725 ]], dtype = float32 ) Looking at training values These are the true values, you can see how it's able to predict similar values. # y = 2x + 1 y_train array ([[ 1. ], [ 3. ], [ 5. ], [ 7. ], [ 9. ], [ 11. ], [ 13. ], [ 15. ], [ 17. ], [ 19. ], [ 21. ]], dtype = float32 ) Plot of predicted and actual values # Clear figure plt . clf () # Get predictions predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () # Plot true data plt . plot ( x_train , y_train , 'go' , label = 'True data' , alpha = 0.5 ) # Plot predictions plt . plot ( x_train , predicted , '--' , label = 'Predictions' , alpha = 0.5 ) # Legend and plot plt . legend ( loc = 'best' ) plt . show () Save Model save_model = False if save_model is True : # Saves only parameters # alpha & beta torch . save ( model . state_dict (), 'awesome_model.pkl' ) Load Model load_model = False if load_model is True : model . load_state_dict ( torch . load ( 'awesome_model.pkl' )) Building a Linear Regression Model with PyTorch (GPU) \u00b6 CPU Summary import torch import torch.nn as nn ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () GPU Summary Just remember always 2 things must be on GPU model tensors with gradients import torch import torch.nn as nn import numpy as np ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable ####################### # USE GPU FOR MODEL # ####################### inputs = torch . from_numpy ( x_train ) . to ( device ) labels = torch . from_numpy ( y_train ) . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () # Logging print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 336.0314025878906 epoch 2 , loss 27.67657470703125 epoch 3 , loss 2.5220539569854736 epoch 4 , loss 0.46732547879219055 epoch 5 , loss 0.2968060076236725 epoch 6 , loss 0.2800087630748749 epoch 7 , loss 0.27578213810920715 epoch 8 , loss 0.2726128399372101 epoch 9 , loss 0.269561231136322 epoch 10 , loss 0.2665504515171051 epoch 11 , loss 0.2635740041732788 epoch 12 , loss 0.26063060760498047 epoch 13 , loss 0.2577202618122101 epoch 14 , loss 0.2548423111438751 epoch 15 , loss 0.25199657678604126 epoch 16 , loss 0.24918246269226074 epoch 17 , loss 0.24639996886253357 epoch 18 , loss 0.24364829063415527 epoch 19 , loss 0.24092751741409302 epoch 20 , loss 0.2382371574640274 epoch 21 , loss 0.23557686805725098 epoch 22 , loss 0.2329462170600891 epoch 23 , loss 0.2303449958562851 epoch 24 , loss 0.22777271270751953 epoch 25 , loss 0.2252292037010193 epoch 26 , loss 0.22271405160427094 epoch 27 , loss 0.22022713720798492 epoch 28 , loss 0.21776780486106873 epoch 29 , loss 0.21533599495887756 epoch 30 , loss 0.21293145418167114 epoch 31 , loss 0.21055366098880768 epoch 32 , loss 0.20820240676403046 epoch 33 , loss 0.2058774083852768 epoch 34 , loss 0.20357847213745117 epoch 35 , loss 0.20130516588687897 epoch 36 , loss 0.1990572065114975 epoch 37 , loss 0.19683438539505005 epoch 38 , loss 0.19463638961315155 epoch 39 , loss 0.19246290624141693 epoch 40 , loss 0.1903136670589447 epoch 41 , loss 0.1881885528564453 epoch 42 , loss 0.18608702719211578 epoch 43 , loss 0.18400898575782776 epoch 44 , loss 0.18195408582687378 epoch 45 , loss 0.17992223799228668 epoch 46 , loss 0.17791320383548737 epoch 47 , loss 0.17592646181583405 epoch 48 , loss 0.17396186292171478 epoch 49 , loss 0.17201924324035645 epoch 50 , loss 0.17009828984737396 epoch 51 , loss 0.16819894313812256 epoch 52 , loss 0.16632060706615448 epoch 53 , loss 0.16446338593959808 epoch 54 , loss 0.16262666881084442 epoch 55 , loss 0.16081078350543976 epoch 56 , loss 0.15901507437229156 epoch 57 , loss 0.15723931789398193 epoch 58 , loss 0.15548335015773773 epoch 59 , loss 0.15374726057052612 epoch 60 , loss 0.1520303338766098 epoch 61 , loss 0.15033268928527832 epoch 62 , loss 0.14865389466285706 epoch 63 , loss 0.14699392020702362 epoch 64 , loss 0.14535246789455414 epoch 65 , loss 0.14372935891151428 epoch 66 , loss 0.14212435483932495 epoch 67 , loss 0.14053721725940704 epoch 68 , loss 0.13896773755550385 epoch 69 , loss 0.1374160647392273 epoch 70 , loss 0.1358814686536789 epoch 71 , loss 0.13436420261859894 epoch 72 , loss 0.13286370038986206 epoch 73 , loss 0.1313801407814026 epoch 74 , loss 0.12991292774677277 epoch 75 , loss 0.12846232950687408 epoch 76 , loss 0.1270277351140976 epoch 77 , loss 0.12560924887657166 epoch 78 , loss 0.12420656532049179 epoch 79 , loss 0.12281957268714905 epoch 80 , loss 0.1214480847120285 epoch 81 , loss 0.12009195983409882 epoch 82 , loss 0.1187509223818779 epoch 83 , loss 0.11742479354143143 epoch 84 , loss 0.11611353605985641 epoch 85 , loss 0.11481687426567078 epoch 86 , loss 0.11353478580713272 epoch 87 , loss 0.11226697266101837 epoch 88 , loss 0.11101329326629639 epoch 89 , loss 0.10977360606193542 epoch 90 , loss 0.10854770988225937 epoch 91 , loss 0.10733554512262344 epoch 92 , loss 0.10613703727722168 epoch 93 , loss 0.10495180636644363 epoch 94 , loss 0.10377981513738632 epoch 95 , loss 0.10262089222669601 epoch 96 , loss 0.10147502273321152 epoch 97 , loss 0.1003417894244194 epoch 98 , loss 0.09922132641077042 epoch 99 , loss 0.0981132984161377 epoch 100 , loss 0.09701769798994064 Summary \u00b6 We've learnt to... Success Simple linear regression basics y = Ax + B y = Ax + B y = 2x + 1 y = 2x + 1 Example of simple linear regression Aim of linear regression Minimizing distance between the points and the line Calculate \"distance\" through MSE Calculate gradients Update parameters with parameters = parameters - learning_rate * gradients Slowly update parameters A A and B B model the linear relationship between y y and x x of the form y = 2x + 1 y = 2x + 1 Built a linear regression model in CPU and GPU Step 1: Create Model Class Step 2: Instantiate Model Class Step 3: Instantiate Loss Class Step 4: Instantiate Optimizer Class Step 5: Train Model Important things to be on GPU model tensors with gradients How to bring to GPU ? model_name.to(device) variable_name.to(device)","title":"PyTorch Fundamentals - Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#linear-regression-with-pytorch","text":"","title":"Linear Regression with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#about-linear-regression","text":"","title":"About Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#simple-linear-regression-basics","text":"Allows us to understand relationship between two continuous variables Example x: independent variable weight y: dependent variable height y = \\alpha x + \\beta y = \\alpha x + \\beta","title":"Simple Linear Regression Basics"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example-of-simple-linear-regression","text":"Create plot for simple linear regression Take note that this code is not important at all. It simply creates random data points and does a simple best-fit line to best approximate the underlying function if one even exists. import numpy as np import matplotlib.pyplot as plt % matplotlib inline # Creates 50 random x and y numbers np . random . seed ( 1 ) n = 50 x = np . random . randn ( n ) y = x * np . random . randn ( n ) # Makes the dots colorful colors = np . random . rand ( n ) # Plots best-fit line via polyfit plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) # Plots the random x and y data points we created # Interestingly, alpha makes it more aesthetically pleasing plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ()","title":"Example of simple linear regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#aim-of-linear-regression","text":"Minimize the distance between the points and the line ( y = \\alpha x + \\beta y = \\alpha x + \\beta ) Adjusting Coefficient: \\alpha \\alpha Bias/intercept: \\beta \\beta","title":"Aim of Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch","text":"","title":"Building a Linear Regression Model with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#example","text":"Coefficient: \\alpha = 2 \\alpha = 2 Bias/intercept: \\beta = 1 \\beta = 1 Equation: y = 2x + 1 y = 2x + 1","title":"Example"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-toy-dataset","text":"Create a list of values from 0 to 11 x_values = [ i for i in range ( 11 )] x_values [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Convert list of numbers to numpy array # Convert to numpy x_train = np . array ( x_values , dtype = np . float32 ) x_train . shape ( 11 ,) Convert to 2-dimensional array If you don't this you will get an error stating you need 2D. Simply just reshape accordingly if you ever face such errors down the road. # IMPORTANT: 2D required x_train = x_train . reshape ( - 1 , 1 ) x_train . shape ( 11 , 1 ) Create list of y values We want y values for every x value we have above. y = 2x + 1 y = 2x + 1 y_values = [ 2 * i + 1 for i in x_values ] y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Alternative to create list of y values If you're weak in list iterators, this might be an easier alternative. # In case you're weak in list iterators... y_values = [] for i in x_values : result = 2 * i + 1 y_values . append ( result ) y_values [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 , 19 , 21 ] Convert to numpy array You will slowly get a hang on how when you deal with PyTorch tensors, you just keep on making sure your raw data is in numpy form to make sure everything's good. y_train = np . array ( y_values , dtype = np . float32 ) y_train . shape ( 11 ,) Reshape y numpy array to 2-dimension # IMPORTANT: 2D required y_train = y_train . reshape ( - 1 , 1 ) y_train . shape ( 11 , 1 )","title":"Building a Toy Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-model","text":"Critical Imports import torch import torch.nn as nn Create Model Linear model True Equation: y = 2x + 1 y = 2x + 1 Forward Example Input x = 1 x = 1 Output \\hat y = ? \\hat y = ? # Create class class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Instantiate Model Class input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21] input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) Instantiate Loss Class MSE Loss: Mean Squared Error MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i) \\hat y \\hat y : prediction y y : true value criterion = nn . MSELoss () Instantiate Optimizer Class Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients parameters: \\alpha \\alpha and \\beta \\beta in y = \\alpha x + \\beta y = \\alpha x + \\beta desired parameters: \\alpha = 2 \\alpha = 2 and \\beta = 1 \\beta = 1 in $ y = 2x + 1$ learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Train Model 1 epoch: going through the whole x_train data once 100 epochs: 100x mapping x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 140.58143615722656 epoch 2 , loss 11.467253684997559 epoch 3 , loss 0.9358152747154236 epoch 4 , loss 0.07679400593042374 epoch 5 , loss 0.0067212567664682865 epoch 6 , loss 0.0010006226366385818 epoch 7 , loss 0.0005289533291943371 epoch 8 , loss 0.0004854927829001099 epoch 9 , loss 0.00047700389404781163 epoch 10 , loss 0.0004714332753792405 epoch 11 , loss 0.00046614606981165707 epoch 12 , loss 0.0004609318566508591 epoch 13 , loss 0.0004557870561257005 epoch 14 , loss 0.00045069155748933554 epoch 15 , loss 0.00044567222357727587 epoch 16 , loss 0.00044068993884138763 epoch 17 , loss 0.00043576463940553367 epoch 18 , loss 0.00043090470717288554 epoch 19 , loss 0.00042609183583408594 epoch 20 , loss 0.0004213254142086953 epoch 21 , loss 0.0004166301223449409 epoch 22 , loss 0.0004119801160413772 epoch 23 , loss 0.00040738462121225893 epoch 24 , loss 0.0004028224211651832 epoch 25 , loss 0.0003983367350883782 epoch 26 , loss 0.0003938761365134269 epoch 27 , loss 0.000389480876037851 epoch 28 , loss 0.00038514015614055097 epoch 29 , loss 0.000380824290914461 epoch 30 , loss 0.00037657516077160835 epoch 31 , loss 0.000372376263840124 epoch 32 , loss 0.0003682126116473228 epoch 33 , loss 0.0003640959912445396 epoch 34 , loss 0.00036003670538775623 epoch 35 , loss 0.00035601368290372193 epoch 36 , loss 0.00035203873994760215 epoch 37 , loss 0.00034810820943675935 epoch 38 , loss 0.000344215368386358 epoch 39 , loss 0.0003403784066904336 epoch 40 , loss 0.00033658024040050805 epoch 41 , loss 0.0003328165039420128 epoch 42 , loss 0.0003291067841928452 epoch 43 , loss 0.0003254293987993151 epoch 44 , loss 0.0003217888588551432 epoch 45 , loss 0.0003182037326041609 epoch 46 , loss 0.0003146533854305744 epoch 47 , loss 0.00031113551813177764 epoch 48 , loss 0.0003076607536058873 epoch 49 , loss 0.00030422292184084654 epoch 50 , loss 0.00030083119054324925 epoch 51 , loss 0.00029746422660537064 epoch 52 , loss 0.0002941471466328949 epoch 53 , loss 0.00029085995629429817 epoch 54 , loss 0.0002876132493838668 epoch 55 , loss 0.00028440452297218144 epoch 56 , loss 0.00028122696676291525 epoch 57 , loss 0.00027808290906250477 epoch 58 , loss 0.00027497278642840683 epoch 59 , loss 0.00027190230321139097 epoch 60 , loss 0.00026887087733484805 epoch 61 , loss 0.0002658693410921842 epoch 62 , loss 0.0002629039518069476 epoch 63 , loss 0.00025996880140155554 epoch 64 , loss 0.0002570618235040456 epoch 65 , loss 0.00025419273879379034 epoch 66 , loss 0.00025135406758636236 epoch 67 , loss 0.0002485490695107728 epoch 68 , loss 0.0002457649679854512 epoch 69 , loss 0.0002430236927466467 epoch 70 , loss 0.00024031475186347961 epoch 71 , loss 0.00023762597993481904 epoch 72 , loss 0.00023497406800743192 epoch 73 , loss 0.0002323519001947716 epoch 74 , loss 0.00022976362379267812 epoch 75 , loss 0.0002271933335578069 epoch 76 , loss 0.00022465786605607718 epoch 77 , loss 0.00022214400814846158 epoch 78 , loss 0.00021966728672850877 epoch 79 , loss 0.0002172116219298914 epoch 80 , loss 0.00021478648704942316 epoch 81 , loss 0.00021239375928416848 epoch 82 , loss 0.0002100227284245193 epoch 83 , loss 0.00020767028036061674 epoch 84 , loss 0.00020534756185952574 epoch 85 , loss 0.00020305956422816962 epoch 86 , loss 0.0002007894654525444 epoch 87 , loss 0.00019854879064951092 epoch 88 , loss 0.00019633043848443776 epoch 89 , loss 0.00019413618429098278 epoch 90 , loss 0.00019197272195015103 epoch 91 , loss 0.0001898303598864004 epoch 92 , loss 0.00018771187751553953 epoch 93 , loss 0.00018561164324637502 epoch 94 , loss 0.00018354636267758906 epoch 95 , loss 0.00018149390234611928 epoch 96 , loss 0.0001794644631445408 epoch 97 , loss 0.00017746571393217891 epoch 98 , loss 0.00017548113828524947 epoch 99 , loss 0.00017352371651213616 epoch 100 , loss 0.00017157981346827 Looking at predicted values # Purely inference predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () predicted array ([[ 0.9756333 ], [ 2.9791424 ], [ 4.982651 ], [ 6.9861603 ], [ 8.98967 ], [ 10.993179 ], [ 12.996688 ], [ 15.000196 ], [ 17.003706 ], [ 19.007215 ], [ 21.010725 ]], dtype = float32 ) Looking at training values These are the true values, you can see how it's able to predict similar values. # y = 2x + 1 y_train array ([[ 1. ], [ 3. ], [ 5. ], [ 7. ], [ 9. ], [ 11. ], [ 13. ], [ 15. ], [ 17. ], [ 19. ], [ 21. ]], dtype = float32 ) Plot of predicted and actual values # Clear figure plt . clf () # Get predictions predicted = model ( torch . from_numpy ( x_train ) . requires_grad_ ()) . data . numpy () # Plot true data plt . plot ( x_train , y_train , 'go' , label = 'True data' , alpha = 0.5 ) # Plot predictions plt . plot ( x_train , predicted , '--' , label = 'Predictions' , alpha = 0.5 ) # Legend and plot plt . legend ( loc = 'best' ) plt . show () Save Model save_model = False if save_model is True : # Saves only parameters # alpha & beta torch . save ( model . state_dict (), 'awesome_model.pkl' ) Load Model load_model = False if load_model is True : model . load_state_dict ( torch . load ( 'awesome_model.pkl' ))","title":"Building Model"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#building-a-linear-regression-model-with-pytorch-gpu","text":"CPU Summary import torch import torch.nn as nn ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable inputs = torch . from_numpy ( x_train ) . requires_grad_ () labels = torch . from_numpy ( y_train ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () GPU Summary Just remember always 2 things must be on GPU model tensors with gradients import torch import torch.nn as nn import numpy as np ''' STEP 1: CREATE MODEL CLASS ''' class LinearRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LinearRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 2: INSTANTIATE MODEL CLASS ''' input_dim = 1 output_dim = 1 model = LinearRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 3: INSTANTIATE LOSS CLASS ''' criterion = nn . MSELoss () ''' STEP 4: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.01 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 5: TRAIN THE MODEL ''' epochs = 100 for epoch in range ( epochs ): epoch += 1 # Convert numpy array to torch Variable ####################### # USE GPU FOR MODEL # ####################### inputs = torch . from_numpy ( x_train ) . to ( device ) labels = torch . from_numpy ( y_train ) . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward to get output outputs = model ( inputs ) # Calculate Loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () # Logging print ( 'epoch {}, loss {}' . format ( epoch , loss . item ())) epoch 1 , loss 336.0314025878906 epoch 2 , loss 27.67657470703125 epoch 3 , loss 2.5220539569854736 epoch 4 , loss 0.46732547879219055 epoch 5 , loss 0.2968060076236725 epoch 6 , loss 0.2800087630748749 epoch 7 , loss 0.27578213810920715 epoch 8 , loss 0.2726128399372101 epoch 9 , loss 0.269561231136322 epoch 10 , loss 0.2665504515171051 epoch 11 , loss 0.2635740041732788 epoch 12 , loss 0.26063060760498047 epoch 13 , loss 0.2577202618122101 epoch 14 , loss 0.2548423111438751 epoch 15 , loss 0.25199657678604126 epoch 16 , loss 0.24918246269226074 epoch 17 , loss 0.24639996886253357 epoch 18 , loss 0.24364829063415527 epoch 19 , loss 0.24092751741409302 epoch 20 , loss 0.2382371574640274 epoch 21 , loss 0.23557686805725098 epoch 22 , loss 0.2329462170600891 epoch 23 , loss 0.2303449958562851 epoch 24 , loss 0.22777271270751953 epoch 25 , loss 0.2252292037010193 epoch 26 , loss 0.22271405160427094 epoch 27 , loss 0.22022713720798492 epoch 28 , loss 0.21776780486106873 epoch 29 , loss 0.21533599495887756 epoch 30 , loss 0.21293145418167114 epoch 31 , loss 0.21055366098880768 epoch 32 , loss 0.20820240676403046 epoch 33 , loss 0.2058774083852768 epoch 34 , loss 0.20357847213745117 epoch 35 , loss 0.20130516588687897 epoch 36 , loss 0.1990572065114975 epoch 37 , loss 0.19683438539505005 epoch 38 , loss 0.19463638961315155 epoch 39 , loss 0.19246290624141693 epoch 40 , loss 0.1903136670589447 epoch 41 , loss 0.1881885528564453 epoch 42 , loss 0.18608702719211578 epoch 43 , loss 0.18400898575782776 epoch 44 , loss 0.18195408582687378 epoch 45 , loss 0.17992223799228668 epoch 46 , loss 0.17791320383548737 epoch 47 , loss 0.17592646181583405 epoch 48 , loss 0.17396186292171478 epoch 49 , loss 0.17201924324035645 epoch 50 , loss 0.17009828984737396 epoch 51 , loss 0.16819894313812256 epoch 52 , loss 0.16632060706615448 epoch 53 , loss 0.16446338593959808 epoch 54 , loss 0.16262666881084442 epoch 55 , loss 0.16081078350543976 epoch 56 , loss 0.15901507437229156 epoch 57 , loss 0.15723931789398193 epoch 58 , loss 0.15548335015773773 epoch 59 , loss 0.15374726057052612 epoch 60 , loss 0.1520303338766098 epoch 61 , loss 0.15033268928527832 epoch 62 , loss 0.14865389466285706 epoch 63 , loss 0.14699392020702362 epoch 64 , loss 0.14535246789455414 epoch 65 , loss 0.14372935891151428 epoch 66 , loss 0.14212435483932495 epoch 67 , loss 0.14053721725940704 epoch 68 , loss 0.13896773755550385 epoch 69 , loss 0.1374160647392273 epoch 70 , loss 0.1358814686536789 epoch 71 , loss 0.13436420261859894 epoch 72 , loss 0.13286370038986206 epoch 73 , loss 0.1313801407814026 epoch 74 , loss 0.12991292774677277 epoch 75 , loss 0.12846232950687408 epoch 76 , loss 0.1270277351140976 epoch 77 , loss 0.12560924887657166 epoch 78 , loss 0.12420656532049179 epoch 79 , loss 0.12281957268714905 epoch 80 , loss 0.1214480847120285 epoch 81 , loss 0.12009195983409882 epoch 82 , loss 0.1187509223818779 epoch 83 , loss 0.11742479354143143 epoch 84 , loss 0.11611353605985641 epoch 85 , loss 0.11481687426567078 epoch 86 , loss 0.11353478580713272 epoch 87 , loss 0.11226697266101837 epoch 88 , loss 0.11101329326629639 epoch 89 , loss 0.10977360606193542 epoch 90 , loss 0.10854770988225937 epoch 91 , loss 0.10733554512262344 epoch 92 , loss 0.10613703727722168 epoch 93 , loss 0.10495180636644363 epoch 94 , loss 0.10377981513738632 epoch 95 , loss 0.10262089222669601 epoch 96 , loss 0.10147502273321152 epoch 97 , loss 0.1003417894244194 epoch 98 , loss 0.09922132641077042 epoch 99 , loss 0.0981132984161377 epoch 100 , loss 0.09701769798994064","title":"Building a Linear Regression Model with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_linear_regression/#summary","text":"We've learnt to... Success Simple linear regression basics y = Ax + B y = Ax + B y = 2x + 1 y = 2x + 1 Example of simple linear regression Aim of linear regression Minimizing distance between the points and the line Calculate \"distance\" through MSE Calculate gradients Update parameters with parameters = parameters - learning_rate * gradients Slowly update parameters A A and B B model the linear relationship between y y and x x of the form y = 2x + 1 y = 2x + 1 Built a linear regression model in CPU and GPU Step 1: Create Model Class Step 2: Instantiate Model Class Step 3: Instantiate Loss Class Step 4: Instantiate Optimizer Class Step 5: Train Model Important things to be on GPU model tensors with gradients How to bring to GPU ? model_name.to(device) variable_name.to(device)","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/","text":"Logistic Regression with PyTorch \u00b6 About Logistic Regression \u00b6 Logistic Regression Basics \u00b6 Classification algorithm \u00b6 Example: Spam vs No Spam Input: Bunch of words Output: Probability spam or not Basic Comparison \u00b6 Linear regression Output: numeric value given inputs Logistic regression : Output: probability [0, 1] given input belonging to a class Input/Output Comparison \u00b6 Linear regression: Multiplication Input: [1] Output: 2 Input: [2] Output: 4 Trying to model the relationship y = 2x Logistic regression: Spam Input: \"Sign up to get 1 million dollars by tonight\" Output: p = 0.8 Input: \"This is a receipt for your recent purchase with Amazon\" Output: p = 0.3 p: probability it is spam Problems of Linear Regression \u00b6 Example Fever Input : temperature Output : fever or no fever Remember Linear regression : minimize error between points and line Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1) If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import numpy as np import matplotlib.pyplot as plt % matplotlib inline x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 100 ,] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Linear Regression Problem 2: Fever points are not predicted with the presence of outliers Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import numpy as np import matplotlib.pyplot as plt x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 300 ] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Logistic Regression In-Depth \u00b6 Predicting Probability \u00b6 Linear regression doesn't work Instead of predicting direct values: predict probability Logistic Function g() \u00b6 Two-class logistic regression y = A x + b y = A x + b g(y) = A x + b g(y) = A x + b g(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}} g(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}} g(y) g(y) = Estimated probability that y = 1 y = 1 given x x Softmax Function g() \u00b6 Multi-class logistic regression Generalization of logistic function Cross Entropy Function D() \u00b6 D(S, L) = L log S - (1-L)log(1-S) D(S, L) = L log S - (1-L)log(1-S) If L = 0 (label) D(S, 0) = - log(1-S) D(S, 0) = - log(1-S) - log(1-S) - log(1-S) : less positive if S \\longrightarrow 0 S \\longrightarrow 0 - log(1-S) - log(1-S) : more positive if S \\longrightarrow 1 S \\longrightarrow 1 (BIGGER LOSS) If L = 1 (label) D(S, 1) = log S D(S, 1) = log S logS logS : less negative if S \\longrightarrow 1 S \\longrightarrow 1 logS logS : more negative if S \\longrightarrow 0 S \\longrightarrow 0 (BIGGER LOSS) Numerical example of bigger or small loss You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import math print ( - math . log ( 1 - 0.00001 )) You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1 - 0.99999 )) You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( math . log ( 0.99999 )) You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( math . log ( 0.00001 )) 1.0000050000287824e-05 11.51292546497478 - 1.0000050000287824e-05 - 11.512925464970229 Cross Entropy Loss L \u00b6 Goal: Minimizing Cross Entropy Loss L = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i) L = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i) Building a Logistic Regression Model with PyTorch \u00b6 Steps \u00b6 Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Step 1a: Loading MNIST Train Dataset \u00b6 Images from 1 to 9 Inspect length of training dataset You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) len ( train_dataset ) 60000 Inspecting a single image So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ] ( tensor ([[[ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0118 , 0.0706 , 0.0706 , 0.0706 , 0.4941 , 0.5333 , 0.6863 , 0.1020 , 0.6510 , 1.0000 , 0.9686 , 0.4980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1176 , 0.1412 , 0.3686 , 0.6039 , 0.6667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.8824 , 0.6745 , 0.9922 , 0.9490 , 0.7647 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1922 , 0.9333 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9843 , 0.3647 , 0.3216 , 0.3216 , 0.2196 , 0.1529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.7137 , 0.9686 , 0.9451 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3137 , 0.6118 , 0.4196 , 0.9922 , 0.9922 , 0.8039 , 0.0431 , 0.0000 , 0.1686 , 0.6039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0549 , 0.0039 , 0.6039 , 0.9922 , 0.3529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5451 , 0.9922 , 0.7451 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0431 , 0.7451 , 0.9922 , 0.2745 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1373 , 0.9451 , 0.8824 , 0.6275 , 0.4235 , 0.0039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3176 , 0.9412 , 0.9922 , 0.9922 , 0.4667 , 0.0980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1765 , 0.7294 , 0.9922 , 0.9922 , 0.5882 , 0.1059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0627 , 0.3647 , 0.9882 , 0.9922 , 0.7333 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.9765 , 0.9922 , 0.9765 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1804 , 0.5098 , 0.7176 , 0.9922 , 0.9922 , 0.8118 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1529 , 0.5804 , 0.8980 , 0.9922 , 0.9922 , 0.9922 , 0.9804 , 0.7137 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0941 , 0.4471 , 0.8667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7882 , 0.3059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0902 , 0.2588 , 0.8353 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.3176 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.6706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7647 , 0.3137 , 0.0353 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.2157 , 0.6745 , 0.8863 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9569 , 0.5216 , 0.0431 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5333 , 0.9922 , 0.9922 , 0.9922 , 0.8314 , 0.5294 , 0.5176 , 0.0627 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ]]]), tensor ( 5 )) Inspecting a single data point in the training dataset When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label. type ( train_dataset [ 0 ]) tuple Inspecting training dataset first element of tuple This means to access the image, you need to access the first element in the tuple. # Input Matrix train_dataset [ 0 ][ 0 ] . size () # A 28x28 sized image of a digit torch . Size ([ 1 , 28 , 28 ]) Inspecting training dataset second element of tuple The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Displaying MNIST \u00b6 Verifying shape of MNIST image As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel. import matplotlib.pyplot as plt % matplotlib inline import numpy as np train_dataset [ 0 ][ 0 ] . numpy () . shape ( 1 , 28 , 28 ) Plot image of MNIST image show_img = train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label As you would expect, the label is 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Plot second image of MNIST image show_img = train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label We should see 0 here as the label. # Label train_dataset [ 1 ][ 1 ] tensor ( 0 ) Step 1b: Loading MNIST Test Dataset \u00b6 Show our algorithm works beyond the data we have trained on. Out-of-sample Load test dataset Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) len ( test_dataset ) 10000 Test dataset elements Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ]) tuple Test dataset first element in tuple This contains the image matrix, similar to the training set. # Image matrix test_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Plot image sample from test dataset show_img = test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Test dataset second element in tuple # Label test_dataset [ 0 ][ 1 ] tensor ( 7 ) Step 2: Make Dataset Iterable \u00b6 Aim: make the dataset iterable totaldata : 60000 minibatch : 100 Number of examples in 1 iteration iterations : 3000 1 iteration: one mini-batch forward & backward pass epochs 1 epoch: running through the whole dataset once $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 $ Recap training dataset Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset ) 60000 Defining epochs When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. batch_size = 100 We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters = 3000 One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) num_epochs 5 Create Iterable Object: Training Dataset train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) Check Iterability import collections isinstance ( train_loader , collections . Iterable ) True Create Iterable Object: Testing Dataset # Iterable object test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Check iterability of testing dataset isinstance ( test_loader , collections . Iterable ) True Iterate through dataset This is just a simplified example of what we're doing above where we're creating an iterable object lst to loop through so we can access all the images img_1 and img_2 . Above, the equivalent of lst is train_loader and test_loader . img_1 = np . ones (( 28 , 28 )) img_2 = np . ones (( 28 , 28 )) lst = [ img_1 , img_2 ] # Need to iterate # Think of numbers as the images for i in lst : print ( i . shape ) ( 28 , 28 ) ( 28 , 28 ) Step 3: Building Model \u00b6 Create model class # Same as linear regression! class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out Step 4: Instantiate Model Class \u00b6 Input dimension: Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Check size of dataset This should be 28x28. # Size of images train_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Instantiate model class based on input and out dimensions As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) Step 5: Instantiate Loss Class \u00b6 Logistic Regression : Cross Entropy Loss Linear Regression: MSE Create Cross Entry Loss Class Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters. criterion = nn . CrossEntropyLoss () What happens in nn.CrossEntropyLoss()? It does 2 things at the same time. 1. Computes softmax (logistic/softmax function) 2. Computes cross entropy Step 6: Instantiate Optimizer Class \u00b6 Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Create optimizer Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the y = AX + b y = AX + b equation where X is our input of size 784. We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. # Type of parameter object print ( model . parameters ()) # Length of parameters print ( len ( list ( model . parameters ()))) # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) < generator object Module . parameters at 0x7ff7c884f830 > 2 torch . Size ([ 10 , 784 ]) torch . Size ([ 10 ]) Quick Dot Product Review Example 1: dot product A: (100, 10) A: (100, 10) B: (10, 1) B: (10, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) Example 2: dot product A: (50, 5) A: (50, 5) B: (5, 2) B: (5, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) Example 3: element-wise addition A: (10, 1) A: (10, 1) B: (10, 1) B: (10, 1) A + B = (10, 1) A + B = (10, 1) Step 7: Train Model \u00b6 7 step process for training models Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8513233661651611 . Accuracy : 70 Iteration : 1000. Loss : 1.5732524394989014 . Accuracy : 77 Iteration : 1500. Loss : 1.3840199708938599 . Accuracy : 79 Iteration : 2000. Loss : 1.1711134910583496 . Accuracy : 81 Iteration : 2500. Loss : 1.1094708442687988 . Accuracy : 82 Iteration : 3000. Loss : 1.002761721611023 . Accuracy : 82 Break Down Accuracy Calculation \u00b6 Printing outputs of our model As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model. This would print out the output of the model's predictions on your notebook. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs ) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor ([[ - 0.4181 , - 1.0784 , - 0.4840 , - 0.0985 , - 0.2394 , - 0.1801 , - 1.1639 , 2.9352 , - 0.1552 , 0.8852 ], [ 0.5117 , - 0.1099 , 1.5295 , 0.8863 , - 1.8813 , 0.5967 , 1.3632 , - 1.8977 , 0.4183 , - 1.4990 ], [ - 1.0126 , 2.4112 , 0.2373 , 0.0857 , - 0.7007 , - 0.2015 , - 0.3428 , - 0.2548 , 0.1659 , - 0.4703 ], [ 2.8072 , - 2.2973 , - 0.0984 , - 0.4313 , - 0.9619 , 0.8670 , 1.2201 , 0.3752 , - 0.2873 , - 0.3272 ], [ - 0.0343 , - 2.0043 , 0.5081 , - 0.6452 , 1.8647 , - 0.6924 , 0.1435 , 0.4330 , 0.2958 , 1.0339 ], [ - 1.5392 , 2.9070 , 0.2297 , 0.3139 , - 0.6863 , - 0.2734 , - 0.8377 , - 0.1238 , 0.3285 , - 0.3004 ], [ - 1.2037 , - 1.3739 , - 0.5947 , 0.3530 , 1.4205 , 0.0593 , - 0.7307 , 0.6642 , 0.3937 , 0.8004 ], [ - 1.4439 , - 0.3284 , - 0.7652 , - 0.0952 , 0.9323 , 0.3006 , 0.0238 , - 0.0810 , 0.0612 , 1.3295 ], [ 0.5409 , - 0.5266 , 0.9914 , - 1.2369 , 0.6583 , 0.0992 , 0.8525 , - 1.0562 , 0.2013 , 0.0462 ], [ - 0.6548 , - 0.7253 , - 0.9825 , - 1.1663 , 0.9076 , - 0.0694 , - 0.3708 , 1.8270 , 0.2457 , 1.5921 ], [ 3.2147 , - 1.7689 , 0.8531 , 1.2320 , - 0.8126 , 1.1251 , - 0.2776 , - 1.4244 , 0.5930 , - 1.6183 ], [ 0.7470 , - 0.5545 , 1.0251 , 0.0529 , 0.4384 , - 0.5934 , 0.7666 , - 1.0084 , 0.5313 , - 0.3465 ], [ - 0.7916 , - 1.7064 , - 0.7805 , - 1.1588 , 1.3284 , - 0.1708 , - 0.2092 , 0.9495 , 0.1033 , 2.0208 ], [ 3.0602 , - 2.3578 , - 0.2576 , - 0.2198 , - 0.2372 , 0.9765 , - 0.1514 , - 0.5380 , 0.7970 , 0.1374 ], [ - 1.2613 , 2.8594 , - 0.0874 , 0.1974 , - 1.2018 , - 0.0064 , - 0.0923 , - 0.2142 , 0.2575 , - 0.3218 ], [ 0.4348 , - 0.7216 , 0.0021 , 1.2864 , - 0.5062 , 0.7761 , - 0.3236 , - 0.5667 , 0.5431 , - 0.7781 ], [ - 0.2157 , - 2.0200 , 0.1829 , - 0.6882 , 1.3815 , - 0.7609 , - 0.0902 , 0.8647 , 0.3679 , 1.8843 ], [ 0.0950 , - 1.5009 , - 0.6347 , 0.3662 , - 0.4679 , - 0.0359 , - 0.7671 , 2.7155 , - 0.3991 , 0.5737 ], [ - 0.7005 , - 0.5366 , - 0.0434 , 1.1289 , - 0.5873 , 0.2555 , 0.8187 , - 0.6557 , 0.1241 , - 0.4297 ], [ - 1.0635 , - 1.5991 , - 0.4677 , - 0.1231 , 2.0445 , 0.1128 , - 0.1825 , 0.1075 , 0.0348 , 1.4317 ], [ - 1.0319 , - 0.1595 , - 1.3415 , 0.1095 , 0.5339 , 0.1973 , - 1.3272 , 1.5765 , 0.4784 , 1.4176 ], [ - 0.4928 , - 1.5653 , - 0.0672 , 0.3325 , 0.5359 , 0.5368 , 2.1542 , - 1.4276 , 0.3605 , 0.0587 ], [ - 0.4761 , 0.2958 , 0.6597 , - 0.2658 , 1.1279 , - 1.0676 , 1.2506 , - 0.2059 , - 0.1489 , 0.1051 ], [ - 0.0764 , - 0.9274 , - 0.6838 , 0.3464 , - 0.2656 , 1.4099 , 0.4486 , - 0.9527 , 0.5682 , 0.0156 ], [ - 0.6900 , - 0.9611 , 0.1395 , - 0.0079 , 1.5424 , - 0.3208 , - 0.2682 , 0.3586 , - 0.2771 , 1.0389 ], [ 4.3606 , - 2.8621 , 0.6310 , - 0.9657 , - 0.2486 , 1.2009 , 1.1873 , - 0.8255 , - 0.2103 , - 1.2172 ], [ - 0.1000 , - 1.4268 , - 0.4627 , - 0.1041 , 0.2959 , - 0.1392 , - 0.6855 , 1.8622 , - 0.2580 , 1.1347 ], [ - 0.3625 , - 2.1323 , - 0.2224 , - 0.8754 , 2.4684 , 0.0295 , 0.1161 , - 0.2660 , 0.3037 , 1.4570 ], [ 2.8688 , - 2.4517 , 0.1782 , 1.1149 , - 1.0898 , 1.1062 , - 0.0681 , - 0.5697 , 0.8888 , - 0.6965 ], [ - 1.0429 , 1.4446 , - 0.3349 , 0.1254 , - 0.5017 , 0.2286 , 0.2328 , - 0.3290 , 0.3949 , - 0.2586 ], [ - 0.8476 , - 0.0004 , - 1.1003 , 2.2806 , - 1.2226 , 0.9251 , - 0.3165 , 0.4957 , 0.0690 , 0.0232 ], [ - 0.9108 , 1.1355 , - 0.2715 , 0.2233 , - 0.3681 , 0.1442 , - 0.0001 , - 0.0174 , 0.1454 , 0.2286 ], [ - 1.0663 , - 0.8466 , - 0.7147 , 2.5685 , - 0.2090 , 1.2993 , - 0.3057 , - 0.8314 , 0.7046 , - 0.0176 ], [ 1.7013 , - 1.8051 , 0.7541 , - 1.5248 , 0.8972 , 0.1518 , 1.4876 , - 0.8454 , - 0.2022 , - 0.2829 ], [ - 0.8179 , - 0.1239 , 0.8630 , - 0.2137 , - 0.2275 , - 0.5411 , - 1.3448 , 1.7354 , 0.7751 , 0.6234 ], [ 0.6515 , - 1.0431 , 2.7165 , 0.1873 , - 1.0623 , 0.1286 , 0.3597 , - 0.2739 , 0.3871 , - 1.6699 ], [ - 0.2828 , - 1.4663 , 0.1182 , - 0.0896 , - 0.3640 , - 0.5129 , - 0.4905 , 2.2914 , - 0.2227 , 0.9463 ], [ - 1.2596 , 2.0468 , - 0.4405 , - 0.0411 , - 0.8073 , 0.0490 , - 0.0604 , - 0.1206 , 0.3504 , - 0.1059 ], [ 0.6089 , 0.5885 , 0.7898 , 1.1318 , - 1.9008 , 0.5875 , 0.4227 , - 1.1815 , 0.5652 , - 1.3590 ], [ - 1.4551 , 2.9537 , - 0.2805 , 0.2372 , - 1.4180 , 0.0297 , - 0.1515 , - 0.6111 , 0.6140 , - 0.3354 ], [ - 0.7182 , 1.6778 , 0.0553 , 0.0461 , - 0.5446 , - 0.0338 , - 0.0215 , - 0.0881 , 0.1506 , - 0.2107 ], [ - 0.8027 , - 0.7854 , - 0.1275 , - 0.3177 , - 0.1600 , - 0.1964 , - 0.6084 , 2.1285 , - 0.1815 , 1.1911 ], [ - 2.0656 , - 0.4959 , - 0.1154 , - 0.1363 , 2.2426 , - 0.7441 , - 0.8413 , 0.4675 , 0.3269 , 1.7279 ], [ - 0.3004 , 1.0166 , 1.1175 , - 0.0618 , - 0.0937 , - 0.4221 , 0.1943 , - 1.1020 , 0.3670 , - 0.4683 ], [ - 1.0720 , 0.2252 , 0.0175 , 1.3644 , - 0.7409 , 0.4655 , 0.5439 , 0.0380 , 0.1279 , - 0.2302 ], [ 0.2409 , - 1.2622 , - 0.6336 , 1.8240 , - 0.5951 , 1.3408 , 0.2130 , - 1.3789 , 0.8363 , - 0.2101 ], [ - 1.3849 , 0.3773 , - 0.0585 , 0.6896 , - 0.0998 , 0.2804 , 0.0696 , - 0.2529 , 0.3143 , 0.3409 ], [ - 0.9103 , - 0.1578 , 1.6673 , - 0.4817 , 0.4088 , - 0.5484 , 0.6103 , - 0.2287 , - 0.0665 , 0.0055 ], [ - 1.1692 , - 2.8531 , - 1.2499 , - 0.0257 , 2.8580 , 0.2616 , - 0.7122 , - 0.0551 , 0.8112 , 2.3233 ], [ - 0.2790 , - 1.9494 , 0.6096 , - 0.5653 , 2.2792 , - 1.0687 , 0.1634 , 0.3122 , 0.1053 , 1.0884 ], [ 0.1267 , - 1.2297 , - 0.1315 , 0.2428 , - 0.5436 , 0.4123 , 2.3060 , - 0.9278 , - 0.1528 , - 0.4224 ], [ - 0.0235 , - 0.9137 , - 0.1457 , 1.6858 , - 0.7552 , 0.7293 , 0.2510 , - 0.3955 , - 0.2187 , - 0.1505 ], [ 0.5643 , - 1.2783 , - 1.4149 , 0.0304 , 0.8375 , 1.5018 , 0.0338 , - 0.3875 , - 0.0117 , 0.5751 ], [ 0.2926 , - 0.7486 , - 0.3238 , 1.0384 , 0.0308 , 0.6792 , - 0.0170 , - 0.5797 , 0.2819 , - 0.3510 ], [ 0.1219 , - 0.5862 , 1.5817 , - 0.1297 , 0.4730 , - 0.9171 , 0.7886 , - 0.7022 , - 0.0501 , - 0.2812 ], [ 1.7587 , - 2.4511 , - 0.7369 , 0.4082 , - 0.6426 , 1.1784 , 0.6052 , - 0.7178 , 1.6161 , - 0.2220 ], [ - 0.1267 , - 2.6719 , 0.0505 , - 0.4972 , 2.9027 , - 0.1461 , 0.2807 , - 0.2921 , 0.2231 , 1.1327 ], [ - 0.9892 , 2.4401 , 0.1274 , 0.2838 , - 0.7535 , - 0.1684 , - 0.6493 , - 0.1908 , 0.2290 , - 0.2150 ], [ - 0.2071 , - 2.1351 , - 0.9191 , - 0.9309 , 1.7747 , - 0.3046 , 0.0183 , 1.0136 , - 0.1016 , 2.1288 ], [ - 0.0103 , 0.3280 , - 0.6974 , - 0.2504 , 0.3187 , 0.4390 , - 0.1879 , 0.3954 , 0.2332 , - 0.1971 ], [ - 0.2280 , - 1.6754 , - 0.7438 , 0.5078 , 0.2544 , - 0.1020 , - 0.2503 , 2.0799 , - 0.5033 , 0.5890 ], [ 0.3972 , - 0.9369 , 1.2696 , - 1.6713 , - 0.4159 , - 0.0221 , 0.6489 , - 0.4777 , 1.2497 , 0.3931 ], [ - 0.7566 , - 0.8230 , - 0.0785 , - 0.3083 , 0.7821 , 0.1880 , 0.1037 , - 0.0956 , 0.4219 , 1.0798 ], [ - 1.0328 , - 0.1700 , 1.3806 , 0.5445 , - 0.2624 , - 0.0780 , - 0.3595 , - 0.6253 , 0.4309 , 0.1813 ], [ - 1.0360 , - 0.4704 , 0.1948 , - 0.7066 , 0.6600 , - 0.4633 , - 0.3602 , 1.7494 , 0.1522 , 0.6086 ], [ - 1.2032 , - 0.7903 , - 0.5754 , 0.4722 , 0.6068 , 0.5752 , 0.2151 , - 0.2495 , 0.3420 , 0.9278 ], [ 0.2247 , - 0.1361 , 0.9374 , - 0.1543 , 0.4921 , - 0.6553 , 0.5885 , 0.2617 , - 0.2216 , - 0.3736 ], [ - 0.2867 , - 1.4486 , 0.6658 , - 0.8755 , 2.3195 , - 0.7627 , - 0.2132 , 0.2488 , 0.3484 , 1.0860 ], [ - 1.4031 , - 0.4518 , - 0.3181 , 2.8268 , - 0.5371 , 1.0154 , - 0.9247 , - 0.7385 , 1.1031 , 0.0422 ], [ 2.8604 , - 1.5413 , 0.6241 , - 0.8017 , - 1.4104 , 0.6314 , 0.4614 , - 0.0218 , - 0.3411 , - 0.2609 ], [ 0.2113 , - 1.2348 , - 0.8535 , - 0.1041 , - 0.2703 , - 0.1294 , - 0.7057 , 2.7552 , - 0.4429 , 0.4517 ], [ 4.5191 , - 2.7407 , 1.1091 , 0.3975 , - 0.9456 , 1.2277 , 0.3616 , - 1.6564 , 0.5063 , - 1.4274 ], [ 1.4615 , - 1.0765 , 1.8388 , 1.5006 , - 1.2351 , 0.2781 , 0.2830 , - 0.8491 , 0.2222 , - 1.7779 ], [ - 1.2160 , 0.8502 , 0.2413 , - 0.0798 , - 0.7880 , - 0.4286 , - 0.8060 , 0.7194 , 1.2663 , 0.6412 ], [ - 1.3318 , 2.3388 , - 0.4003 , - 0.1094 , - 1.0285 , 0.1021 , - 0.0388 , - 0.0497 , 0.5137 , - 0.2507 ], [ - 1.7853 , 0.5884 , - 0.6108 , - 0.5557 , 0.8696 , - 0.6226 , - 0.7983 , 1.7169 , - 0.0145 , 0.8231 ], [ - 0.1739 , 0.1562 , - 0.2933 , 2.3195 , - 0.9480 , 1.2019 , - 0.4834 , - 1.0567 , 0.5685 , - 0.6841 ], [ - 0.7920 , - 0.3339 , 0.7452 , - 0.6529 , - 0.3307 , - 0.6092 , - 0.0950 , 1.7311 , - 0.3481 , 0.3801 ], [ - 1.7810 , 1.0676 , - 0.7611 , 0.3658 , - 0.0431 , - 0.1012 , - 0.6048 , 0.3089 , 0.9998 , 0.7164 ], [ - 0.5856 , - 0.5261 , - 0.4859 , - 1.0551 , - 0.1838 , - 0.2144 , - 1.2599 , 3.3891 , 0.4691 , 0.7566 ], [ - 0.4984 , - 1.7770 , - 1.1998 , - 0.1075 , 1.0882 , 0.4539 , - 0.5651 , 1.4381 , - 0.5678 , 1.7479 ], [ 0.2938 , - 1.8536 , 0.4259 , - 0.5429 , 0.0066 , 0.4120 , 2.3793 , - 0.3666 , - 0.2604 , 0.0382 ], [ - 0.4080 , - 0.9851 , 4.0264 , 0.1099 , - 0.1766 , - 1.1557 , 0.6419 , - 0.8147 , 0.7535 , - 1.1452 ], [ - 0.4636 , - 1.7323 , - 0.6433 , - 0.0274 , 0.7227 , - 0.1799 , - 0.9336 , 2.1881 , - 0.2073 , 1.6522 ], [ - 0.9617 , - 0.0348 , - 0.3980 , - 0.4738 , 0.7790 , 0.4671 , - 0.6115 , - 0.7067 , 1.3036 , 0.4923 ], [ - 1.0151 , - 2.5385 , - 0.6072 , 0.2902 , 3.1570 , 0.1062 , - 0.2169 , - 0.4491 , 0.6326 , 1.6829 ], [ - 1.8852 , 0.6066 , - 0.2840 , - 0.4475 , - 0.1147 , - 0.7858 , - 1.1805 , 3.0723 , 0.3960 , 0.9720 ], [ 0.0344 , - 1.4878 , - 0.9675 , 1.9649 , - 0.3146 , 1.2183 , 0.6730 , - 0.3650 , 0.0646 , - 0.0898 ], [ - 0.2118 , - 2.0350 , 0.9917 , - 0.8993 , 1.2334 , - 0.6723 , 2.5847 , - 0.0454 , - 0.4149 , 0.3927 ], [ - 1.7365 , 3.0447 , 0.5115 , 0.0786 , - 0.7544 , - 0.2158 , - 0.4876 , - 0.2891 , 0.5089 , - 0.6719 ], [ 0.3652 , - 0.5457 , - 0.1167 , 2.9056 , - 1.1622 , 0.8192 , - 1.3245 , - 0.6414 , 0.8097 , - 0.4958 ], [ - 0.8755 , - 0.6983 , 0.2208 , - 0.6463 , 0.5276 , 0.1145 , 2.7229 , - 1.0316 , 0.1905 , 0.2090 ], [ - 0.9702 , 0.1265 , - 0.0007 , - 0.5106 , 0.4970 , - 0.0804 , 0.0017 , 0.0607 , 0.6164 , 0.4490 ], [ - 0.8271 , - 0.6822 , - 0.7434 , 2.6457 , - 1.6143 , 1.1486 , - 1.0705 , 0.5611 , 0.6422 , 0.1250 ], [ - 1.9979 , 1.8175 , - 0.1658 , - 0.0343 , - 0.6292 , 0.1774 , 0.3150 , - 0.4633 , 0.9266 , 0.0252 ], [ - 0.9039 , - 0.6030 , - 0.2173 , - 1.1768 , 2.3198 , - 0.5072 , 0.3418 , - 0.1551 , 0.1282 , 1.4250 ], [ - 0.9891 , 0.5212 , - 0.4518 , 0.3267 , - 0.0759 , 0.3826 , - 0.0341 , 0.0382 , 0.2451 , 0.3658 ], [ - 2.1217 , 1.5102 , - 0.7828 , 0.3554 , - 0.4192 , - 0.0772 , 0.0578 , 0.8070 , 0.1701 , 0.5880 ], [ 1.0665 , - 1.3826 , 0.6243 , - 0.8096 , - 0.4227 , 0.5925 , 1.8112 , - 0.9946 , 0.2010 , - 0.7731 ], [ - 1.1263 , - 1.7484 , 0.0041 , - 0.5439 , 1.7242 , - 0.9475 , - 0.3835 , 0.8452 , 0.3077 , 2.2689 ]]) Printing output size This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs . size ()) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS torch . Size ([ 100 , 10 ]) Printing one output This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7. number 0: -0.4181 number 1: -1.0784 ... number 7: 2.9352 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs [ 0 , :]) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639, 2.9352, -0.1552, 0.8852]) Printing prediction output Because our output is of size 100 (our batch size), our prediction size would also of the size 100. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted . size ()) PREDICTION torch . Size ([ 100 ]) Print prediction value We are printing our prediction which as verified above, should be digit 7. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) PREDICTION tensor ( 7 ) Print prediction, label and label size We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7! iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 0' ) print ( labels [ 0 ]) PREDICTION tensor ( 7 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 0 tensor ( 7 ) Print second prediction and ground truth Again, the prediction is correct. Naturally, as our model is quite competent in this simple task. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 1 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 1' ) print ( labels [ 1 ]) PREDICTION tensor ( 2 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 1 tensor ( 2 ) Print accuracy Now we know what each object represents, we can understand how we arrived at our accuracy numbers. One last thing to note is that correct.item() has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this. correct = 0 total = 0 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * ( correct . item () / total ) print ( accuracy ) 82.94 Explanation of Python's .sum() function Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return True or in our case, those predictions that match actual labels (correct predictions). # Explaining .sum() python built-in function # correct += (predicted == labels).sum() import numpy as np a = np . ones (( 10 )) print ( a ) b = np . ones (( 10 )) print ( b ) print ( a == b ) print (( a == b ) . sum ()) # matrix a [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # matrix b [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # boolean array [ True True True True True True True True True True ] # number of elementswhere a matches b 10 Saving Model \u00b6 Saving PyTorch model This is how you save your model. Feel free to just change save_model = True to save your model save_model = False if save_model is True : # Saves only parameters torch . save ( model . state_dict (), 'awesome_model.pkl' ) Building a Logistic Regression Model with PyTorch (GPU) \u00b6 CPU version The usual 7-step process, getting repetitive by now which we like. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # 100 x 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value # 100 x 1 _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.876196026802063 . Accuracy : 64.44 Iteration : 1000. Loss : 1.5153584480285645 . Accuracy : 75.68 Iteration : 1500. Loss : 1.3521136045455933 . Accuracy : 78.98 Iteration : 2000. Loss : 1.2136967182159424 . Accuracy : 80.95 Iteration : 2500. Loss : 1.0934826135635376 . Accuracy : 81.97 Iteration : 3000. Loss : 1.024120569229126 . Accuracy : 82.49 GPU version 2 things must be on GPU - model - variables Remember step 4 and 7 will be affected and this will be the same for all model building moving forward. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8571407794952393 . Accuracy : 68.99 Iteration : 1000. Loss : 1.5415704250335693 . Accuracy : 75.86 Iteration : 1500. Loss : 1.2755383253097534 . Accuracy : 78.92 Iteration : 2000. Loss : 1.2468739748001099 . Accuracy : 80.72 Iteration : 2500. Loss : 1.0708973407745361 . Accuracy : 81.73 Iteration : 3000. Loss : 1.0359245538711548 . Accuracy : 82.74 Summary \u00b6 We've learnt to... Success Logistic regression basics Problems of linear regression In-depth Logistic Regression Get logits Get softmax Get cross-entropy loss Aim : reduce cross-entropy loss Built a logistic regression model in CPU and GPU Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Important things to be on GPU model tensors with gradients","title":"PyTorch Fundamentals - Logistic Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-with-pytorch","text":"","title":"Logistic Regression with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#about-logistic-regression","text":"","title":"About Logistic Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-basics","text":"","title":"Logistic Regression Basics"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#classification-algorithm","text":"Example: Spam vs No Spam Input: Bunch of words Output: Probability spam or not","title":"Classification algorithm"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#basic-comparison","text":"Linear regression Output: numeric value given inputs Logistic regression : Output: probability [0, 1] given input belonging to a class","title":"Basic Comparison"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#inputoutput-comparison","text":"Linear regression: Multiplication Input: [1] Output: 2 Input: [2] Output: 4 Trying to model the relationship y = 2x Logistic regression: Spam Input: \"Sign up to get 1 million dollars by tonight\" Output: p = 0.8 Input: \"This is a receipt for your recent purchase with Amazon\" Output: p = 0.3 p: probability it is spam","title":"Input/Output Comparison"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#problems-of-linear-regression","text":"Example Fever Input : temperature Output : fever or no fever Remember Linear regression : minimize error between points and line Linear Regression Problem 1: Fever value can go negative (below 0) and positive (above 1) If you simply tried to do a simple linear regression on this fever problem, you would realize an apparent error. Fever can go beyond 1 and below 0 which does not make sense in this context. import numpy as np import matplotlib.pyplot as plt % matplotlib inline x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 100 ,] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show () Linear Regression Problem 2: Fever points are not predicted with the presence of outliers Previously at least some points could be properly predicted. However, with the presence of outliers, everything goes wonky for simple linear regression, having no predictive capacity at all. import numpy as np import matplotlib.pyplot as plt x = [ 1 , 5 , 10 , 10 , 25 , 50 , 70 , 75 , 300 ] y = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] colors = np . random . rand ( len ( x )) plt . plot ( np . unique ( x ), np . poly1d ( np . polyfit ( x , y , 1 ))( np . unique ( x ))) plt . ylabel ( \"Fever\" ) plt . xlabel ( \"Temperature\" ) plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ()","title":"Problems of Linear Regression"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-regression-in-depth","text":"","title":"Logistic Regression In-Depth"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#predicting-probability","text":"Linear regression doesn't work Instead of predicting direct values: predict probability","title":"Predicting Probability"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#logistic-function-g","text":"Two-class logistic regression y = A x + b y = A x + b g(y) = A x + b g(y) = A x + b g(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}} g(y) = \\frac {1} {1 + e^{-y}} = \\frac {1} {1 + e^{-(A x + b)}} g(y) g(y) = Estimated probability that y = 1 y = 1 given x x","title":"Logistic Function g()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#softmax-function-g","text":"Multi-class logistic regression Generalization of logistic function","title":"Softmax Function g()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-function-d","text":"D(S, L) = L log S - (1-L)log(1-S) D(S, L) = L log S - (1-L)log(1-S) If L = 0 (label) D(S, 0) = - log(1-S) D(S, 0) = - log(1-S) - log(1-S) - log(1-S) : less positive if S \\longrightarrow 0 S \\longrightarrow 0 - log(1-S) - log(1-S) : more positive if S \\longrightarrow 1 S \\longrightarrow 1 (BIGGER LOSS) If L = 1 (label) D(S, 1) = log S D(S, 1) = log S logS logS : less negative if S \\longrightarrow 1 S \\longrightarrow 1 logS logS : more negative if S \\longrightarrow 0 S \\longrightarrow 0 (BIGGER LOSS) Numerical example of bigger or small loss You get a small error of 1e-5 if your label = 0 and your S is closer to 0 (very correct prediction). import math print ( - math . log ( 1 - 0.00001 )) You get a large error of 11.51 if your label is 0 and S is near to 1 (very wrong prediction). print ( - math . log ( 1 - 0.99999 )) You get a small error of -1e-5 if your label is 1 and S is near 1 (very correct prediction). print ( math . log ( 0.99999 )) You get a big error of -11.51 if your label is 1 and S is near 0 (very wrong prediction). print ( math . log ( 0.00001 )) 1.0000050000287824e-05 11.51292546497478 - 1.0000050000287824e-05 - 11.512925464970229","title":"Cross Entropy Function D()"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#cross-entropy-loss-l","text":"Goal: Minimizing Cross Entropy Loss L = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i) L = \\frac {1}{N} \\sum_i D(g(Ax_i + b), L_i)","title":"Cross Entropy Loss L"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch","text":"","title":"Building a Logistic Regression Model with PyTorch"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#steps","text":"Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model","title":"Steps"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1a-loading-mnist-train-dataset","text":"Images from 1 to 9 Inspect length of training dataset You can easily load MNIST dataset with PyTorch. Here we inspect the training set, where our algorithms will learn from, and you will discover it is made up of 60,000 images. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) len ( train_dataset ) 60000 Inspecting a single image So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers. train_dataset [ 0 ] ( tensor ([[[ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0118 , 0.0706 , 0.0706 , 0.0706 , 0.4941 , 0.5333 , 0.6863 , 0.1020 , 0.6510 , 1.0000 , 0.9686 , 0.4980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1176 , 0.1412 , 0.3686 , 0.6039 , 0.6667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.8824 , 0.6745 , 0.9922 , 0.9490 , 0.7647 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1922 , 0.9333 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9843 , 0.3647 , 0.3216 , 0.3216 , 0.2196 , 0.1529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.7137 , 0.9686 , 0.9451 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3137 , 0.6118 , 0.4196 , 0.9922 , 0.9922 , 0.8039 , 0.0431 , 0.0000 , 0.1686 , 0.6039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0549 , 0.0039 , 0.6039 , 0.9922 , 0.3529 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5451 , 0.9922 , 0.7451 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0431 , 0.7451 , 0.9922 , 0.2745 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1373 , 0.9451 , 0.8824 , 0.6275 , 0.4235 , 0.0039 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.3176 , 0.9412 , 0.9922 , 0.9922 , 0.4667 , 0.0980 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1765 , 0.7294 , 0.9922 , 0.9922 , 0.5882 , 0.1059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0627 , 0.3647 , 0.9882 , 0.9922 , 0.7333 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.9765 , 0.9922 , 0.9765 , 0.2510 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1804 , 0.5098 , 0.7176 , 0.9922 , 0.9922 , 0.8118 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.1529 , 0.5804 , 0.8980 , 0.9922 , 0.9922 , 0.9922 , 0.9804 , 0.7137 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0941 , 0.4471 , 0.8667 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7882 , 0.3059 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0902 , 0.2588 , 0.8353 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7765 , 0.3176 , 0.0078 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0706 , 0.6706 , 0.8588 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.7647 , 0.3137 , 0.0353 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.2157 , 0.6745 , 0.8863 , 0.9922 , 0.9922 , 0.9922 , 0.9922 , 0.9569 , 0.5216 , 0.0431 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.5333 , 0.9922 , 0.9922 , 0.9922 , 0.8314 , 0.5294 , 0.5176 , 0.0627 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 , 0.0000 ]]]), tensor ( 5 )) Inspecting a single data point in the training dataset When you load MNIST dataset, each data point is actually a tuple containing the image matrix and the label. type ( train_dataset [ 0 ]) tuple Inspecting training dataset first element of tuple This means to access the image, you need to access the first element in the tuple. # Input Matrix train_dataset [ 0 ][ 0 ] . size () # A 28x28 sized image of a digit torch . Size ([ 1 , 28 , 28 ]) Inspecting training dataset second element of tuple The second element actually represents the image's label. Meaning if the second element says 5, it means the 28x28 matrix of numbers represent a digit 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 )","title":"Step 1a: Loading MNIST Train Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#displaying-mnist","text":"Verifying shape of MNIST image As mentioned, a single MNIST image is of the shape 28 pixel x 28 pixel. import matplotlib.pyplot as plt % matplotlib inline import numpy as np train_dataset [ 0 ][ 0 ] . numpy () . shape ( 1 , 28 , 28 ) Plot image of MNIST image show_img = train_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label As you would expect, the label is 5. # Label train_dataset [ 0 ][ 1 ] tensor ( 5 ) Plot second image of MNIST image show_img = train_dataset [ 1 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Second element of tuple shows label We should see 0 here as the label. # Label train_dataset [ 1 ][ 1 ] tensor ( 0 )","title":"Displaying MNIST"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-1b-loading-mnist-test-dataset","text":"Show our algorithm works beyond the data we have trained on. Out-of-sample Load test dataset Compared to the 60k images in the training set, the testing set where the model will not be trained on has 10k images to check for its out-of-sample performance. test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) len ( test_dataset ) 10000 Test dataset elements Exactly like the training set, the testing set has 10k tuples containing the 28x28 matrices and their respective labels. type ( test_dataset [ 0 ]) tuple Test dataset first element in tuple This contains the image matrix, similar to the training set. # Image matrix test_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Plot image sample from test dataset show_img = test_dataset [ 0 ][ 0 ] . numpy () . reshape ( 28 , 28 ) plt . imshow ( show_img , cmap = 'gray' ) Test dataset second element in tuple # Label test_dataset [ 0 ][ 1 ] tensor ( 7 )","title":"Step 1b: Loading MNIST Test Dataset"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-2-make-dataset-iterable","text":"Aim: make the dataset iterable totaldata : 60000 minibatch : 100 Number of examples in 1 iteration iterations : 3000 1 iteration: one mini-batch forward & backward pass epochs 1 epoch: running through the whole dataset once $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{60000}{100} = 5 $ Recap training dataset Remember training dataset has 60k images and testing dataset has 10k images. len ( train_dataset ) 60000 Defining epochs When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. batch_size = 100 We arbitrarily set 3000 iterations here which means the model would update 3000 times. n_iters = 3000 One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations. num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) num_epochs 5 Create Iterable Object: Training Dataset train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) Check Iterability import collections isinstance ( train_loader , collections . Iterable ) True Create Iterable Object: Testing Dataset # Iterable object test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) Check iterability of testing dataset isinstance ( test_loader , collections . Iterable ) True Iterate through dataset This is just a simplified example of what we're doing above where we're creating an iterable object lst to loop through so we can access all the images img_1 and img_2 . Above, the equivalent of lst is train_loader and test_loader . img_1 = np . ones (( 28 , 28 )) img_2 = np . ones (( 28 , 28 )) lst = [ img_1 , img_2 ] # Need to iterate # Think of numbers as the images for i in lst : print ( i . shape ) ( 28 , 28 ) ( 28 , 28 )","title":"Step 2: Make Dataset Iterable"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-3-building-model","text":"Create model class # Same as linear regression! class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_dim , output_dim ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out","title":"Step 3: Building Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-4-instantiate-model-class","text":"Input dimension: Size of image 28 \\times 28 = 784 28 \\times 28 = 784 Output dimension: 10 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 Check size of dataset This should be 28x28. # Size of images train_dataset [ 0 ][ 0 ] . size () torch . Size ([ 1 , 28 , 28 ]) Instantiate model class based on input and out dimensions As we're trying to classify digits 0-9 a total of 10 classes, our output dimension is 10. And we're feeding the model with 28x28 images, hence our input dimension is 28x28. input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim )","title":"Step 4: Instantiate Model Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-5-instantiate-loss-class","text":"Logistic Regression : Cross Entropy Loss Linear Regression: MSE Create Cross Entry Loss Class Unlike linear regression, we do not use MSE here, we need Cross Entry Loss to calculate our loss before we backpropagate and update our parameters. criterion = nn . CrossEntropyLoss () What happens in nn.CrossEntropyLoss()? It does 2 things at the same time. 1. Computes softmax (logistic/softmax function) 2. Computes cross entropy","title":"Step 5: Instantiate Loss Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-6-instantiate-optimizer-class","text":"Simplified equation \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\theta \\theta : parameters (our variables) \\eta \\eta : learning rate (how fast we want to learn) \\nabla_\\theta \\nabla_\\theta : parameters' gradients Even simplier equation parameters = parameters - learning_rate * parameters_gradients At every iteration, we update our model's parameters Create optimizer Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently. learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) Parameters In-Depth You'll realize we have 2 sets of parameters, 10x784 which is A and 10x1 which is b in the y = AX + b y = AX + b equation where X is our input of size 784. We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output. # Type of parameter object print ( model . parameters ()) # Length of parameters print ( len ( list ( model . parameters ()))) # FC 1 Parameters print ( list ( model . parameters ())[ 0 ] . size ()) # FC 1 Bias Parameters print ( list ( model . parameters ())[ 1 ] . size ()) < generator object Module . parameters at 0x7ff7c884f830 > 2 torch . Size ([ 10 , 784 ]) torch . Size ([ 10 ]) Quick Dot Product Review Example 1: dot product A: (100, 10) A: (100, 10) B: (10, 1) B: (10, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) A \\cdot B = (100, 10) \\cdot (10, 1) = (100, 1) Example 2: dot product A: (50, 5) A: (50, 5) B: (5, 2) B: (5, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) A \\cdot B = (50, 5) \\cdot (5, 2) = (50, 2) Example 3: element-wise addition A: (10, 1) A: (10, 1) B: (10, 1) B: (10, 1) A + B = (10, 1) A + B = (10, 1)","title":"Step 6: Instantiate Optimizer Class"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#step-7-train-model","text":"7 step process for training models Process Convert inputs/labels to tensors with gradients Clear gradient buffets Get output given inputs Get loss Get gradients w.r.t. parameters Update parameters using gradients parameters = parameters - learning_rate * parameters_gradients REPEAT iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8513233661651611 . Accuracy : 70 Iteration : 1000. Loss : 1.5732524394989014 . Accuracy : 77 Iteration : 1500. Loss : 1.3840199708938599 . Accuracy : 79 Iteration : 2000. Loss : 1.1711134910583496 . Accuracy : 81 Iteration : 2500. Loss : 1.1094708442687988 . Accuracy : 82 Iteration : 3000. Loss : 1.002761721611023 . Accuracy : 82","title":"Step 7: Train Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#break-down-accuracy-calculation","text":"Printing outputs of our model As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model. This would print out the output of the model's predictions on your notebook. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs ) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor ([[ - 0.4181 , - 1.0784 , - 0.4840 , - 0.0985 , - 0.2394 , - 0.1801 , - 1.1639 , 2.9352 , - 0.1552 , 0.8852 ], [ 0.5117 , - 0.1099 , 1.5295 , 0.8863 , - 1.8813 , 0.5967 , 1.3632 , - 1.8977 , 0.4183 , - 1.4990 ], [ - 1.0126 , 2.4112 , 0.2373 , 0.0857 , - 0.7007 , - 0.2015 , - 0.3428 , - 0.2548 , 0.1659 , - 0.4703 ], [ 2.8072 , - 2.2973 , - 0.0984 , - 0.4313 , - 0.9619 , 0.8670 , 1.2201 , 0.3752 , - 0.2873 , - 0.3272 ], [ - 0.0343 , - 2.0043 , 0.5081 , - 0.6452 , 1.8647 , - 0.6924 , 0.1435 , 0.4330 , 0.2958 , 1.0339 ], [ - 1.5392 , 2.9070 , 0.2297 , 0.3139 , - 0.6863 , - 0.2734 , - 0.8377 , - 0.1238 , 0.3285 , - 0.3004 ], [ - 1.2037 , - 1.3739 , - 0.5947 , 0.3530 , 1.4205 , 0.0593 , - 0.7307 , 0.6642 , 0.3937 , 0.8004 ], [ - 1.4439 , - 0.3284 , - 0.7652 , - 0.0952 , 0.9323 , 0.3006 , 0.0238 , - 0.0810 , 0.0612 , 1.3295 ], [ 0.5409 , - 0.5266 , 0.9914 , - 1.2369 , 0.6583 , 0.0992 , 0.8525 , - 1.0562 , 0.2013 , 0.0462 ], [ - 0.6548 , - 0.7253 , - 0.9825 , - 1.1663 , 0.9076 , - 0.0694 , - 0.3708 , 1.8270 , 0.2457 , 1.5921 ], [ 3.2147 , - 1.7689 , 0.8531 , 1.2320 , - 0.8126 , 1.1251 , - 0.2776 , - 1.4244 , 0.5930 , - 1.6183 ], [ 0.7470 , - 0.5545 , 1.0251 , 0.0529 , 0.4384 , - 0.5934 , 0.7666 , - 1.0084 , 0.5313 , - 0.3465 ], [ - 0.7916 , - 1.7064 , - 0.7805 , - 1.1588 , 1.3284 , - 0.1708 , - 0.2092 , 0.9495 , 0.1033 , 2.0208 ], [ 3.0602 , - 2.3578 , - 0.2576 , - 0.2198 , - 0.2372 , 0.9765 , - 0.1514 , - 0.5380 , 0.7970 , 0.1374 ], [ - 1.2613 , 2.8594 , - 0.0874 , 0.1974 , - 1.2018 , - 0.0064 , - 0.0923 , - 0.2142 , 0.2575 , - 0.3218 ], [ 0.4348 , - 0.7216 , 0.0021 , 1.2864 , - 0.5062 , 0.7761 , - 0.3236 , - 0.5667 , 0.5431 , - 0.7781 ], [ - 0.2157 , - 2.0200 , 0.1829 , - 0.6882 , 1.3815 , - 0.7609 , - 0.0902 , 0.8647 , 0.3679 , 1.8843 ], [ 0.0950 , - 1.5009 , - 0.6347 , 0.3662 , - 0.4679 , - 0.0359 , - 0.7671 , 2.7155 , - 0.3991 , 0.5737 ], [ - 0.7005 , - 0.5366 , - 0.0434 , 1.1289 , - 0.5873 , 0.2555 , 0.8187 , - 0.6557 , 0.1241 , - 0.4297 ], [ - 1.0635 , - 1.5991 , - 0.4677 , - 0.1231 , 2.0445 , 0.1128 , - 0.1825 , 0.1075 , 0.0348 , 1.4317 ], [ - 1.0319 , - 0.1595 , - 1.3415 , 0.1095 , 0.5339 , 0.1973 , - 1.3272 , 1.5765 , 0.4784 , 1.4176 ], [ - 0.4928 , - 1.5653 , - 0.0672 , 0.3325 , 0.5359 , 0.5368 , 2.1542 , - 1.4276 , 0.3605 , 0.0587 ], [ - 0.4761 , 0.2958 , 0.6597 , - 0.2658 , 1.1279 , - 1.0676 , 1.2506 , - 0.2059 , - 0.1489 , 0.1051 ], [ - 0.0764 , - 0.9274 , - 0.6838 , 0.3464 , - 0.2656 , 1.4099 , 0.4486 , - 0.9527 , 0.5682 , 0.0156 ], [ - 0.6900 , - 0.9611 , 0.1395 , - 0.0079 , 1.5424 , - 0.3208 , - 0.2682 , 0.3586 , - 0.2771 , 1.0389 ], [ 4.3606 , - 2.8621 , 0.6310 , - 0.9657 , - 0.2486 , 1.2009 , 1.1873 , - 0.8255 , - 0.2103 , - 1.2172 ], [ - 0.1000 , - 1.4268 , - 0.4627 , - 0.1041 , 0.2959 , - 0.1392 , - 0.6855 , 1.8622 , - 0.2580 , 1.1347 ], [ - 0.3625 , - 2.1323 , - 0.2224 , - 0.8754 , 2.4684 , 0.0295 , 0.1161 , - 0.2660 , 0.3037 , 1.4570 ], [ 2.8688 , - 2.4517 , 0.1782 , 1.1149 , - 1.0898 , 1.1062 , - 0.0681 , - 0.5697 , 0.8888 , - 0.6965 ], [ - 1.0429 , 1.4446 , - 0.3349 , 0.1254 , - 0.5017 , 0.2286 , 0.2328 , - 0.3290 , 0.3949 , - 0.2586 ], [ - 0.8476 , - 0.0004 , - 1.1003 , 2.2806 , - 1.2226 , 0.9251 , - 0.3165 , 0.4957 , 0.0690 , 0.0232 ], [ - 0.9108 , 1.1355 , - 0.2715 , 0.2233 , - 0.3681 , 0.1442 , - 0.0001 , - 0.0174 , 0.1454 , 0.2286 ], [ - 1.0663 , - 0.8466 , - 0.7147 , 2.5685 , - 0.2090 , 1.2993 , - 0.3057 , - 0.8314 , 0.7046 , - 0.0176 ], [ 1.7013 , - 1.8051 , 0.7541 , - 1.5248 , 0.8972 , 0.1518 , 1.4876 , - 0.8454 , - 0.2022 , - 0.2829 ], [ - 0.8179 , - 0.1239 , 0.8630 , - 0.2137 , - 0.2275 , - 0.5411 , - 1.3448 , 1.7354 , 0.7751 , 0.6234 ], [ 0.6515 , - 1.0431 , 2.7165 , 0.1873 , - 1.0623 , 0.1286 , 0.3597 , - 0.2739 , 0.3871 , - 1.6699 ], [ - 0.2828 , - 1.4663 , 0.1182 , - 0.0896 , - 0.3640 , - 0.5129 , - 0.4905 , 2.2914 , - 0.2227 , 0.9463 ], [ - 1.2596 , 2.0468 , - 0.4405 , - 0.0411 , - 0.8073 , 0.0490 , - 0.0604 , - 0.1206 , 0.3504 , - 0.1059 ], [ 0.6089 , 0.5885 , 0.7898 , 1.1318 , - 1.9008 , 0.5875 , 0.4227 , - 1.1815 , 0.5652 , - 1.3590 ], [ - 1.4551 , 2.9537 , - 0.2805 , 0.2372 , - 1.4180 , 0.0297 , - 0.1515 , - 0.6111 , 0.6140 , - 0.3354 ], [ - 0.7182 , 1.6778 , 0.0553 , 0.0461 , - 0.5446 , - 0.0338 , - 0.0215 , - 0.0881 , 0.1506 , - 0.2107 ], [ - 0.8027 , - 0.7854 , - 0.1275 , - 0.3177 , - 0.1600 , - 0.1964 , - 0.6084 , 2.1285 , - 0.1815 , 1.1911 ], [ - 2.0656 , - 0.4959 , - 0.1154 , - 0.1363 , 2.2426 , - 0.7441 , - 0.8413 , 0.4675 , 0.3269 , 1.7279 ], [ - 0.3004 , 1.0166 , 1.1175 , - 0.0618 , - 0.0937 , - 0.4221 , 0.1943 , - 1.1020 , 0.3670 , - 0.4683 ], [ - 1.0720 , 0.2252 , 0.0175 , 1.3644 , - 0.7409 , 0.4655 , 0.5439 , 0.0380 , 0.1279 , - 0.2302 ], [ 0.2409 , - 1.2622 , - 0.6336 , 1.8240 , - 0.5951 , 1.3408 , 0.2130 , - 1.3789 , 0.8363 , - 0.2101 ], [ - 1.3849 , 0.3773 , - 0.0585 , 0.6896 , - 0.0998 , 0.2804 , 0.0696 , - 0.2529 , 0.3143 , 0.3409 ], [ - 0.9103 , - 0.1578 , 1.6673 , - 0.4817 , 0.4088 , - 0.5484 , 0.6103 , - 0.2287 , - 0.0665 , 0.0055 ], [ - 1.1692 , - 2.8531 , - 1.2499 , - 0.0257 , 2.8580 , 0.2616 , - 0.7122 , - 0.0551 , 0.8112 , 2.3233 ], [ - 0.2790 , - 1.9494 , 0.6096 , - 0.5653 , 2.2792 , - 1.0687 , 0.1634 , 0.3122 , 0.1053 , 1.0884 ], [ 0.1267 , - 1.2297 , - 0.1315 , 0.2428 , - 0.5436 , 0.4123 , 2.3060 , - 0.9278 , - 0.1528 , - 0.4224 ], [ - 0.0235 , - 0.9137 , - 0.1457 , 1.6858 , - 0.7552 , 0.7293 , 0.2510 , - 0.3955 , - 0.2187 , - 0.1505 ], [ 0.5643 , - 1.2783 , - 1.4149 , 0.0304 , 0.8375 , 1.5018 , 0.0338 , - 0.3875 , - 0.0117 , 0.5751 ], [ 0.2926 , - 0.7486 , - 0.3238 , 1.0384 , 0.0308 , 0.6792 , - 0.0170 , - 0.5797 , 0.2819 , - 0.3510 ], [ 0.1219 , - 0.5862 , 1.5817 , - 0.1297 , 0.4730 , - 0.9171 , 0.7886 , - 0.7022 , - 0.0501 , - 0.2812 ], [ 1.7587 , - 2.4511 , - 0.7369 , 0.4082 , - 0.6426 , 1.1784 , 0.6052 , - 0.7178 , 1.6161 , - 0.2220 ], [ - 0.1267 , - 2.6719 , 0.0505 , - 0.4972 , 2.9027 , - 0.1461 , 0.2807 , - 0.2921 , 0.2231 , 1.1327 ], [ - 0.9892 , 2.4401 , 0.1274 , 0.2838 , - 0.7535 , - 0.1684 , - 0.6493 , - 0.1908 , 0.2290 , - 0.2150 ], [ - 0.2071 , - 2.1351 , - 0.9191 , - 0.9309 , 1.7747 , - 0.3046 , 0.0183 , 1.0136 , - 0.1016 , 2.1288 ], [ - 0.0103 , 0.3280 , - 0.6974 , - 0.2504 , 0.3187 , 0.4390 , - 0.1879 , 0.3954 , 0.2332 , - 0.1971 ], [ - 0.2280 , - 1.6754 , - 0.7438 , 0.5078 , 0.2544 , - 0.1020 , - 0.2503 , 2.0799 , - 0.5033 , 0.5890 ], [ 0.3972 , - 0.9369 , 1.2696 , - 1.6713 , - 0.4159 , - 0.0221 , 0.6489 , - 0.4777 , 1.2497 , 0.3931 ], [ - 0.7566 , - 0.8230 , - 0.0785 , - 0.3083 , 0.7821 , 0.1880 , 0.1037 , - 0.0956 , 0.4219 , 1.0798 ], [ - 1.0328 , - 0.1700 , 1.3806 , 0.5445 , - 0.2624 , - 0.0780 , - 0.3595 , - 0.6253 , 0.4309 , 0.1813 ], [ - 1.0360 , - 0.4704 , 0.1948 , - 0.7066 , 0.6600 , - 0.4633 , - 0.3602 , 1.7494 , 0.1522 , 0.6086 ], [ - 1.2032 , - 0.7903 , - 0.5754 , 0.4722 , 0.6068 , 0.5752 , 0.2151 , - 0.2495 , 0.3420 , 0.9278 ], [ 0.2247 , - 0.1361 , 0.9374 , - 0.1543 , 0.4921 , - 0.6553 , 0.5885 , 0.2617 , - 0.2216 , - 0.3736 ], [ - 0.2867 , - 1.4486 , 0.6658 , - 0.8755 , 2.3195 , - 0.7627 , - 0.2132 , 0.2488 , 0.3484 , 1.0860 ], [ - 1.4031 , - 0.4518 , - 0.3181 , 2.8268 , - 0.5371 , 1.0154 , - 0.9247 , - 0.7385 , 1.1031 , 0.0422 ], [ 2.8604 , - 1.5413 , 0.6241 , - 0.8017 , - 1.4104 , 0.6314 , 0.4614 , - 0.0218 , - 0.3411 , - 0.2609 ], [ 0.2113 , - 1.2348 , - 0.8535 , - 0.1041 , - 0.2703 , - 0.1294 , - 0.7057 , 2.7552 , - 0.4429 , 0.4517 ], [ 4.5191 , - 2.7407 , 1.1091 , 0.3975 , - 0.9456 , 1.2277 , 0.3616 , - 1.6564 , 0.5063 , - 1.4274 ], [ 1.4615 , - 1.0765 , 1.8388 , 1.5006 , - 1.2351 , 0.2781 , 0.2830 , - 0.8491 , 0.2222 , - 1.7779 ], [ - 1.2160 , 0.8502 , 0.2413 , - 0.0798 , - 0.7880 , - 0.4286 , - 0.8060 , 0.7194 , 1.2663 , 0.6412 ], [ - 1.3318 , 2.3388 , - 0.4003 , - 0.1094 , - 1.0285 , 0.1021 , - 0.0388 , - 0.0497 , 0.5137 , - 0.2507 ], [ - 1.7853 , 0.5884 , - 0.6108 , - 0.5557 , 0.8696 , - 0.6226 , - 0.7983 , 1.7169 , - 0.0145 , 0.8231 ], [ - 0.1739 , 0.1562 , - 0.2933 , 2.3195 , - 0.9480 , 1.2019 , - 0.4834 , - 1.0567 , 0.5685 , - 0.6841 ], [ - 0.7920 , - 0.3339 , 0.7452 , - 0.6529 , - 0.3307 , - 0.6092 , - 0.0950 , 1.7311 , - 0.3481 , 0.3801 ], [ - 1.7810 , 1.0676 , - 0.7611 , 0.3658 , - 0.0431 , - 0.1012 , - 0.6048 , 0.3089 , 0.9998 , 0.7164 ], [ - 0.5856 , - 0.5261 , - 0.4859 , - 1.0551 , - 0.1838 , - 0.2144 , - 1.2599 , 3.3891 , 0.4691 , 0.7566 ], [ - 0.4984 , - 1.7770 , - 1.1998 , - 0.1075 , 1.0882 , 0.4539 , - 0.5651 , 1.4381 , - 0.5678 , 1.7479 ], [ 0.2938 , - 1.8536 , 0.4259 , - 0.5429 , 0.0066 , 0.4120 , 2.3793 , - 0.3666 , - 0.2604 , 0.0382 ], [ - 0.4080 , - 0.9851 , 4.0264 , 0.1099 , - 0.1766 , - 1.1557 , 0.6419 , - 0.8147 , 0.7535 , - 1.1452 ], [ - 0.4636 , - 1.7323 , - 0.6433 , - 0.0274 , 0.7227 , - 0.1799 , - 0.9336 , 2.1881 , - 0.2073 , 1.6522 ], [ - 0.9617 , - 0.0348 , - 0.3980 , - 0.4738 , 0.7790 , 0.4671 , - 0.6115 , - 0.7067 , 1.3036 , 0.4923 ], [ - 1.0151 , - 2.5385 , - 0.6072 , 0.2902 , 3.1570 , 0.1062 , - 0.2169 , - 0.4491 , 0.6326 , 1.6829 ], [ - 1.8852 , 0.6066 , - 0.2840 , - 0.4475 , - 0.1147 , - 0.7858 , - 1.1805 , 3.0723 , 0.3960 , 0.9720 ], [ 0.0344 , - 1.4878 , - 0.9675 , 1.9649 , - 0.3146 , 1.2183 , 0.6730 , - 0.3650 , 0.0646 , - 0.0898 ], [ - 0.2118 , - 2.0350 , 0.9917 , - 0.8993 , 1.2334 , - 0.6723 , 2.5847 , - 0.0454 , - 0.4149 , 0.3927 ], [ - 1.7365 , 3.0447 , 0.5115 , 0.0786 , - 0.7544 , - 0.2158 , - 0.4876 , - 0.2891 , 0.5089 , - 0.6719 ], [ 0.3652 , - 0.5457 , - 0.1167 , 2.9056 , - 1.1622 , 0.8192 , - 1.3245 , - 0.6414 , 0.8097 , - 0.4958 ], [ - 0.8755 , - 0.6983 , 0.2208 , - 0.6463 , 0.5276 , 0.1145 , 2.7229 , - 1.0316 , 0.1905 , 0.2090 ], [ - 0.9702 , 0.1265 , - 0.0007 , - 0.5106 , 0.4970 , - 0.0804 , 0.0017 , 0.0607 , 0.6164 , 0.4490 ], [ - 0.8271 , - 0.6822 , - 0.7434 , 2.6457 , - 1.6143 , 1.1486 , - 1.0705 , 0.5611 , 0.6422 , 0.1250 ], [ - 1.9979 , 1.8175 , - 0.1658 , - 0.0343 , - 0.6292 , 0.1774 , 0.3150 , - 0.4633 , 0.9266 , 0.0252 ], [ - 0.9039 , - 0.6030 , - 0.2173 , - 1.1768 , 2.3198 , - 0.5072 , 0.3418 , - 0.1551 , 0.1282 , 1.4250 ], [ - 0.9891 , 0.5212 , - 0.4518 , 0.3267 , - 0.0759 , 0.3826 , - 0.0341 , 0.0382 , 0.2451 , 0.3658 ], [ - 2.1217 , 1.5102 , - 0.7828 , 0.3554 , - 0.4192 , - 0.0772 , 0.0578 , 0.8070 , 0.1701 , 0.5880 ], [ 1.0665 , - 1.3826 , 0.6243 , - 0.8096 , - 0.4227 , 0.5925 , 1.8112 , - 0.9946 , 0.2010 , - 0.7731 ], [ - 1.1263 , - 1.7484 , 0.0041 , - 0.5439 , 1.7242 , - 0.9475 , - 0.3835 , 0.8452 , 0.3077 , 2.2689 ]]) Printing output size This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs . size ()) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS torch . Size ([ 100 , 10 ]) Printing one output This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7. number 0: -0.4181 number 1: -1.0784 ... number 7: 2.9352 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) if iter_test == 1 : print ( 'OUTPUTS' ) print ( outputs [ 0 , :]) _ , predicted = torch . max ( outputs . data , 1 ) OUTPUTS tensor([-0.4181, -1.0784, -0.4840, -0.0985, -0.2394, -0.1801, -1.1639, 2.9352, -0.1552, 0.8852]) Printing prediction output Because our output is of size 100 (our batch size), our prediction size would also of the size 100. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted . size ()) PREDICTION torch . Size ([ 100 ]) Print prediction value We are printing our prediction which as verified above, should be digit 7. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) PREDICTION tensor ( 7 ) Print prediction, label and label size We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7! iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 0 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 0' ) print ( labels [ 0 ]) PREDICTION tensor ( 7 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 0 tensor ( 7 ) Print second prediction and ground truth Again, the prediction is correct. Naturally, as our model is quite competent in this simple task. iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) if iter_test == 1 : print ( 'PREDICTION' ) print ( predicted [ 1 ]) print ( 'LABEL SIZE' ) print ( labels . size ()) print ( 'LABEL FOR IMAGE 1' ) print ( labels [ 1 ]) PREDICTION tensor ( 2 ) LABEL SIZE torch . Size ([ 100 ]) LABEL FOR IMAGE 1 tensor ( 2 ) Print accuracy Now we know what each object represents, we can understand how we arrived at our accuracy numbers. One last thing to note is that correct.item() has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this. correct = 0 total = 0 iter_test = 0 for images , labels in test_loader : iter_test += 1 images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () outputs = model ( images ) _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * ( correct . item () / total ) print ( accuracy ) 82.94 Explanation of Python's .sum() function Python's .sum() function allows you to do a comparison between two matrices and sum the ones that return True or in our case, those predictions that match actual labels (correct predictions). # Explaining .sum() python built-in function # correct += (predicted == labels).sum() import numpy as np a = np . ones (( 10 )) print ( a ) b = np . ones (( 10 )) print ( b ) print ( a == b ) print (( a == b ) . sum ()) # matrix a [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # matrix b [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ] # boolean array [ True True True True True True True True True True ] # number of elementswhere a matches b 10","title":"Break Down Accuracy Calculation"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#saving-model","text":"Saving PyTorch model This is how you save your model. Feel free to just change save_model = True to save your model save_model = False if save_model is True : # Saves only parameters torch . save ( model . state_dict (), 'awesome_model.pkl' )","title":"Saving Model"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#building-a-logistic-regression-model-with-pytorch-gpu","text":"CPU version The usual 7-step process, getting repetitive by now which we like. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): # Load images as Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () labels = labels # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits # 100 x 10 outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : # Load images to a Torch Variable images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value # 100 x 1 _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) # Total correct predictions correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.876196026802063 . Accuracy : 64.44 Iteration : 1000. Loss : 1.5153584480285645 . Accuracy : 75.68 Iteration : 1500. Loss : 1.3521136045455933 . Accuracy : 78.98 Iteration : 2000. Loss : 1.2136967182159424 . Accuracy : 80.95 Iteration : 2500. Loss : 1.0934826135635376 . Accuracy : 81.97 Iteration : 3000. Loss : 1.024120569229126 . Accuracy : 82.49 GPU version 2 things must be on GPU - model - variables Remember step 4 and 7 will be affected and this will be the same for all model building moving forward. import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets ''' STEP 1: LOADING DATASET ''' train_dataset = dsets . MNIST ( root = './data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = dsets . MNIST ( root = './data' , train = False , transform = transforms . ToTensor ()) ''' STEP 2: MAKING DATASET ITERABLE ''' batch_size = 100 n_iters = 3000 num_epochs = n_iters / ( len ( train_dataset ) / batch_size ) num_epochs = int ( num_epochs ) train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True ) test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False ) ''' STEP 3: CREATE MODEL CLASS ''' class LogisticRegressionModel ( nn . Module ): def __init__ ( self , input_size , num_classes ): super ( LogisticRegressionModel , self ) . __init__ () self . linear = nn . Linear ( input_dim , output_dim ) def forward ( self , x ): out = self . linear ( x ) return out ''' STEP 4: INSTANTIATE MODEL CLASS ''' input_dim = 28 * 28 output_dim = 10 model = LogisticRegressionModel ( input_dim , output_dim ) ####################### # USE GPU FOR MODEL # ####################### device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model . to ( device ) ''' STEP 5: INSTANTIATE LOSS CLASS ''' criterion = nn . CrossEntropyLoss () ''' STEP 6: INSTANTIATE OPTIMIZER CLASS ''' learning_rate = 0.001 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) ''' STEP 7: TRAIN THE MODEL ''' iter = 0 for epoch in range ( num_epochs ): for i , ( images , labels ) in enumerate ( train_loader ): ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . requires_grad_ () . to ( device ) labels = labels . to ( device ) # Clear gradients w.r.t. parameters optimizer . zero_grad () # Forward pass to get output/logits outputs = model ( images ) # Calculate Loss: softmax --> cross entropy loss loss = criterion ( outputs , labels ) # Getting gradients w.r.t. parameters loss . backward () # Updating parameters optimizer . step () iter += 1 if iter % 500 == 0 : # Calculate Accuracy correct = 0 total = 0 # Iterate through test dataset for images , labels in test_loader : ####################### # USE GPU FOR MODEL # ####################### images = images . view ( - 1 , 28 * 28 ) . to ( device ) # Forward pass only to get logits/output outputs = model ( images ) # Get predictions from the maximum value _ , predicted = torch . max ( outputs . data , 1 ) # Total number of labels total += labels . size ( 0 ) ####################### # USE GPU FOR MODEL # ####################### # Total correct predictions if torch . cuda . is_available (): correct += ( predicted . cpu () == labels . cpu ()) . sum () else : correct += ( predicted == labels ) . sum () accuracy = 100 * correct . item () / total # Print Loss print ( 'Iteration: {}. Loss: {}. Accuracy: {}' . format ( iter , loss . item (), accuracy )) Iteration : 500. Loss : 1.8571407794952393 . Accuracy : 68.99 Iteration : 1000. Loss : 1.5415704250335693 . Accuracy : 75.86 Iteration : 1500. Loss : 1.2755383253097534 . Accuracy : 78.92 Iteration : 2000. Loss : 1.2468739748001099 . Accuracy : 80.72 Iteration : 2500. Loss : 1.0708973407745361 . Accuracy : 81.73 Iteration : 3000. Loss : 1.0359245538711548 . Accuracy : 82.74","title":"Building a Logistic Regression Model with PyTorch (GPU)"},{"location":"deep_learning/practical_pytorch/pytorch_logistic_regression/#summary","text":"We've learnt to... Success Logistic regression basics Problems of linear regression In-depth Logistic Regression Get logits Get softmax Get cross-entropy loss Aim : reduce cross-entropy loss Built a logistic regression model in CPU and GPU Step 1: Load Dataset Step 2: Make Dataset Iterable Step 3: Create Model Class Step 4: Instantiate Model Class Step 5: Instantiate Loss Class Step 6: Instantiate Optimizer Class Step 7: Train Model Important things to be on GPU model tensors with gradients","title":"Summary"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/","text":"PyTorch Fundamentals - Matrices \u00b6 Matrices \u00b6 Matrices Brief Introduction \u00b6 Basic definition: rectangular array of numbers. Tensors (PyTorch) Ndarrays (NumPy) 2 x 2 Matrix (R x C) 1 1 1 1 2 x 3 Matrix 1 1 1 1 1 1 Creating Matrices \u00b6 Create list # Creating a 2x2 array arr = [[ 1 , 2 ], [ 3 , 4 ]] print ( arr ) [[ 1 , 2 ], [ 3 , 4 ]] Create numpy array via list import numpy as np # Convert to NumPy np . array ( arr ) array ([[ 1 , 2 ], [ 3 , 4 ]]) Convert numpy array to PyTorch tensor import torch # Convert to PyTorch Tensor torch . Tensor ( arr ) 1 2 3 4 [ torch . FloatTensor of size 2 x2 ] Create Matrices with Default Values \u00b6 Create 2x2 numpy array of 1's np . ones (( 2 , 2 )) array ([[ 1. , 1. ], [ 1. , 1. ]]) Create 2x2 torch tensor of 1's torch . ones (( 2 , 2 )) 1 1 1 1 [torch.FloatTensor of size 2x2] Create 2x2 numpy array of random numbers np . random . rand ( 2 , 2 ) array ([[ 0.68270631 , 0.87721678 ], [ 0.07420986 , 0.79669375 ]]) Create 2x2 PyTorch tensor of random numbers torch . rand ( 2 , 2 ) 0.3900 0.8268 0.3888 0.5914 [ torch . FloatTensor of size 2 x2 ] Seeds for Reproducibility \u00b6 Why do we need seeds? We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced. Create seed to enable fixed numbers for random number generation # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Repeat random array generation to check If you do not set the seed, you would not get the same set of numbers like here. # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Create a numpy array without seed Notice how you get different numbers compared to the first 2 tries? # No seed np . random . rand ( 2 , 2 ) array ([[ 0.56804456 , 0.92559664 ], [ 0.07103606 , 0.0871293 ]]) Repeat numpy array generation without seed You get the point now, you get a totally different set of numbers. # No seed np . random . rand ( 2 , 2 ) array ([[ 0.0202184 , 0.83261985 ], [ 0.77815675 , 0.87001215 ]]) Create a PyTorch tensor with a fixed seed # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) Repeat creating a PyTorch fixed seed tensor # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) 0.5488 0.5928 0.7152 0.8443 [ torch . FloatTensor of size 2 x2 ] Creating a PyTorch tensor without seed Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed torch . rand ( 2 , 2 ) 0.6028 0.8579 0.5449 0.8473 [ torch . FloatTensor of size 2 x2 ] Repeat creating a PyTorch tensor without seed Notice how these are different numbers again? # Torch No Seed torch . rand ( 2 , 2 ) 0.4237 0.6236 0.6459 0.3844 [ torch . FloatTensor of size 2 x2 ] Seed for GPU is different for now... Fix a seed for GPU tensors When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. if torch . cuda . is_available (): torch . cuda . manual_seed_all ( 0 ) NumPy and Torch Bridge \u00b6 NumPy to Torch \u00b6 Create a numpy array of 1's # Numpy array np_array = np . ones (( 2 , 2 )) print ( np_array ) [[ 1. 1. ] [ 1. 1. ]] Get the type of class for the numpy array print ( type ( np_array )) < class ' numpy . ndarray '> Convert numpy array to PyTorch tensor # Convert to Torch Tensor torch_tensor = torch . from_numpy ( np_array ) print ( torch_tensor ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Get type of class for PyTorch tensor Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor )) < class ' torch . DoubleTensor '> Create PyTorch tensor from a different numpy datatype You will get an error running this code because PyTorch tensor don't support all datatype. # Data types matter: intentional error np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) torch . from_numpy ( np_array_new ) --------------------------------------------------------------------------- RuntimeError Traceback ( most recent call last ) < ipython - input - 57 - b8b085f9b39d > in < module > () 1 # Data types matter 2 np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) ----> 3 torch . from_numpy ( np_array_new ) RuntimeError : can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8. What conversion support does Numpy to PyTorch tensor bridge gives? double float int64 , int32 , uint8 Create PyTorch long tensor See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int64 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [torch.LongTensor of size 2x2] Create PyTorch int tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . IntTensor of size 2 x2 ] Create PyTorch byte tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . uint8 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . ByteTensor of size 2 x2 ] Create PyTorch Double Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float64 ) torch . from_numpy ( np_array_new ) Alternatively you can do this too via np.double # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . double ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Create PyTorch Float Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Tensor Type Bug Guide These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide! NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor Torch to NumPy \u00b6 Create PyTorch tensor of 1's You would realize this defaults to a float tensor by default if you do this. torch_tensor = torch . ones ( 2 , 2 ) type ( torch_tensor ) torch . FloatTensor Convert tensor to numpy It's as simple as this. torch_to_numpy = torch_tensor . numpy () type ( torch_to_numpy ) # Wowza, we did it. numpy . ndarray Tensors on CPU vs GPU \u00b6 Move tensor to CPU and back This by default creates a tensor on CPU. You do not need to do anything. # CPU tensor_cpu = torch . ones ( 2 , 2 ) If you would like to send a tensor to your GPU, you just need to do a simple .cuda () # CPU to GPU device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) tensor_cpu . to ( device ) And if you want to move that tensor on the GPU back to the CPU, just do the following. # GPU to CPU tensor_cpu . cpu () Tensor Operations \u00b6 Resizing Tensor \u00b6 Creating a 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Getting size of tensor print ( a . size ()) torch . Size ([ 2 , 2 ]) Resize tensor to 4x1 a . view ( 4 ) 1 1 1 1 [ torch . FloatTensor of size 4 ] Get size of resized tensor a . view ( 4 ) . size () torch . Size ([ 4 ]) Element-wise Addition \u00b6 Creating first 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Creating second 2x2 tensor b = torch . ones ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise addition of 2 tensors # Element-wise addition c = a + b print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] Alternative element-wise addition of 2 tensors # Element-wise addition c = torch . add ( a , b ) print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] In-place element-wise addition This would replace the c tensor values with the new addition. # In-place addition print ( 'Old c tensor' ) print ( c ) c . add_ ( a ) print ( '-' * 60 ) print ( 'New c tensor' ) print ( c ) Old c tensor 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] ------------------------------------------------------------ New c tensor 3 3 3 3 [ torch . FloatTensor of size 2 x2 ] Element-wise Subtraction \u00b6 Check values of tensor a and b' Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above. print ( a ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 1 a - b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 2 # Not in-place print ( a . sub ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 3 This will replace a with the final result filled with 2's # Inplace print ( a . sub_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-Wise Multiplication \u00b6 Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 1 a * b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 2 # Not in-place print ( torch . mul ( a , b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 3 # In-place print ( a . mul_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-Wise Division \u00b6 Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 1 b / a 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 2 torch . div ( b , a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 3 # Inplace b . div_ ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Tensor Mean \u00b6 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 mean = 55 /10 = 5.5 mean = 55 /10 = 5.5 Create tensor of size 10 filled from 1 to 10 a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . size () torch . Size ([ 10 ]) Get tensor mean Here we get 5.5 as we've calculated manually above. a . mean ( dim = 0 ) 5.5000 [ torch . FloatTensor of size 1 ] Get tensor mean on second dimension Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate. a . mean ( dim = 1 ) RuntimeError Traceback ( most recent call last ) < ipython - input - 7 - 81 aec0cf1c00 > in < module > () ----> 1 a . mean ( dim = 1 ) RuntimeError : dimension out of range ( expected to be in range of [ - 1 , 0 ], but got 1 ) Create a 2x10 Tensor, of 1-10 digits each a = torch . Tensor ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]]) a . size () torch . Size ([ 2 , 10 ]) Get tensor mean on second dimension Here we won't get an error like previously because we've a tensor of size 2x10 a . mean ( dim = 1 ) 5.5000 5.5000 [ torch . FloatTensor of size 2 x1 ] Tensor Standard Deviation \u00b6 Get standard deviation of tensor a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . std ( dim = 0 ) 3.0277 [ torch . FloatTensor of size 1 ] Summary \u00b6 We've learnt to... Success Create Matrices Create Matrices with Default Initialization Values Zeros Ones Initialize Seeds for Reproducibility on GPU and CPU Convert Matrices: NumPy to Torch and Torch to NumPy Move Tensors: CPU to GPU and GPU to CPU Run Important Tensor Operations Element-wise addition, subtraction, multiplication and division Resize Calculate mean Calculate standard deviation","title":"PyTorch Fundamentals - Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#pytorch-fundamentals-matrices","text":"","title":"PyTorch Fundamentals - Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices","text":"","title":"Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#matrices-brief-introduction","text":"Basic definition: rectangular array of numbers. Tensors (PyTorch) Ndarrays (NumPy) 2 x 2 Matrix (R x C) 1 1 1 1 2 x 3 Matrix 1 1 1 1 1 1","title":"Matrices Brief Introduction"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#creating-matrices","text":"Create list # Creating a 2x2 array arr = [[ 1 , 2 ], [ 3 , 4 ]] print ( arr ) [[ 1 , 2 ], [ 3 , 4 ]] Create numpy array via list import numpy as np # Convert to NumPy np . array ( arr ) array ([[ 1 , 2 ], [ 3 , 4 ]]) Convert numpy array to PyTorch tensor import torch # Convert to PyTorch Tensor torch . Tensor ( arr ) 1 2 3 4 [ torch . FloatTensor of size 2 x2 ]","title":"Creating Matrices"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#create-matrices-with-default-values","text":"Create 2x2 numpy array of 1's np . ones (( 2 , 2 )) array ([[ 1. , 1. ], [ 1. , 1. ]]) Create 2x2 torch tensor of 1's torch . ones (( 2 , 2 )) 1 1 1 1 [torch.FloatTensor of size 2x2] Create 2x2 numpy array of random numbers np . random . rand ( 2 , 2 ) array ([[ 0.68270631 , 0.87721678 ], [ 0.07420986 , 0.79669375 ]]) Create 2x2 PyTorch tensor of random numbers torch . rand ( 2 , 2 ) 0.3900 0.8268 0.3888 0.5914 [ torch . FloatTensor of size 2 x2 ]","title":"Create Matrices with Default Values"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#seeds-for-reproducibility","text":"Why do we need seeds? We need seeds to enable reproduction of experimental results. This becomes critical later on where you can easily let people reproduce your code's output exactly as you've produced. Create seed to enable fixed numbers for random number generation # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Repeat random array generation to check If you do not set the seed, you would not get the same set of numbers like here. # Seed np . random . seed ( 0 ) np . random . rand ( 2 , 2 ) array ([[ 0.5488135 , 0.71518937 ], [ 0.60276338 , 0.54488318 ]]) Create a numpy array without seed Notice how you get different numbers compared to the first 2 tries? # No seed np . random . rand ( 2 , 2 ) array ([[ 0.56804456 , 0.92559664 ], [ 0.07103606 , 0.0871293 ]]) Repeat numpy array generation without seed You get the point now, you get a totally different set of numbers. # No seed np . random . rand ( 2 , 2 ) array ([[ 0.0202184 , 0.83261985 ], [ 0.77815675 , 0.87001215 ]]) Create a PyTorch tensor with a fixed seed # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) Repeat creating a PyTorch fixed seed tensor # Torch Seed torch . manual_seed ( 0 ) torch . rand ( 2 , 2 ) 0.5488 0.5928 0.7152 0.8443 [ torch . FloatTensor of size 2 x2 ] Creating a PyTorch tensor without seed Like with a numpy array of random numbers without seed, you will not get the same results as above. # Torch No Seed torch . rand ( 2 , 2 ) 0.6028 0.8579 0.5449 0.8473 [ torch . FloatTensor of size 2 x2 ] Repeat creating a PyTorch tensor without seed Notice how these are different numbers again? # Torch No Seed torch . rand ( 2 , 2 ) 0.4237 0.6236 0.6459 0.3844 [ torch . FloatTensor of size 2 x2 ] Seed for GPU is different for now... Fix a seed for GPU tensors When you conduct deep learning experiments, typically you want to use GPUs to accelerate your computations and fixing seed for tensors on GPUs is different from CPUs as we have done above. if torch . cuda . is_available (): torch . cuda . manual_seed_all ( 0 )","title":"Seeds for Reproducibility"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-and-torch-bridge","text":"","title":"NumPy and Torch Bridge"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#numpy-to-torch","text":"Create a numpy array of 1's # Numpy array np_array = np . ones (( 2 , 2 )) print ( np_array ) [[ 1. 1. ] [ 1. 1. ]] Get the type of class for the numpy array print ( type ( np_array )) < class ' numpy . ndarray '> Convert numpy array to PyTorch tensor # Convert to Torch Tensor torch_tensor = torch . from_numpy ( np_array ) print ( torch_tensor ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Get type of class for PyTorch tensor Notice how it shows it's a torch DoubleTensor? There're actually tensor types and it depends on the numpy data type. print ( type ( torch_tensor )) < class ' torch . DoubleTensor '> Create PyTorch tensor from a different numpy datatype You will get an error running this code because PyTorch tensor don't support all datatype. # Data types matter: intentional error np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) torch . from_numpy ( np_array_new ) --------------------------------------------------------------------------- RuntimeError Traceback ( most recent call last ) < ipython - input - 57 - b8b085f9b39d > in < module > () 1 # Data types matter 2 np_array_new = np . ones (( 2 , 2 ), dtype = np . int8 ) ----> 3 torch . from_numpy ( np_array_new ) RuntimeError : can 't convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8. What conversion support does Numpy to PyTorch tensor bridge gives? double float int64 , int32 , uint8 Create PyTorch long tensor See how a int64 numpy array gives you a PyTorch long tensor? # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int64 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [torch.LongTensor of size 2x2] Create PyTorch int tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . int32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . IntTensor of size 2 x2 ] Create PyTorch byte tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . uint8 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . ByteTensor of size 2 x2 ] Create PyTorch Double Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float64 ) torch . from_numpy ( np_array_new ) Alternatively you can do this too via np.double # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . double ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . DoubleTensor of size 2 x2 ] Create PyTorch Float Tensor # Data types matter np_array_new = np . ones (( 2 , 2 ), dtype = np . float32 ) torch . from_numpy ( np_array_new ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Tensor Type Bug Guide These things don't matter much now. But later when you see error messages that require these particular tensor types, refer to this guide! NumPy Array Type Torch Tensor Type int64 LongTensor int32 IntegerTensor uint8 ByteTensor float64 DoubleTensor float32 FloatTensor double DoubleTensor","title":"NumPy to Torch"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#torch-to-numpy","text":"Create PyTorch tensor of 1's You would realize this defaults to a float tensor by default if you do this. torch_tensor = torch . ones ( 2 , 2 ) type ( torch_tensor ) torch . FloatTensor Convert tensor to numpy It's as simple as this. torch_to_numpy = torch_tensor . numpy () type ( torch_to_numpy ) # Wowza, we did it. numpy . ndarray","title":"Torch to NumPy"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensors-on-cpu-vs-gpu","text":"Move tensor to CPU and back This by default creates a tensor on CPU. You do not need to do anything. # CPU tensor_cpu = torch . ones ( 2 , 2 ) If you would like to send a tensor to your GPU, you just need to do a simple .cuda () # CPU to GPU device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) tensor_cpu . to ( device ) And if you want to move that tensor on the GPU back to the CPU, just do the following. # GPU to CPU tensor_cpu . cpu ()","title":"Tensors on CPU vs GPU"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-operations","text":"","title":"Tensor Operations"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#resizing-tensor","text":"Creating a 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Getting size of tensor print ( a . size ()) torch . Size ([ 2 , 2 ]) Resize tensor to 4x1 a . view ( 4 ) 1 1 1 1 [ torch . FloatTensor of size 4 ] Get size of resized tensor a . view ( 4 ) . size () torch . Size ([ 4 ])","title":"Resizing Tensor"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-addition","text":"Creating first 2x2 tensor a = torch . ones ( 2 , 2 ) print ( a ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Creating second 2x2 tensor b = torch . ones ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise addition of 2 tensors # Element-wise addition c = a + b print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] Alternative element-wise addition of 2 tensors # Element-wise addition c = torch . add ( a , b ) print ( c ) 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] In-place element-wise addition This would replace the c tensor values with the new addition. # In-place addition print ( 'Old c tensor' ) print ( c ) c . add_ ( a ) print ( '-' * 60 ) print ( 'New c tensor' ) print ( c ) Old c tensor 2 2 2 2 [ torch . FloatTensor of size 2 x2 ] ------------------------------------------------------------ New c tensor 3 3 3 3 [ torch . FloatTensor of size 2 x2 ]","title":"Element-wise Addition"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-subtraction","text":"Check values of tensor a and b' Take note that you've created tensor a and b of sizes 2x2 filled with 1's each above. print ( a ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 1 a - b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 2 # Not in-place print ( a . sub ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise subtraction: method 3 This will replace a with the final result filled with 2's # Inplace print ( a . sub_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-wise Subtraction"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-multiplication","text":"Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 1 a * b 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 2 # Not in-place print ( torch . mul ( a , b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] Element-wise multiplication: method 3 # In-place print ( a . mul_ ( b )) print ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-Wise Multiplication"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#element-wise-division","text":"Create tensor a and b of sizes 2x2 filled with 1's and 0's a = torch . ones ( 2 , 2 ) print ( a ) b = torch . zeros ( 2 , 2 ) print ( b ) 1 1 1 1 [ torch . FloatTensor of size 2 x2 ] 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 1 b / a 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 2 torch . div ( b , a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ] Element-wise division: method 3 # Inplace b . div_ ( a ) 0 0 0 0 [ torch . FloatTensor of size 2 x2 ]","title":"Element-Wise Division"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-mean","text":"1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 = 55 mean = 55 /10 = 5.5 mean = 55 /10 = 5.5 Create tensor of size 10 filled from 1 to 10 a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . size () torch . Size ([ 10 ]) Get tensor mean Here we get 5.5 as we've calculated manually above. a . mean ( dim = 0 ) 5.5000 [ torch . FloatTensor of size 1 ] Get tensor mean on second dimension Here we get an error because the tensor is of size 10 and not 10x1 so there's no second dimension to calculate. a . mean ( dim = 1 ) RuntimeError Traceback ( most recent call last ) < ipython - input - 7 - 81 aec0cf1c00 > in < module > () ----> 1 a . mean ( dim = 1 ) RuntimeError : dimension out of range ( expected to be in range of [ - 1 , 0 ], but got 1 ) Create a 2x10 Tensor, of 1-10 digits each a = torch . Tensor ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]]) a . size () torch . Size ([ 2 , 10 ]) Get tensor mean on second dimension Here we won't get an error like previously because we've a tensor of size 2x10 a . mean ( dim = 1 ) 5.5000 5.5000 [ torch . FloatTensor of size 2 x1 ]","title":"Tensor Mean"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#tensor-standard-deviation","text":"Get standard deviation of tensor a = torch . Tensor ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) a . std ( dim = 0 ) 3.0277 [ torch . FloatTensor of size 1 ]","title":"Tensor Standard Deviation"},{"location":"deep_learning/practical_pytorch/pytorch_matrices/#summary","text":"We've learnt to... Success Create Matrices Create Matrices with Default Initialization Values Zeros Ones Initialize Seeds for Reproducibility on GPU and CPU Convert Matrices: NumPy to Torch and Torch to NumPy Move Tensors: CPU to GPU and GPU to CPU Run Important Tensor Operations Element-wise addition, subtraction, multiplication and division Resize Calculate mean Calculate standard deviation","title":"Summary"},{"location":"news/deep_learning_wizard_1y_2018_06_01/","text":"Featured on PyTorch Website \u00b6 PyTorch a Year Later \u00b6 We are featured on PyTorch website's post I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday. A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned! A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome. To more great years ahead for PyTorch Cheers, Ritchie Ng","title":"Featured on PyTorch Website 2018"},{"location":"news/deep_learning_wizard_1y_2018_06_01/#featured-on-pytorch-website","text":"","title":"Featured on PyTorch Website"},{"location":"news/deep_learning_wizard_1y_2018_06_01/#pytorch-a-year-later","text":"We are featured on PyTorch website's post I used PyTorch from day 1 and I fell in love with it. One year later, PyTorch's community has grown tremendously. I thank Facebook AI Research (FAIR) and everyone who contributed to building one of the best deep learning framework. In particular Soumith Chintala from FAIR where he is continually fixing bugs and making PyTorch better everyday. A year later, Deep Learning Wizard has taught more than 2000 students across 100+ countries, and we continue to strive to release new and updated content to enable anyone to build customized deep learning systems to solve many problems. And I'm trying to push for PyTorch workshops at NVIDIA Deep Learning Institute in Singapore and across the world, so stay tuned! A big shoutout for Alfredo Canziani who's currently a postdoc at NYU co-teaching with Yann LeCun on deep learning classes and he's releasing another technical course on PyTorch that is going to be awesome. To more great years ahead for PyTorch Cheers, Ritchie Ng","title":"PyTorch a Year Later"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/","text":"We Are an NVIDIA Inception Partner \u00b6 We did it! \u00b6 After almost a year, we are an NVIDIA Inception Partner now! \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Cheers, Ritchie Ng","title":"NVIDIA Inception Partner Status, Singapore, May 2017"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-are-an-nvidia-inception-partner","text":"","title":"We Are an NVIDIA Inception Partner"},{"location":"news/deep_learning_wizard_nvidia_inception_2018_05_01/#we-did-it","text":"After almost a year, we are an NVIDIA Inception Partner now! \"NVIDIA Inception Partner Deep Learning Wizard is proudly an NVIDIA Inception Partner, empowering people across the world to leverage on deep learning via open-source programming languages and frameworks to solve problems. Cheers, Ritchie Ng","title":"We did it!"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/","text":"Facebook PyTorch Developer Conference \u00b6 We are heading down! \u00b6 In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA. I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it! Cheers, Ritchie Ng","title":"Facebook PyTorch Developer Conference, San Francisco, September 2018"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#facebook-pytorch-developer-conference","text":"","title":"Facebook PyTorch Developer Conference"},{"location":"news/facebook_pytorch_developer_conference_2018_09_05/#we-are-heading-down","text":"In barely 2 short years, PyTorch (Facebook) will be hosting their first PyTorch Developer Conference in San Francisco, USA. I will be heading down thanks to Soumith Chintala for the invite and arrangements. Looking forward to meet anyone there. The PyTorch ecosystem has grown tremendously from when I first started using it. To this date, I've taught more than 3000 students worldwide in 120+ countries and every single wizard has fallen in love with it! Cheers, Ritchie Ng","title":"We are heading down!"},{"location":"news/news/","text":"Welcome to our Blog \u00b6 Here, we post news related to Deep Learning Wizard's releases, features and achievements Notable News \u00b6 Facebook PyTorch Developer Conference, San Francisco, USA, September 2018 Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018 Reached 2200+ students, 2018 Featured on PyTorch Website, January 2018 Reached 1000+ students, 2017 Hosted NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017 NVIDIA Inception Partner, May 2017 And more...","title":"Welcome"},{"location":"news/news/#welcome-to-our-blog","text":"Here, we post news related to Deep Learning Wizard's releases, features and achievements","title":"Welcome to our Blog"},{"location":"news/news/#notable-news","text":"Facebook PyTorch Developer Conference, San Francisco, USA, September 2018 Conducted NUS-MIT-NUHS Datathon NVIDIA Image Recognition Workshop, Singapore, July 2018 Reached 2200+ students, 2018 Featured on PyTorch Website, January 2018 Reached 1000+ students, 2017 Hosted NVIDIA Self-Driving Cars and Healthcare Talk, Singapore, June 2017 NVIDIA Inception Partner, May 2017 And more...","title":"Notable News"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/","text":"NVIDIA Workshop at NUS-MIT-NUHS Datathon \u00b6 Image Recognition Workshop by Ritchie Ng \u00b6 The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning. In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance. Link to the NUS-MIT-NUHS Datathon workshop .","title":"NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#nvidia-workshop-at-nus-mit-nuhs-datathon","text":"","title":"NVIDIA Workshop at NUS-MIT-NUHS Datathon"},{"location":"news/nvidia_nus_mit_datathon_2018_07_05/#image-recognition-workshop-by-ritchie-ng","text":"The NVIDIA Deep Learning Institute (DLI) trains developers, data scientists, and researchers on how to use artificial intelligence to solve real-world problems across a wide range of domains. In the deep learning courses, you\u2019ll learn how to train, optimize, and deploy neural networks using the latest tools, frameworks, and techniques for deep learning. In \u201cImage Classification with DIGITS\u201d mini-course, you will learn how to train a deep neural network to recognize handwritten digits by loading image data into a training environment, choosing and training a network, testing with new data, and iterating to improve performance. Link to the NUS-MIT-NUHS Datathon workshop .","title":"Image Recognition Workshop by Ritchie Ng"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/","text":"NVIDIA Self-Driving Cars and Healthcare Workshop \u00b6 Hosted by Ritchie Ng \u00b6 A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS. We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA. Details: Wednesday, June 21 st 1:00 PM to 3:30 PM The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613","title":"NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#nvidia-self-driving-cars-and-healthcare-workshop","text":"","title":"NVIDIA Self-Driving Cars and Healthcare Workshop"},{"location":"news/nvidia_self_driving_cars_talk_2017_06_21/#hosted-by-ritchie-ng","text":"A talk by Marek Bardo\u0144ski, Senior Deep Learning Research Engineer, NVIDIA Switzerland. This is hosted by Ritchie Ng, Deep Learning Researcher, NExT Search Centre, NUS. We will be touching on cutting-edge applications of deep learning for self-driving cars and medical diagnostics. Also there will be a tutorial followed by networking with deep learning researchers from NUS and NVIDIA. Details: Wednesday, June 21 st 1:00 PM to 3:30 PM The Hangar by NUS Enterprise 21 Heng Mui Keng Terrace, Singapore 119613","title":"Hosted by Ritchie Ng"}]}